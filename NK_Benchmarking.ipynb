{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NK Benchmarking\n",
    "\n",
    "This notebook serves as a user guide to performing the benchmarks outlined in _____. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Python Tools \"\"\"\n",
    "import numpy as np \n",
    "np.set_printoptions(suppress=True) # Changes print functionality\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" Benchmarking Functions \"\"\"\n",
    "from Benchmarking.benchmarking_images import generate_extrapolation_plot,gen_interpolation_plot, plot_seq_results, plot_ablation_results\n",
    "import Benchmarking.interpolation as inter\n",
    "import Benchmarking.extrapolation as ext\n",
    "import Benchmarking.ablation as ablation\n",
    "import Benchmarking.length_dependency as length\n",
    "import Benchmarking.positional_extrapolation as pos_ext\n",
    "\n",
    "\"\"\" NK Specific Utilities \"\"\"\n",
    "from utils.landscape_class import Protein_Landscape\n",
    "from utils.nk_utils.NK_landscape import makeNK,hamming,collapse_single\n",
    "from utils.nk_utils.generate_datasets import gen_distance_subsets\n",
    "\n",
    "\"\"\" Sci kit Learn Models \"\"\"\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "\"\"\" PyTorch Models \"\"\"\n",
    "import torch\n",
    "from architectures.RNN import RNN\n",
    "import architectures.DNN as DNN\n",
    "\n",
    "\"\"\" Skorch Packages \"\"\"\n",
    "from skorch import NeuralNetRegressor \n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from utils.sklearn_utils import save_landscape_dict\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dict(name):\n",
    "    with open(name, \"rb\") as file:\n",
    "        return pkl.load(file)\n",
    "    \n",
    "def save_dict(dictionary,name):\n",
    "    with open(name, \"wb\") as f:\n",
    "        pkl.dump(dictionary, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device = torch.device('cuda')\n",
    "    print('Using device:', torch.cuda.get_device_name(0)) \n",
    "except: \n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Model Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldict = {\"RF\" : (RandomForestRegressor, {'n_jobs':32}),\n",
    "             \"Linear\"  :  (linear_model.LinearRegression, {'n_jobs':32}),\n",
    "             \"GB\" : (GradientBoostingRegressor, {\"n_estimators\" : 200,\"max_depth\" : 5}),\n",
    "              \"MLP\" :(MLPRegressor, {\"hidden_layer_sizes\" : (50,100,50),\n",
    "                             \"activation\" : 'relu'}),\n",
    "              \"SVR\"  : (BaggingRegressor, {\"base_estimator\" : svm.SVR(),\n",
    "                                           \"n_estimators\"   : 10,\n",
    "                                          \"n_jobs\"  : 32}),\n",
    "              \"RNN\" : (NeuralNetRegressor, {\"module\" : RNN(5,32,20,1).double(),\n",
    "                                   \"max_epochs\" : 1000,\n",
    "                                   \"lr\" : 0.1,\n",
    "                                   \"callbacks\" : [EarlyStopping(patience=20)],\n",
    "                                   \"iterator_train__shuffle\" : True,\n",
    "                                   \"batch_size\" : 128,\n",
    "                                   \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu')})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/K0/V0.txt\n",
      "Data/K0/V1.txt\n",
      "Data/K0/V2.txt\n",
      "Data/K0/V3.txt\n",
      "Data/K0/V4.txt\n",
      "Data/K1/V0.txt\n",
      "Data/K1/V1.txt\n",
      "Data/K1/V2.txt\n",
      "Data/K1/V3.txt\n",
      "Data/K1/V4.txt\n",
      "Data/K2/V0.txt\n",
      "Data/K2/V1.txt\n",
      "Data/K2/V2.txt\n",
      "Data/K2/V3.txt\n",
      "Data/K2/V4.txt\n",
      "Data/K3/V0.txt\n",
      "Data/K3/V1.txt\n",
      "Data/K3/V2.txt\n",
      "Data/K3/V3.txt\n",
      "Data/K3/V4.txt\n",
      "Data/K4/V0.txt\n",
      "Data/K4/V1.txt\n",
      "Data/K4/V2.txt\n",
      "Data/K4/V3.txt\n",
      "Data/K4/V4.txt\n"
     ]
    }
   ],
   "source": [
    "# Load old NK Landscape Dicts\n",
    "saved_nk_landscapes = {\"NK-{}\".format(x) : [Protein_Landscape(saved_file=\"Data/K{0}/V{1}.txt\".format(x,y)) for y in range(5)] for x in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph for entire dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536962/536962 [04:39<00:00, 1918.31it/s]\n",
      "  0%|          | 749/536962 [00:00<01:11, 7480.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536962/536962 [00:10<00:00, 53494.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Protein Landscape class\n",
      "            Number of Sequences : 536962\n",
      "            Max Distance        : 2\n",
      "            Number of Distances : 2\n",
      "            Seed Sequence       : M\u001b[32mQ\u001b[0m\u001b[32mY\u001b[0m\u001b[32mK\u001b[0m\u001b[32mL\u001b[0m\u001b[32mI\u001b[0m\u001b[32mL\u001b[0m\u001b[32mN\u001b[0m\u001b[32mG\u001b[0m\u001b[32mK\u001b[0m\u001b[32mT\u001b[0m\u001b[32mL\u001b[0m\u001b[32mK\u001b[0m\u001b[32mG\u001b[0m\u001b[32mE\u001b[0m\u001b[32mT\u001b[0m\u001b[32mT\u001b[0m\u001b[32mT\u001b[0m\u001b[32mE\u001b[0m\u001b[32mA\u001b[0m\u001b[32mV\u001b[0m\u001b[32mD\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mT\u001b[0m\u001b[32mA\u001b[0m\u001b[32mE\u001b[0m\u001b[32mK\u001b[0m\u001b[32mV\u001b[0m\u001b[32mF\u001b[0m\u001b[32mK\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mY\u001b[0m\u001b[32mA\u001b[0m\u001b[32mN\u001b[0m\u001b[32mD\u001b[0m\u001b[32mN\u001b[0m\u001b[32mG\u001b[0m\u001b[32mV\u001b[0m\u001b[32mD\u001b[0m\u001b[32mG\u001b[0m\u001b[32mE\u001b[0m\u001b[32mW\u001b[0m\u001b[32mT\u001b[0m\u001b[32mY\u001b[0m\u001b[32mD\u001b[0m\u001b[32mD\u001b[0m\u001b[32mA\u001b[0m\u001b[32mT\u001b[0m\u001b[32mK\u001b[0m\u001b[32mT\u001b[0m\u001b[32mF\u001b[0m\u001b[32mT\u001b[0m\u001b[32mV\u001b[0m\u001b[32mT\u001b[0m\u001b[32mE\u001b[0m\n",
      "                Modified positions are shown in green\n",
      "            Number of minima : 1177\n",
      "            Number of maxima : 1490\n",
      "            Normalized Extrema Ruggedness : 0.004966831917342381\n",
      "            R/S Ruggedness : 4.607086345132251e-10\n",
      "        \n",
      "Building Protein Graph for entire dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149361/149361 [01:22<00:00, 1807.18it/s]\n",
      "  3%|▎         | 4093/149361 [00:00<00:03, 40927.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149361/149361 [00:03<00:00, 43169.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Protein Landscape class\n",
      "            Number of Sequences : 149361\n",
      "            Max Distance        : 4\n",
      "            Number of Distances : 5\n",
      "            Seed Sequence       : MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNG\u001b[32mV\u001b[0m\u001b[32mD\u001b[0m\u001b[32mG\u001b[0mEWTYDDATKTFT\u001b[32mV\u001b[0mTE\n",
      "                Modified positions are shown in green\n",
      "            Number of minima : 1\n",
      "            Number of maxima : 182\n",
      "            Normalized Extrema Ruggedness : 0.0012252194347922149\n",
      "            R/S Ruggedness : 2.6370513692301434e-11\n",
      "        \n",
      "Building Protein Graph for entire dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5230/5230 [00:46<00:00, 113.41it/s]\n",
      " 40%|████      | 2110/5230 [00:00<00:00, 21042.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5230/5230 [00:00<00:00, 21247.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Protein Landscape class\n",
      "            Number of Sequences : 5230\n",
      "            Max Distance        : 1\n",
      "            Number of Distances : 2\n",
      "            Seed Sequence       : \u001b[32mM\u001b[0m\u001b[32mF\u001b[0m\u001b[32mK\u001b[0m\u001b[32mL\u001b[0m\u001b[32mL\u001b[0m\u001b[32mS\u001b[0m\u001b[32mK\u001b[0m\u001b[32mL\u001b[0m\u001b[32mL\u001b[0m\u001b[32mV\u001b[0m\u001b[32mY\u001b[0m\u001b[32mL\u001b[0m\u001b[32mT\u001b[0m\u001b[32mA\u001b[0m\u001b[32mS\u001b[0m\u001b[32mI\u001b[0m\u001b[32mM\u001b[0m\u001b[32mA\u001b[0m\u001b[32mI\u001b[0m\u001b[32mA\u001b[0m\u001b[32mS\u001b[0m\u001b[32mP\u001b[0m\u001b[32mL\u001b[0m\u001b[32mA\u001b[0m\u001b[32mF\u001b[0m\u001b[32mS\u001b[0m\u001b[32mV\u001b[0m\u001b[32mD\u001b[0m\u001b[32mS\u001b[0m\u001b[32mS\u001b[0m\u001b[32mG\u001b[0m\u001b[32mE\u001b[0m\u001b[32mY\u001b[0m\u001b[32mP\u001b[0m\u001b[32mT\u001b[0m\u001b[32mV\u001b[0m\u001b[32mS\u001b[0m\u001b[32mE\u001b[0m\u001b[32mI\u001b[0m\u001b[32mP\u001b[0m\u001b[32mV\u001b[0m\u001b[32mG\u001b[0m\u001b[32mE\u001b[0m\u001b[32mV\u001b[0m\u001b[32mR\u001b[0m\u001b[32mL\u001b[0m\u001b[32mY\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mI\u001b[0m\u001b[32mA\u001b[0m\u001b[32mD\u001b[0m\u001b[32mG\u001b[0m\u001b[32mV\u001b[0m\u001b[32mW\u001b[0m\u001b[32mS\u001b[0m\u001b[32mH\u001b[0m\u001b[32mI\u001b[0m\u001b[32mA\u001b[0m\u001b[32mT\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mS\u001b[0m\u001b[32mF\u001b[0m\u001b[32mD\u001b[0m\u001b[32mG\u001b[0m\u001b[32mA\u001b[0m\u001b[32mV\u001b[0m\u001b[32mY\u001b[0m\u001b[32mP\u001b[0m\u001b[32mS\u001b[0m\u001b[32mN\u001b[0m\u001b[32mG\u001b[0m\u001b[32mL\u001b[0m\u001b[32mI\u001b[0m\u001b[32mV\u001b[0m\u001b[32mR\u001b[0m\u001b[32mD\u001b[0m\u001b[32mG\u001b[0m\u001b[32mD\u001b[0m\u001b[32mE\u001b[0m\u001b[32mL\u001b[0m\u001b[32mL\u001b[0m\u001b[32mL\u001b[0m\u001b[32mI\u001b[0m\u001b[32mD\u001b[0m\u001b[32mT\u001b[0m\u001b[32mA\u001b[0m\u001b[32mW\u001b[0m\u001b[32mG\u001b[0m\u001b[32mA\u001b[0m\u001b[32mK\u001b[0m\u001b[32mN\u001b[0m\u001b[32mT\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mL\u001b[0m\u001b[32mL\u001b[0m\u001b[32mA\u001b[0m\u001b[32mE\u001b[0m\u001b[32mI\u001b[0m\u001b[32mE\u001b[0m\u001b[32mK\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mI\u001b[0m\u001b[32mG\u001b[0m\u001b[32mL\u001b[0m\u001b[32mP\u001b[0m\u001b[32mV\u001b[0m\u001b[32mT\u001b[0m\u001b[32mR\u001b[0m\u001b[32mA\u001b[0m\u001b[32mV\u001b[0m\u001b[32mS\u001b[0m\u001b[32mT\u001b[0m\u001b[32mH\u001b[0m\u001b[32mF\u001b[0m\u001b[32mH\u001b[0m\u001b[32mD\u001b[0m\u001b[32mD\u001b[0m\u001b[32mR\u001b[0m\u001b[32mV\u001b[0m\u001b[32mG\u001b[0m\u001b[32mG\u001b[0m\u001b[32mV\u001b[0m\u001b[32mD\u001b[0m\u001b[32mV\u001b[0m\u001b[32mL\u001b[0m\u001b[32mR\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mG\u001b[0m\u001b[32mV\u001b[0m\u001b[32mA\u001b[0m\u001b[32mT\u001b[0m\u001b[32mY\u001b[0m\u001b[32mA\u001b[0m\u001b[32mS\u001b[0m\u001b[32mP\u001b[0m\u001b[32mS\u001b[0m\u001b[32mT\u001b[0m\u001b[32mR\u001b[0m\u001b[32mR\u001b[0m\u001b[32mL\u001b[0m\u001b[32mA\u001b[0m\u001b[32mE\u001b[0m\u001b[32mV\u001b[0m\u001b[32mE\u001b[0m\u001b[32mG\u001b[0m\u001b[32mN\u001b[0m\u001b[32mE\u001b[0m\u001b[32mI\u001b[0m\u001b[32mP\u001b[0m\u001b[32mT\u001b[0m\u001b[32mH\u001b[0m\u001b[32mS\u001b[0m\u001b[32mL\u001b[0m\u001b[32mE\u001b[0m\u001b[32mG\u001b[0m\u001b[32mL\u001b[0m\u001b[32mS\u001b[0m\u001b[32mS\u001b[0m\u001b[32mS\u001b[0m\u001b[32mG\u001b[0m\u001b[32mD\u001b[0m\u001b[32mA\u001b[0m\u001b[32mV\u001b[0m\u001b[32mR\u001b[0m\u001b[32mF\u001b[0m\u001b[32mG\u001b[0m\u001b[32mP\u001b[0m\u001b[32mV\u001b[0m\u001b[32mE\u001b[0m\u001b[32mL\u001b[0m\u001b[32mF\u001b[0m\u001b[32mY\u001b[0m\u001b[32mP\u001b[0m\u001b[32mG\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mH\u001b[0m\u001b[32mS\u001b[0m\u001b[32mT\u001b[0m\u001b[32mD\u001b[0m\u001b[32mN\u001b[0m\u001b[32mL\u001b[0m\u001b[32mI\u001b[0m\u001b[32mV\u001b[0m\u001b[32mY\u001b[0m\u001b[32mV\u001b[0m\u001b[32mP\u001b[0m\u001b[32mS\u001b[0m\u001b[32mA\u001b[0m\u001b[32mS\u001b[0m\u001b[32mV\u001b[0m\u001b[32mL\u001b[0m\u001b[32mY\u001b[0m\u001b[32mG\u001b[0m\u001b[32mG\u001b[0m\u001b[32mC\u001b[0m\u001b[32mA\u001b[0m\u001b[32mI\u001b[0m\u001b[32mY\u001b[0m\u001b[32mE\u001b[0m\u001b[32mL\u001b[0m\u001b[32mS\u001b[0m\u001b[32mR\u001b[0m\u001b[32mT\u001b[0m\u001b[32mS\u001b[0m\u001b[32mA\u001b[0m\u001b[32mG\u001b[0m\u001b[32mN\u001b[0m\u001b[32mV\u001b[0m\u001b[32mA\u001b[0m\u001b[32mD\u001b[0m\u001b[32mA\u001b[0m\u001b[32mD\u001b[0m\u001b[32mL\u001b[0m\u001b[32mA\u001b[0m\u001b[32mE\u001b[0m\u001b[32mW\u001b[0m\u001b[32mP\u001b[0m\u001b[32mT\u001b[0m\u001b[32mS\u001b[0m\u001b[32mI\u001b[0m\u001b[32mE\u001b[0m\u001b[32mR\u001b[0m\u001b[32mI\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mH\u001b[0m\u001b[32mY\u001b[0m\u001b[32mP\u001b[0m\u001b[32mE\u001b[0m\u001b[32mA\u001b[0m\u001b[32mQ\u001b[0m\u001b[32mF\u001b[0m\u001b[32mV\u001b[0m\u001b[32mI\u001b[0m\u001b[32mP\u001b[0m\u001b[32mG\u001b[0m\u001b[32mH\u001b[0m\u001b[32mG\u001b[0m\u001b[32mL\u001b[0m\u001b[32mP\u001b[0m\u001b[32mG\u001b[0m\u001b[32mG\u001b[0m\u001b[32mL\u001b[0m\u001b[32mD\u001b[0m\u001b[32mL\u001b[0m\u001b[32mL\u001b[0m\u001b[32mK\u001b[0m\u001b[32mH\u001b[0m\u001b[32mT\u001b[0m\u001b[32mT\u001b[0m\u001b[32mN\u001b[0m\u001b[32mV\u001b[0m\u001b[32mV\u001b[0m\u001b[32mK\u001b[0m\u001b[32mA\u001b[0m\u001b[32mH\u001b[0m\u001b[32mT\u001b[0m\u001b[32mN\u001b[0m\u001b[32mR\u001b[0m\u001b[32mS\u001b[0m\u001b[32mV\u001b[0m\u001b[32mV\u001b[0m\u001b[32mE\u001b[0m\n",
      "                Modified positions are shown in green\n",
      "            Number of minima : 161\n",
      "            Number of maxima : 169\n",
      "            Normalized Extrema Ruggedness : 0.06309751434034416\n",
      "            R/S Ruggedness : 16.47846152600471\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor land_iter in experimental_dict.values():\\n    for instance in land_iter:\\n        instance.save()'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example code to generate experimental landscapes\n",
    "\n",
    "experimental_dict = {\"GProt_Complete\" : [Protein_Landscape(csv_path=\"experimental_csv\",seed_seq=\"MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")],\n",
    "                     \"GProtein_Mut4\"   : [Protein_Landscape(csv_path=\"experimental_csv\",seed_seq=\"MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\")],\n",
    "                     \"BLactamase\"      : [Protein_Landscape(csv_path=\"experimental_csv\",seed_seq=\"MFKLLSKLLVYLTASIMAIASPLAFSVDSSGEYPTVSEIPVGEVRLYQIADGVWSHIATQSFDGAVYPSNGLIVRDGDELLLIDTAWGAKNTAALLAEIEKQIGLPVTRAVSTHFHDDRVGGVDVLRAAGVATYASPSTRRLAEVEGNEIPTHSLEGLSSSGDAVRFGPVELFYPGAAHSTDNLIVYVPSASVLYGGCAIYELSRTSAGNVADADLAEWPTSIERIQQHYPEAQFVIPGHGLPGGLDLLKHTTNVVKAHTNRSVVE\")]}\n",
    "\n",
    "# Command to save all landscape dictionaries to that they don't have to be regenerated\n",
    "save_landscape_dict(experimental_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Protein Landscape class\n",
      "            Number of Sequences : 100000\n",
      "            Max Distance        : 5\n",
      "            Number of Distances : 6\n",
      "            Seed Sequence       : \u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\u001b[32mA\u001b[0m\n",
      "                Modified positions are shown in green\n",
      "            Number of minima : 1\n",
      "            Number of maxima : 1\n",
      "            Normalized Extrema Ruggedness : 2e-05\n",
      "            R/S Ruggedness : 30.20584027341869\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(saved_nk_landscapes[\"NK-0\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ True, False, False, ..., False, False, False]),\n",
       " 1: array([False,  True,  True, ..., False, False, False]),\n",
       " 2: array([False, False, False, ..., False, False, False]),\n",
       " 3: array([False, False, False, ..., False, False, False]),\n",
       " 4: array([False, False, False, ..., False, False, False]),\n",
       " 5: array([False, False, False, ...,  True,  True,  True])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of querying the landscapes\n",
    "saved_nk_landscapes[\"NK-0\"][0].d_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of how to run positional extrapolation testing\n",
    "\n",
    "#GProtein_Datasets = {key : value for key,value in saved_experimental_landscapes.items() if key in [\"GProtein_Mut4\"]}\n",
    "\n",
    "extrapolation_results = ext.extrapolation(modeldict,saved_nk_landscapes,file_name=\"NK_Extrapolation\",cross_validation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk_extrapolation_results = load_dict(\"Results/Extrapolation/Full_NK_Extrapolation_Results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_Mut4_Results = load_dict(\"Results/Extrapolation/Full_GProtein_Mut4_Extrapolation_Results.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_interpolation_results = load_dict(\"Results/Interpolation/Full_NK_Interpolation_Results.pkl\")\n",
    "# Not included in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for testing ablation\n",
    "\n",
    "full_ablation_results = ablation.ablation_testing(modeldict,saved_nk_landscapes,file_name=\"Full_Ablation_NK_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NK_ablation_results = load_dict(\"Results/Ablation Studies/Full_Ablation_NK_Results.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_ablation_results = load_dict(\"Results/Ablation Studies/Full_Experimental_Ablation_Results_NoSVR.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam/PhD/Protein_Evolution/RNN/Data/GProtein/Full_GProtein.txt\n",
      "[34, 11, 14, 52, 50, 4, 7, 33, 38, 37]\n",
      "[41, 32, 48, 0, 8, 6, 23, 45, 37, 28]\n",
      "[36, 26, 50, 34, 14, 2, 55, 43, 16, 18]\n",
      "[8, 9, 15, 20, 50, 53, 14, 25, 39, 22]\n",
      "[30, 25, 49, 46, 24, 27, 50, 14, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "# Example code to test positional extrapolation\n",
    "\n",
    "from utils.sklearn_utils import reset_params_skorch\n",
    "\n",
    "positional = Protein_Landscape(saved_file=\"/home/adam/PhD/Protein_Evolution/RNN/Data/GProtein/Full_GProtein.txt\")\n",
    "\n",
    "positional_extrapolation_small = np.zeros((5,5))\n",
    "models = [RandomForestRegressor(n_jobs=32),\n",
    "          linear_model.LinearRegression(n_jobs=32),\n",
    "          GradientBoostingRegressor(n_estimators=200,max_depth=5),\n",
    "          MLPRegressor(hidden_layer_sizes=(50,100,50),activation=\"relu\"),\n",
    "          NeuralNetRegressor(**{\"module\" : RNN(5,32,20,1).double(),\n",
    "                                   \"max_epochs\" : 2000,\n",
    "                                   \"lr\" : 0.05,\n",
    "                                   \"callbacks\" : [EarlyStopping(patience=10)],\n",
    "                                   \"iterator_train__shuffle\" : True,\n",
    "                                   \"batch_size\" : 128,\n",
    "                                   \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu')})]\n",
    "\"\"\",\n",
    "         BaggingRegressor(base_estimator=svm.SVR(),n_estimators=10,n_jobs=32)] \"\"\"\n",
    "\n",
    "positional_split_lengths = [10,20,30,40,50]\n",
    "for_positions = [list(np.random.choice(range(len(positional.seed_seq)),x,replace=False)) for x in positional_split_lengths]\n",
    "test_positions = []\n",
    "for i,pos in enumerate(positional_split_lengths):\n",
    "    test_positions.append([x for x in range(len(positional.seed_seq)) if x not in for_positions[i]])\n",
    "\n",
    "\n",
    "#pos_splits2 = [25,50,75,100,125,150,175,200,225,250]\n",
    "\n",
    "sizes = np.zeros((5,3))\n",
    "\n",
    "for i,split in enumerate(positional_split_lengths):\n",
    "    print(for_positions[i][:10])\n",
    "    train = positional.tokenized[(positional.indexing(positions=for_positions[i]))]\n",
    "    x_train = train[:,:-1].astype(\"int\")\n",
    "    y_train = train[:,-1].astype(\"float\")\n",
    "    sizes[i][0] = len(x_train)\n",
    "    test = positional.tokenized[(positional.indexing(positions=test_positions[i]))]\n",
    "    x_test = test[:,:-1].astype(\"int\")\n",
    "    y_test = test[:,-1].astype(\"float\")\n",
    "    sizes[i][2] = len(x_test)\n",
    "    sizes[i][1] = len(positional)-len(x_train)-len(x_test)\n",
    "    \n",
    "    \n",
    "    \"\"\"print(len(y_test))\n",
    "    \n",
    "    for j, model in enumerate(models):\n",
    "        if model.__class__.__name__ == \"NeuralNetRegressor\":\n",
    "            reset_params_skorch(model)\n",
    "            model.fit(x_train,y_train.reshape(-1,1))\n",
    "            print(model.score(x_test,y_test.reshape(-1,1)))\n",
    "            positional_extrapolation_small[i][j] = model.score(x_test,y_test.reshape(-1,1))\n",
    "\n",
    "        else:\n",
    "            \n",
    "            model.fit(x_train,y_train)\n",
    "            print(model.score(x_test,y_test))\n",
    "            positional_extrapolation_small[i][j] = model.score(x_test,y_test)\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADt0lEQVR4nO3d0WnCUBiA0VrcJvt0moIEOk33yTaRdAB9EGm+aHLOALmX+/Dxo+HmtCzLBwCNz603AHAkogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkDovNqTf77a29G/f0/pes8qz8WZ3HIm973DuezkTNaLLsA/Gq9Dut5lpef6eQEgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ+3ThBe3l7lhumXQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEPK5HuAtTPM+Pipk0gUIiS5ASHQBQqILEBJdgJC3F9jceB2ytfbx/zfvzKQLEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugCh89YbOJrxOmRrXbKVgEeZdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACFXO8ILmmYXc+6VSRcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAofNaDx6vw1qPvuuSrgbwHJMuQEh0AUKiCxASXYCQ6AKERBcgJLoAodXe04VHTbO3rDkOky5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNB56w0czTRftt4CsCGTLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQitdom5y7oBbpl0AUKiCxASXYCQ6AKERBcgJLoAodOyLFvvAeAwTLoAIdEFCIkuQEh0AUKiCxASXYDQH+B+LTW5gZgEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = sizes[:,0]\n",
    "omitted = sizes[:,1]\n",
    "test = sizes[:,2]\n",
    "ind = np.arange(5)\n",
    "width=0.4\n",
    "\n",
    "p1 = plt.bar(ind, train, width,color=(85/255,124/255,255/255))\n",
    "p2 = plt.bar(ind, omitted, width,bottom=train,color=(0.5,0.5,0.5))\n",
    "p3 = plt.bar(ind, test, width, bottom=(train+omitted),color=(1,131/255,85/255))\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"Data Split.png\",bbox_inches=\"tight\",dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADnCAYAAAD/yKGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADB0lEQVR4nO3WsU0DURAA0X+IDjDp4SIICSidgJAiwCmmhk8BGMmWsMzI76W7wSYj7TLnHMD/d3PpA4DjiBUixAoRYoUIsULE7SnLm83d3K7ruW6Bq/e+2439/ms5NDsp1u26jrfXlz85Cvjp8en515k3GCLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0Qsc87jl5flc4zxcb5z4Oo9zDnvDw1OihW4HG8wRIgVIsQKEWKFCLFCxDeq1xojGlKaQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = np.load(\"Results/Positional Extrapolation/positional_extrapolation_Gprot_full.npy\")\n",
    "\n",
    "results = test.clip(min=0)\n",
    "plt.imshow(results,vmin=0,vmax=1,cmap=\"Reds\")\n",
    "\n",
    "plt.yticks([])\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "plt.savefig(\"Images/Exciting Heatmap.png\",dpi=600,bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2005321584840258\n",
      "-0.7023427225668522\n",
      "-0.9964150354108479\n",
      "-0.6298069622686315\n",
      "-1.1314063666576217\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m19.0609\u001b[0m        \u001b[32m5.4183\u001b[0m  0.1800\n",
      "      2        \u001b[36m3.8269\u001b[0m        8.6696  0.1782\n",
      "      3        \u001b[36m1.7471\u001b[0m        \u001b[32m5.2989\u001b[0m  0.1824\n",
      "      4        \u001b[36m1.2491\u001b[0m        6.5875  0.1765\n",
      "      5        \u001b[36m1.1079\u001b[0m        5.6825  0.1772\n",
      "      6        1.1154        6.8563  0.1768\n",
      "      7        1.1090        5.3487  0.1810\n",
      "      8        1.1940        6.4933  0.1770\n",
      "      9        1.2175        5.7591  0.1760\n",
      "     10        1.1143        6.1473  0.1791\n",
      "     11        1.1260        5.7182  0.1800\n",
      "     12        1.1257        6.0178  0.1780\n",
      "     13        1.1192        6.3825  0.1802\n",
      "     14        1.1199        6.1361  0.1756\n",
      "     15        1.2109        5.4277  0.1794\n",
      "     16        1.1185        6.1200  0.1798\n",
      "     17        1.1153        6.1986  0.1775\n",
      "     18        1.1485        5.5604  0.1789\n",
      "     19        1.1322        5.6363  0.1844\n",
      "     20        1.1336        5.6565  0.1883\n",
      "     21        1.1234        6.1320  0.1774\n",
      "     22        1.1654        5.6076  0.1812\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-1.3027448909987727\n",
      "-0.5846424467634532\n",
      "-0.8307649802198556\n",
      "-0.9531908376431417\n",
      "-0.960251294986175\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1       \u001b[36m11.0678\u001b[0m        \u001b[32m4.4827\u001b[0m  0.3905\n",
      "      2        \u001b[36m1.8780\u001b[0m        \u001b[32m3.3448\u001b[0m  0.3690\n",
      "      3        \u001b[36m1.6343\u001b[0m        \u001b[32m3.3296\u001b[0m  0.3698\n",
      "      4        1.6395        3.3968  0.3463\n",
      "      5        1.6696        3.4031  0.3460\n",
      "      6        1.7219        3.3586  0.3509\n",
      "      7        1.6505        3.3554  0.3496\n",
      "      8        1.6930        3.3565  0.3529\n",
      "      9        1.6636        \u001b[32m3.3286\u001b[0m  0.3513\n",
      "     10        \u001b[36m1.6295\u001b[0m        3.4026  0.3530\n",
      "     11        1.6446        3.3511  0.3507\n",
      "     12        1.7357        3.4941  0.3624\n",
      "     13        1.7082        \u001b[32m3.3283\u001b[0m  0.3668\n",
      "     14        1.7722        3.4345  0.3785\n",
      "     15        1.6749        3.3755  0.3700\n",
      "     16        1.7144        3.3867  0.3666\n",
      "     17        1.6706        3.3356  0.3633\n",
      "     18        1.8127        3.4283  0.3685\n",
      "     19        1.7523        3.4722  0.3619\n",
      "     20        1.6810        3.3394  0.3582\n",
      "     21        1.7075        3.3630  0.3582\n",
      "     22        1.7554        3.3602  0.3667\n",
      "     23        1.7130        3.4454  0.3638\n",
      "     24        1.6668        \u001b[32m3.3276\u001b[0m  0.3679\n",
      "     25        1.6501        3.3311  0.3558\n",
      "     26        1.6470        3.3308  0.3849\n",
      "     27        1.6346        3.3528  0.3538\n",
      "     28        1.6692        3.4337  0.3831\n",
      "     29        1.6525        3.3306  0.3601\n",
      "     30        1.6370        3.4079  0.3608\n",
      "     31        1.6576        3.3735  0.3645\n",
      "     32        1.6449        3.3352  0.3638\n",
      "     33        1.6714        3.3393  0.3599\n",
      "     34        1.6338        3.3276  0.3589\n",
      "     35        1.7988        \u001b[32m3.3274\u001b[0m  0.3613\n",
      "     36        1.6469        3.3315  0.4523\n",
      "     37        1.6922        3.3621  0.3578\n",
      "     38        1.7934        3.6887  0.3548\n",
      "     39        1.7495        3.3312  0.3553\n",
      "     40        1.6492        3.3603  0.3639\n",
      "     41        1.6735        3.4446  0.3579\n",
      "     42        1.7289        3.3465  0.3578\n",
      "     43        1.6679        3.3286  0.3646\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-1.2661626375164694\n",
      "-0.2846022070131251\n",
      "-0.6472719154601072\n",
      "-0.10302475247178622\n",
      "-0.7138179928041186\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m7.9152\u001b[0m        \u001b[32m3.1671\u001b[0m  0.5719\n",
      "      2        \u001b[36m3.2781\u001b[0m        \u001b[32m2.9188\u001b[0m  0.5293\n",
      "      3        3.4746        3.0179  0.5243\n",
      "      4        3.8142        3.0763  0.5276\n",
      "      5        \u001b[36m3.1496\u001b[0m        2.9641  0.5313\n",
      "      6        \u001b[36m3.1267\u001b[0m        3.3366  0.6194\n",
      "      7        3.1324        3.4665  0.5296\n",
      "      8        3.1503        \u001b[32m2.8721\u001b[0m  0.5402\n",
      "      9        3.1975        3.5953  0.5439\n",
      "     10        \u001b[36m3.1251\u001b[0m        3.2824  0.5558\n",
      "     11        3.1847        3.0825  0.5371\n",
      "     12        3.1301        3.7485  0.5389\n",
      "     13        3.2013        \u001b[32m2.8041\u001b[0m  0.5329\n",
      "     14        3.1439        3.3384  0.5359\n",
      "     15        3.1627        3.1407  0.5342\n",
      "     16        3.1393        3.8354  0.5302\n",
      "     17        \u001b[36m3.1222\u001b[0m        \u001b[32m2.7930\u001b[0m  0.5329\n",
      "     18        3.2597        3.2327  0.5324\n",
      "     19        3.1548        3.3447  0.6727\n",
      "     20        3.2025        \u001b[32m2.7674\u001b[0m  0.6801\n",
      "     21        3.1451        3.0683  0.5394\n",
      "     22        3.1647        3.9398  0.5084\n",
      "     23        3.2100        4.1682  0.5176\n",
      "     24        3.1414        4.3943  0.5900\n",
      "     25        3.1885        2.9132  0.6087\n",
      "     26        3.1587        2.7875  0.5287\n",
      "     27        3.2115        3.3973  0.5147\n",
      "     28        3.1404        3.1395  0.5210\n",
      "     29        3.1720        3.2488  0.5191\n",
      "     30        3.1382        3.2905  0.5245\n",
      "     31        3.1324        2.8584  0.5151\n",
      "     32        3.2137        3.4322  0.5184\n",
      "     33        3.1894        2.9614  0.5280\n",
      "     34        3.1312        3.1178  0.5130\n",
      "     35        3.1769        3.2245  0.5217\n",
      "     36        3.1553        2.9960  0.6804\n",
      "     37        3.1475        3.2923  0.7059\n",
      "     38        3.1542        2.9538  0.6249\n",
      "     39        \u001b[36m3.1149\u001b[0m        3.1807  0.5149\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-1.1076686016300572\n",
      "-0.08890256626014104\n",
      "-0.43612228651826346\n",
      "-0.2042893527841656\n",
      "-0.5224016352513441\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m7.2979\u001b[0m        \u001b[32m3.5109\u001b[0m  0.7433\n",
      "      2        \u001b[36m3.9384\u001b[0m        3.8187  0.6725\n",
      "      3        3.9830        \u001b[32m3.5015\u001b[0m  0.6808\n",
      "      4        \u001b[36m3.9274\u001b[0m        3.5918  0.6656\n",
      "      5        3.9986        6.2387  0.6640\n",
      "      6        \u001b[36m3.9085\u001b[0m        \u001b[32m3.4571\u001b[0m  0.6627\n",
      "      7        \u001b[36m3.9077\u001b[0m        3.6117  0.6638\n",
      "      8        3.9377        4.7236  0.6677\n",
      "      9        3.9666        4.7065  0.6743\n",
      "     10        3.9225        3.9634  0.6717\n",
      "     11        \u001b[36m3.8542\u001b[0m        \u001b[32m3.4287\u001b[0m  0.6658\n",
      "     12        \u001b[36m3.8434\u001b[0m        4.3913  0.6659\n",
      "     13        3.8665        6.2459  0.6662\n",
      "     14        3.9007        4.5187  0.6648\n",
      "     15        3.8754        3.7872  0.6644\n",
      "     16        3.8859        3.7784  0.6647\n",
      "     17        3.8653        3.6768  0.6657\n",
      "     18        3.8530        3.6770  0.6662\n",
      "     19        \u001b[36m3.8413\u001b[0m        3.8319  0.6616\n",
      "     20        3.8435        4.4496  0.6570\n",
      "     21        4.0896        3.9375  0.6639\n",
      "     22        3.9289        4.1759  0.6569\n",
      "     23        3.8942        3.4771  0.6597\n",
      "     24        \u001b[36m3.8250\u001b[0m        4.9612  0.6718\n",
      "     25        3.8544        4.1243  0.6730\n",
      "     26        4.0077        6.0235  0.6601\n",
      "     27        3.9131        4.1172  0.6726\n",
      "     28        3.9495        4.4792  0.6618\n",
      "     29        3.8883        5.1374  0.6725\n",
      "     30        3.8512        4.2933  0.6639\n",
      "     31        3.8852        \u001b[32m3.2874\u001b[0m  0.6635\n",
      "     32        3.8735        4.4072  0.6530\n",
      "     33        3.8875        3.9707  0.6722\n",
      "     34        3.8449        3.5517  0.6666\n",
      "     35        3.8536        3.3495  0.6604\n",
      "     36        3.8921        3.6598  0.6625\n",
      "     37        3.8640        4.1838  0.6669\n",
      "     38        3.8513        5.6373  0.6598\n",
      "     39        3.8396        3.6561  0.6755\n",
      "     40        3.8419        3.5102  0.6728\n",
      "     41        3.8669        3.5063  0.6657\n",
      "     42        3.8962        3.3865  0.6676\n",
      "     43        3.8891        3.6795  0.6574\n",
      "     44        3.8545        4.2536  0.6582\n",
      "     45        3.8427        3.8887  0.6670\n",
      "     46        3.8532        3.8668  0.6697\n",
      "     47        3.8513        3.8481  0.6670\n",
      "     48        3.8521        5.4325  0.6615\n",
      "     49        3.8446        4.2923  0.6669\n",
      "     50        4.0141        \u001b[32m2.9820\u001b[0m  0.6622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     51        3.8973        5.5980  0.6625\n",
      "     52        3.8687        4.6675  0.6603\n",
      "     53        3.8425        3.0447  0.6660\n",
      "     54        3.9136        4.0494  0.6597\n",
      "     55        3.8441        4.7185  0.6594\n",
      "     56        3.9243        3.3825  0.6604\n",
      "     57        3.8368        4.0080  0.6572\n",
      "     58        3.9148        3.8538  0.6657\n",
      "     59        3.8411        4.0482  0.6655\n",
      "     60        3.8454        3.5856  0.6607\n",
      "     61        3.8341        3.9916  0.6602\n",
      "     62        3.8274        3.6822  0.6652\n",
      "     63        3.8786        3.7289  0.6612\n",
      "     64        3.8714        3.9420  0.6626\n",
      "     65        3.8770        3.5807  0.6617\n",
      "     66        3.8337        4.7287  0.6599\n",
      "     67        3.8670        3.6323  0.6648\n",
      "     68        3.9062        4.5420  0.6610\n",
      "     69        3.8349        3.8166  0.6823\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-1.0618422059467778\n",
      "-0.029828106302050017\n",
      "-0.22064274179422272\n",
      "-0.01633688715168846\n",
      "-0.38290833853982864\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m6.0098\u001b[0m       \u001b[32m10.1080\u001b[0m  0.8751\n",
      "      2        \u001b[36m4.2210\u001b[0m        \u001b[32m4.2356\u001b[0m  0.8894\n",
      "      3        \u001b[36m4.1673\u001b[0m        7.1631  0.8944\n",
      "      4        \u001b[36m4.0173\u001b[0m        5.7387  0.9069\n",
      "      5        4.0672        6.6425  0.9015\n",
      "      6        4.3147        5.5848  0.8861\n",
      "      7        4.1110        5.3311  1.0160\n",
      "      8        4.0950        \u001b[32m3.2046\u001b[0m  0.9412\n",
      "      9        4.1543        4.6558  0.8841\n",
      "     10        4.0511        5.3187  0.9288\n",
      "     11        4.0324        5.3909  0.8787\n",
      "     12        4.0624        3.8834  0.8599\n",
      "     13        4.0745        5.0884  0.8214\n",
      "     14        4.0324        6.9130  0.8084\n",
      "     15        4.0436        5.5706  0.9529\n",
      "     16        4.0469        5.0337  0.9438\n",
      "     17        4.0375        6.0894  0.8227\n",
      "     18        4.1169        5.7619  0.8247\n",
      "     19        4.0236        4.4296  0.8183\n",
      "     20        4.0193        5.9437  0.8190\n",
      "     21        \u001b[36m4.0141\u001b[0m        4.2376  0.8127\n",
      "     22        4.0312        5.9187  0.8094\n",
      "     23        4.0219        4.7572  0.8067\n",
      "     24        4.0390        5.4189  0.8062\n",
      "     25        4.0196        4.6875  0.8111\n",
      "     26        4.0273        3.8847  0.8058\n",
      "     27        4.0435        5.7574  0.8094\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-1.0326069981403916\n",
      "-0.02639285303560146\n",
      "-0.16762696351714923\n",
      "-0.014162242946403891\n",
      "-0.34925065287253165\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.5491\u001b[0m        \u001b[32m5.6843\u001b[0m  1.0448\n",
      "      2        \u001b[36m4.0669\u001b[0m        \u001b[32m4.2740\u001b[0m  1.0240\n",
      "      3        4.1693        4.4122  1.0237\n",
      "      4        \u001b[36m4.0311\u001b[0m        4.5833  1.0133\n",
      "      5        4.0480        7.9657  1.0211\n",
      "      6        4.0590        5.1346  1.0409\n",
      "      7        \u001b[36m3.9798\u001b[0m        \u001b[32m2.6445\u001b[0m  1.0180\n",
      "      8        4.0775        4.6564  1.0099\n",
      "      9        3.9833        5.7697  1.0081\n",
      "     10        4.0391        3.8454  1.0011\n",
      "     11        3.9872        6.9520  1.0107\n",
      "     12        4.0177        4.5235  1.0099\n",
      "     13        \u001b[36m3.9770\u001b[0m        4.6683  1.0130\n",
      "     14        \u001b[36m3.9735\u001b[0m        6.4744  1.0087\n",
      "     15        3.9932        4.0759  1.0127\n",
      "     16        4.0182        3.7974  1.0133\n",
      "     17        \u001b[36m3.9524\u001b[0m        3.9869  1.0085\n",
      "     18        3.9829        3.4928  1.0211\n",
      "     19        4.0440        5.2087  1.0233\n",
      "     20        3.9967        4.3680  1.0246\n",
      "     21        3.9618        5.9525  1.0161\n",
      "     22        3.9769        4.4426  1.0075\n",
      "     23        3.9806        6.1590  1.2049\n",
      "     24        3.9735        4.5316  1.1814\n",
      "     25        3.9927        4.2061  1.0873\n",
      "     26        3.9839        6.8803  1.0500\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-1.0970469685498703\n",
      "-0.012784444094967817\n",
      "-0.12072931618782956\n",
      "-0.026550877553758623\n",
      "-0.31484574989727454\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.4933\u001b[0m        \u001b[32m3.3962\u001b[0m  1.2017\n",
      "      2        \u001b[36m4.0901\u001b[0m        7.5444  1.2141\n",
      "      3        \u001b[36m4.0755\u001b[0m        3.7105  1.2024\n",
      "      4        4.1143        4.6336  1.2012\n",
      "      5        4.1118        5.4493  1.2034\n",
      "      6        4.1217        4.1649  1.3119\n",
      "      7        4.0885        4.2220  1.2426\n",
      "      8        \u001b[36m4.0142\u001b[0m        8.0333  1.2127\n",
      "      9        4.0601        6.4579  1.1857\n",
      "     10        4.0514        5.1279  1.2037\n",
      "     11        4.0998        3.9152  1.2998\n",
      "     12        4.0546        \u001b[32m3.2074\u001b[0m  1.1874\n",
      "     13        4.0294        5.0756  1.2607\n",
      "     14        4.0447        4.3139  1.1718\n",
      "     15        4.0465        4.2591  1.1897\n",
      "     16        4.0287        4.2880  1.1820\n",
      "     17        4.0568        6.4472  1.1794\n",
      "     18        4.0474        4.5258  1.1581\n",
      "     19        4.0534        3.8124  1.1671\n",
      "     20        4.0729        5.1940  1.1989\n",
      "     21        4.0283        5.0146  1.2010\n",
      "     22        4.0339        5.1202  1.2115\n",
      "     23        4.0647        4.9236  1.2047\n",
      "     24        4.0878        \u001b[32m2.5154\u001b[0m  1.1691\n",
      "     25        4.0898        4.9664  1.1667\n",
      "     26        4.0226        4.2751  1.1923\n",
      "     27        4.0764        5.7121  1.1877\n",
      "     28        4.0591        4.3095  1.1766\n",
      "     29        4.0287        6.1162  1.1829\n",
      "     30        4.0360        4.5949  1.2187\n",
      "     31        4.0932        5.2732  1.1827\n",
      "     32        4.0565        4.4804  1.2273\n",
      "     33        4.0301        4.7208  1.2075\n",
      "     34        4.0559        3.3232  1.2723\n",
      "     35        4.0461        4.9530  1.2352\n",
      "     36        4.0437        4.0742  1.2097\n",
      "     37        4.0382        5.7348  1.1989\n",
      "     38        4.0290        5.3966  1.2092\n",
      "     39        4.0421        5.8994  1.2933\n",
      "     40        4.0280        4.7145  1.2397\n",
      "     41        4.0254        4.7636  1.1948\n",
      "     42        4.0336        4.3254  1.1888\n",
      "     43        4.0320        4.6951  1.2017\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-0.7914170802268261\n",
      "-0.034078167875409404\n",
      "-0.0002314413162451956\n",
      "-0.1494762422364455\n",
      "-0.08834178173918095\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.0472\u001b[0m        \u001b[32m7.1763\u001b[0m  1.3677\n",
      "      2        \u001b[36m4.2414\u001b[0m        \u001b[32m5.0293\u001b[0m  1.3543\n",
      "      3        \u001b[36m4.1848\u001b[0m        7.4971  1.3590\n",
      "      4        4.2963        6.2024  1.3814\n",
      "      5        \u001b[36m4.1520\u001b[0m        6.4147  1.3535\n",
      "      6        4.2405        5.1840  1.3505\n",
      "      7        4.2754        6.4579  1.3322\n",
      "      8        4.1895        6.6630  1.3366\n",
      "      9        4.1701        \u001b[32m4.0793\u001b[0m  1.3281\n",
      "     10        4.2289        6.3224  1.3272\n",
      "     11        4.2006        7.0947  1.3195\n",
      "     12        4.2261        4.4679  1.3273\n",
      "     13        4.1643        \u001b[32m3.5713\u001b[0m  1.3228\n",
      "     14        4.1886        6.0581  1.3145\n",
      "     15        4.2183        4.4394  1.3343\n",
      "     16        4.2228        7.6512  1.3510\n",
      "     17        4.1689        4.8744  1.3467\n",
      "     18        4.1737        5.1873  1.3502\n",
      "     19        4.1821        7.6043  1.3273\n",
      "     20        4.2025        6.2340  1.3465\n",
      "     21        4.1679        5.2395  1.3466\n",
      "     22        4.1931        \u001b[32m3.5025\u001b[0m  1.3357\n",
      "     23        4.1883        6.9558  1.3418\n",
      "     24        4.1873        6.0638  1.3531\n",
      "     25        4.1891        5.2154  1.3164\n",
      "     26        4.1799        6.3172  1.3065\n",
      "     27        4.1764        4.7229  1.3126\n",
      "     28        4.1751        5.8653  1.3058\n",
      "     29        4.1595        4.0338  1.2987\n",
      "     30        4.1732        6.1288  1.3115\n",
      "     31        4.1720        5.9567  1.3188\n",
      "     32        4.1656        4.4771  1.3309\n",
      "     33        4.1701        4.9789  1.3140\n",
      "     34        4.1569        5.0053  1.3271\n",
      "     35        4.2026        5.4244  1.3216\n",
      "     36        4.1689        5.9099  1.3202\n",
      "     37        4.1571        6.8326  1.3267\n",
      "     38        4.1814        5.5603  1.3224\n",
      "     39        4.1804        5.5020  1.3247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     40        4.1830        5.5208  1.3254\n",
      "     41        4.1820        4.7716  1.3138\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-0.8038003251304557\n",
      "-0.0025866061421113695\n",
      "-0.01951182923604522\n",
      "-0.07006280820977118\n",
      "-0.16628163590501566\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.9159\u001b[0m        \u001b[32m4.4688\u001b[0m  1.5390\n",
      "      2        \u001b[36m4.2809\u001b[0m        \u001b[32m3.2885\u001b[0m  1.4857\n",
      "      3        4.3026        \u001b[32m3.0527\u001b[0m  1.4774\n",
      "      4        \u001b[36m4.1654\u001b[0m        5.8961  1.4766\n",
      "      5        4.1867        4.9085  1.4863\n",
      "      6        4.1850        \u001b[32m2.9401\u001b[0m  1.5114\n",
      "      7        \u001b[36m4.1646\u001b[0m        4.0149  1.4903\n",
      "      8        4.1671        6.2632  1.5979\n",
      "      9        4.1804        5.7774  1.5674\n",
      "     10        4.1708        4.2113  1.5726\n",
      "     11        \u001b[36m4.1590\u001b[0m        4.4754  1.5811\n",
      "     12        \u001b[36m4.1513\u001b[0m        4.6575  1.6404\n",
      "     13        4.1880        4.0129  1.5505\n",
      "     14        4.1551        4.5235  1.5811\n",
      "     15        \u001b[36m4.1456\u001b[0m        5.3607  1.5724\n",
      "     16        4.1751        5.1712  1.5853\n",
      "     17        4.2021        9.0436  1.5512\n",
      "     18        4.2214        5.2031  1.5486\n",
      "     19        4.1678        7.8755  1.6009\n",
      "     20        4.1878        4.1688  1.5915\n",
      "     21        4.1561        6.2734  1.6022\n",
      "     22        4.1688        4.5221  1.5792\n",
      "     23        \u001b[36m4.1450\u001b[0m        4.2550  1.5882\n",
      "     24        4.1522        \u001b[32m2.7489\u001b[0m  1.5928\n",
      "     25        4.1814        5.0607  1.5708\n",
      "     26        4.1929        4.3605  1.5737\n",
      "     27        4.1526        6.5651  1.5717\n",
      "     28        4.1745        5.6536  1.5820\n",
      "     29        4.1590        4.0533  1.5773\n",
      "     30        4.1882        3.8083  1.5762\n",
      "     31        4.1764        3.2214  1.5847\n",
      "     32        4.1816        4.4901  1.5921\n",
      "     33        4.1893        3.7740  1.6243\n",
      "     34        4.1579        5.0698  1.7719\n",
      "     35        4.1775        5.4612  2.0701\n",
      "     36        \u001b[36m4.1394\u001b[0m        5.0755  1.6760\n",
      "     37        4.1651        4.3995  1.5539\n",
      "     38        4.1501        6.4767  1.4744\n",
      "     39        4.1493        4.4283  1.4808\n",
      "     40        4.1443        3.8606  1.4918\n",
      "     41        4.1794        6.5816  1.4908\n",
      "     42        4.1757        4.8676  1.4813\n",
      "     43        4.1661        4.8267  1.4731\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "-0.43692928937950115\n",
      "-0.25619508207587827\n",
      "-0.09365681128888625\n",
      "-0.40267101293416085\n",
      "-0.00017641551087810292\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.1247\u001b[0m        \u001b[32m6.2202\u001b[0m  1.7550\n",
      "      2        \u001b[36m4.3299\u001b[0m        \u001b[32m3.4647\u001b[0m  1.7432\n",
      "      3        \u001b[36m4.3274\u001b[0m        \u001b[32m2.8539\u001b[0m  1.7600\n",
      "      4        4.3771        4.7346  1.9330\n",
      "      5        \u001b[36m4.3011\u001b[0m        2.8999  1.9392\n",
      "      6        4.3136        5.2326  1.8553\n",
      "      7        \u001b[36m4.2584\u001b[0m        \u001b[32m2.0926\u001b[0m  1.7473\n",
      "      8        4.3547        3.8364  1.7401\n",
      "      9        4.3178        5.5773  1.7406\n",
      "     10        \u001b[36m4.2540\u001b[0m        8.7213  1.7361\n",
      "     11        \u001b[36m4.2472\u001b[0m        3.7472  1.7375\n",
      "     12        4.2701        6.0990  1.7480\n",
      "     13        4.2664        5.3677  1.7098\n",
      "     14        \u001b[36m4.2470\u001b[0m        4.4997  1.6910\n",
      "     15        4.2596        9.0643  1.6750\n",
      "     16        4.2802        4.3080  1.7034\n",
      "     17        \u001b[36m4.2288\u001b[0m        2.9143  1.7388\n",
      "     18        4.2619        5.4663  1.6812\n",
      "     19        4.2336        6.8370  1.6587\n",
      "     20        4.2650        4.2246  1.6638\n",
      "     21        4.2592        4.6359  1.6710\n",
      "     22        4.2476        8.5025  1.6685\n",
      "     23        4.2676        6.9531  1.6549\n",
      "     24        4.2587        4.8755  1.6672\n",
      "     25        4.2435        4.5821  1.6754\n",
      "     26        4.2355        5.2086  1.6551\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n"
     ]
    }
   ],
   "source": [
    "# This code generates positional extrapolation results using either random splits or sequential splits\n",
    "\n",
    "from utils.sklearn_utils import reset_params_skorch\n",
    "\n",
    "positional_extrapolation = np.zeros((10,6))\n",
    "\n",
    "models = [RandomForestRegressor(n_jobs=32),\n",
    "          linear_model.LinearRegression(n_jobs=32),\n",
    "          GradientBoostingRegressor(n_estimators=200,max_depth=5),\n",
    "          MLPRegressor(hidden_layer_sizes=(50,100,50),activation=\"relu\"),\n",
    "          BaggingRegressor(base_estimator=svm.SVR(),n_estimators=10,n_jobs=32),\n",
    "          NeuralNetRegressor(**{\"module\" : RNN(5,32,20,1).double(),\n",
    "                                   \"max_epochs\" : 2000,\n",
    "                                   \"lr\" : 0.05,\n",
    "                                   \"callbacks\" : [EarlyStopping(patience=20)],\n",
    "                                   \"iterator_train__shuffle\" : True,\n",
    "                                   \"batch_size\" : 128,\n",
    "                                   \"device\" : torch.device('cuda' if torch.cuda.is_available() else 'cpu')})] \n",
    "\n",
    "positional_splits = [10,20,30,40,50]\n",
    "\n",
    "for i,split in enumerate(pos_splits2):\n",
    "    train = positional2.tokenized[(positional2.indexing(positions=[x for x in range(split)]))]\n",
    "    x_train = train[:,:-1].astype(\"int\")\n",
    "    y_train = train[:,-1].astype(\"float\")\n",
    "    test = positional2.tokenized[(positional2.indexing(positions=[x for x in range(split,len(positional2.seed_seq))]))]\n",
    "    x_test = test[:,:-1].astype(\"int\")\n",
    "    y_test = test[:,-1].astype(\"float\")\n",
    "    \n",
    "    for j, model in enumerate(models):\n",
    "        if model.__class__.__name__ == \"NeuralNetRegressor\":\n",
    "            reset_params_skorch(model)\n",
    "            model.fit(x_train,y_train.reshape(-1,1))\n",
    "            positional_extrapolation[i][j] = model.score(x_test,y_test.reshape(-1,1))\n",
    "\n",
    "        else:\n",
    "            \n",
    "            model.fit(x_train,y_train)\n",
    "            print(model.score(x_test,y_test))\n",
    "            positional_extrapolation[i][j] = model.score(x_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrema Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command examples\n",
    "\n",
    "# Testing the ruggedness of a subset\n",
    "test.extrema_ruggedness_subset(test.complex_indexing(distances=[1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extrema = np.zeros((5,5))\n",
    "RS = np.zeros((5,5))\n",
    "\n",
    "for i,key in enumerate(saved_nk_landscapes.keys()):\n",
    "    for j,landscape in enumerate(saved_nk_landscapes[key]):\n",
    "        Extrema[i][j] = landscape.extrema_ruggedness\n",
    "        RS[i][j] = landscape.RS_ruggedness\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f99d6c0f0d0>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Averaged_Extrema = np.mean(Extrema, axis=1)\n",
    "Averaged_RS = np.mean(RS,axis=1)\n",
    "Averaged_RS\n",
    "\n",
    "\n",
    "sns.lineplot(x=[0,1,2,3,4],y=Averaged_Extrema)\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(x=[0,1,2,3,4],y=Averaged_RS,ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_ext = np.zeros((5,5,5))\n",
    "\n",
    "for i,key in enumerate(saved_nk_landscapes.keys()):\n",
    "    for j,landscape in enumerate(saved_nk_landscapes[key]):\n",
    "        for k,distance in enumerate([range(1,6)[:x+1] for x in range(5)]):\n",
    "            RS_ext[i,j,k] = landscape.rs_ruggedness(distance=distance)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 19.24it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 24880.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 364.07it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 46522.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8145/8145 [00:02<00:00, 3360.81it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 59532.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10461.52it/s]\n",
      " 13%|█▎        | 5515/40950 [00:00<00:00, 55145.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 58045.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 8894.09it/s] \n",
      " 11%|█         | 10638/99999 [00:00<00:01, 52662.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53955.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3786.53it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 71870.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9330.08it/s]\n",
      " 42%|████▏     | 12663/30000 [00:00<00:00, 62030.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 62226.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 8327.98it/s] \n",
      " 12%|█▏        | 5833/50000 [00:00<00:00, 58321.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 58833.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12550.87it/s]\n",
      "  8%|▊         | 5392/70000 [00:00<00:01, 53917.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 54986.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8761.60it/s] \n",
      "  6%|▌         | 5309/90000 [00:00<00:01, 53085.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53866.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 18.94it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29883.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 367.63it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 52894.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3341.80it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 57067.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10331.92it/s]\n",
      " 27%|██▋       | 11005/40950 [00:00<00:00, 54746.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 57595.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:09<00:00, 10354.50it/s]\n",
      "  5%|▌         | 5192/99999 [00:00<00:01, 51918.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 55461.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3800.34it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69855.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9367.22it/s]\n",
      " 21%|██        | 6349/30000 [00:00<00:00, 63489.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65552.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 8257.23it/s] \n",
      " 11%|█         | 5602/50000 [00:00<00:00, 56013.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 57035.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 7872.70it/s] \n",
      "  8%|▊         | 5592/70000 [00:00<00:01, 55918.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 56927.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8481.84it/s] \n",
      " 12%|█▏        | 11237/90000 [00:00<00:01, 56238.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 57276.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 18.70it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 30097.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 359.49it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 44911.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3340.22it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 58456.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10294.35it/s]\n",
      " 13%|█▎        | 5472/40950 [00:00<00:00, 54711.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 57096.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 8844.97it/s] \n",
      " 11%|█         | 11081/99999 [00:00<00:01, 55359.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 55476.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3802.15it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69858.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9244.47it/s]\n",
      " 21%|██▏       | 6375/30000 [00:00<00:00, 63741.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63793.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11801.65it/s]\n",
      " 12%|█▏        | 5936/50000 [00:00<00:00, 59357.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 58907.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 8002.72it/s] \n",
      " 16%|█▌        | 10954/70000 [00:00<00:01, 54799.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 55282.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12585.56it/s]\n",
      "  6%|▋         | 5790/90000 [00:00<00:01, 57897.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 55399.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 18.95it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 27196.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 366.86it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 44776.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3351.54it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56875.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10346.97it/s]\n",
      " 13%|█▎        | 5358/40950 [00:00<00:00, 53577.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 55870.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12957.57it/s]\n",
      "  5%|▌         | 5370/99999 [00:00<00:01, 53690.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53886.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3791.57it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68425.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9450.61it/s]\n",
      " 22%|██▏       | 6577/30000 [00:00<00:00, 65762.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 66171.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11889.31it/s]\n",
      " 24%|██▍       | 12129/50000 [00:00<00:00, 60713.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 60598.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 8108.33it/s] \n",
      "  8%|▊         | 5786/70000 [00:00<00:01, 57857.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 57832.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12559.99it/s]\n",
      "  6%|▌         | 5573/90000 [00:00<00:01, 55720.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 55642.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.45it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 51639.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 216.74it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 47635.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1950.06it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56313.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:07<00:00, 5669.77it/s] \n",
      " 26%|██▌       | 10673/40950 [00:00<00:00, 53184.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 55326.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12964.15it/s]\n",
      "  5%|▌         | 5360/99999 [00:00<00:01, 53595.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53422.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3850.83it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68764.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9164.76it/s]\n",
      " 21%|██        | 6340/30000 [00:00<00:00, 63397.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63561.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11883.72it/s]\n",
      " 12%|█▏        | 5995/50000 [00:00<00:00, 59940.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 58178.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 7808.86it/s] \n",
      "  8%|▊         | 5630/70000 [00:00<00:01, 56294.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 55861.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12554.18it/s]\n",
      "  6%|▌         | 5149/90000 [00:00<00:01, 51488.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53376.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.59it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29611.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 218.98it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 42857.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 2022.55it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56688.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:06<00:00, 5873.50it/s] \n",
      " 26%|██▌       | 10631/40950 [00:00<00:00, 52881.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54780.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12851.86it/s]\n",
      "  5%|▌         | 5207/99999 [00:00<00:01, 52065.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52865.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2358.33it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69046.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:06<00:00, 4710.35it/s]\n",
      " 42%|████▏     | 12671/30000 [00:00<00:00, 63422.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63797.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11935.31it/s]\n",
      " 12%|█▏        | 5862/50000 [00:00<00:00, 58612.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 59018.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:07<00:00, 9509.56it/s] \n",
      "  8%|▊         | 5603/70000 [00:00<00:01, 56029.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 56226.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12579.23it/s]\n",
      " 12%|█▏        | 10855/90000 [00:00<00:01, 54308.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54569.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.34it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29500.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 217.94it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 47482.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 2002.45it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 54323.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7130.17it/s] \n",
      " 26%|██▌       | 10521/40950 [00:00<00:00, 52365.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54199.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12906.47it/s]\n",
      "  5%|▌         | 5282/99999 [00:00<00:01, 52816.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53214.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2307.15it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 67722.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:06<00:00, 4659.60it/s]\n",
      " 43%|████▎     | 12850/30000 [00:00<00:00, 64321.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 64016.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11833.87it/s]\n",
      " 12%|█▏        | 5877/50000 [00:00<00:00, 58764.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 59428.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12557.79it/s]\n",
      "  8%|▊         | 5617/70000 [00:00<00:01, 56160.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 56063.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8541.64it/s] \n",
      " 12%|█▏        | 10737/90000 [00:00<00:01, 53607.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54128.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.38it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 27891.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 215.90it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 54355.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1982.35it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 57936.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7117.13it/s] \n",
      " 14%|█▎        | 5552/40950 [00:00<00:00, 55519.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 58107.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 13014.10it/s]\n",
      "  6%|▌         | 5543/99999 [00:00<00:01, 55427.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 55354.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2389.80it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 66719.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5907.54it/s]\n",
      " 22%|██▏       | 6633/30000 [00:00<00:00, 66326.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 66198.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11970.89it/s]\n",
      " 12%|█▏        | 6216/50000 [00:00<00:00, 62156.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 62014.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12538.28it/s]\n",
      " 16%|█▌        | 11326/70000 [00:00<00:01, 56671.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 57267.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:09<00:00, 9957.18it/s] \n",
      "  6%|▋         | 5632/90000 [00:00<00:01, 56316.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 56996.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.54it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29611.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 218.62it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 48304.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1971.40it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 55952.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7027.36it/s] \n",
      " 13%|█▎        | 5437/40950 [00:00<00:00, 54362.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 55233.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 9052.49it/s] \n",
      "  5%|▌         | 5170/99999 [00:00<00:01, 51698.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52537.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2354.36it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 66273.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5823.37it/s]\n",
      " 21%|██        | 6305/30000 [00:00<00:00, 63044.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 64333.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6870.79it/s] \n",
      " 12%|█▏        | 5906/50000 [00:00<00:00, 59056.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 59755.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12469.25it/s]\n",
      "  8%|▊         | 5947/70000 [00:00<00:01, 59466.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 59417.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8528.82it/s] \n",
      "  6%|▌         | 5560/90000 [00:00<00:01, 55590.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 56980.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.54it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29167.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 219.73it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 46325.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 2025.28it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 58258.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7074.74it/s] \n",
      " 27%|██▋       | 10980/40950 [00:00<00:00, 54661.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 56932.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:10<00:00, 9206.97it/s] \n",
      "  5%|▌         | 5334/99999 [00:00<00:01, 53330.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53439.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2368.38it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69828.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5981.90it/s]\n",
      " 20%|█▉        | 5938/30000 [00:00<00:00, 59373.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 62957.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 7615.49it/s] \n",
      " 12%|█▏        | 5967/50000 [00:00<00:00, 59669.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 59378.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12454.11it/s]\n",
      " 16%|█▌        | 11160/70000 [00:00<00:01, 55898.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 56318.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8643.31it/s] \n",
      "  6%|▌         | 5388/90000 [00:00<00:01, 53879.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53776.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.55it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29541.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 855/855 [00:03<00:00, 215.93it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 46360.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 2005.46it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56814.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7148.82it/s] \n",
      " 12%|█▏        | 5069/40950 [00:00<00:00, 50688.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54725.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:09<00:00, 10435.56it/s]\n",
      "  5%|▌         | 5227/99999 [00:00<00:01, 52267.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53262.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2344.29it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68485.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5896.50it/s]\n",
      " 22%|██▏       | 6475/30000 [00:00<00:00, 64748.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65226.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6750.87it/s] \n",
      " 11%|█▏        | 5745/50000 [00:00<00:00, 57447.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 58516.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12452.23it/s]\n",
      " 16%|█▌        | 11055/70000 [00:00<00:01, 55407.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 55362.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8540.31it/s] \n",
      " 12%|█▏        | 10650/90000 [00:00<00:01, 53324.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54214.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.59it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 30179.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 217.07it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 45622.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1984.76it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 55836.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7108.60it/s] \n",
      " 26%|██▌       | 10617/40950 [00:00<00:00, 52904.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54841.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:09<00:00, 10219.67it/s]\n",
      " 10%|▉         | 9997/99999 [00:00<00:01, 50040.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53395.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2346.02it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 72265.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5967.83it/s]\n",
      " 22%|██▏       | 6465/30000 [00:00<00:00, 64644.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65802.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6748.52it/s] \n",
      " 24%|██▍       | 11987/50000 [00:00<00:00, 60163.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 62041.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12416.94it/s]\n",
      " 17%|█▋        | 11720/70000 [00:00<00:00, 58636.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 58041.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8617.58it/s] \n",
      "  6%|▌         | 5356/90000 [00:00<00:01, 53550.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53947.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.50it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 56190.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 215.82it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 46595.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1985.92it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56646.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7067.59it/s] \n",
      " 13%|█▎        | 5243/40950 [00:00<00:00, 52424.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54544.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:10<00:00, 9138.19it/s] \n",
      "  5%|▌         | 5216/99999 [00:00<00:01, 52155.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 53034.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2348.53it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 70227.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5889.81it/s]\n",
      " 22%|██▏       | 6728/30000 [00:00<00:00, 67277.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 67720.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6656.59it/s] \n",
      " 13%|█▎        | 6257/50000 [00:00<00:00, 62564.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61929.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12488.64it/s]\n",
      "  8%|▊         | 5857/70000 [00:00<00:01, 58560.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 59221.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8612.98it/s] \n",
      " 12%|█▏        | 10689/90000 [00:00<00:01, 53528.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54222.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.31it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29546.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 217.15it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 43400.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1961.92it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56487.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7076.91it/s] \n",
      " 25%|██▌       | 10418/40950 [00:00<00:00, 51594.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 53689.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 9076.79it/s] \n",
      " 10%|█         | 10283/99999 [00:00<00:01, 50890.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52875.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2310.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68885.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5934.30it/s]\n",
      " 22%|██▏       | 6495/30000 [00:00<00:00, 64947.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63776.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6690.69it/s] \n",
      " 24%|██▍       | 11955/50000 [00:00<00:00, 59801.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 60015.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12487.36it/s]\n",
      "  8%|▊         | 5768/70000 [00:00<00:01, 57678.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 57489.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8596.53it/s] \n",
      "  6%|▌         | 5500/90000 [00:00<00:01, 54998.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54849.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.40it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 33014.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 216.08it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 46918.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1970.70it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56299.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7116.74it/s] \n",
      " 26%|██▌       | 10443/40950 [00:00<00:00, 51923.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54564.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 8921.70it/s] \n",
      " 10%|█         | 10362/99999 [00:00<00:01, 51794.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52915.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2351.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69286.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5909.65it/s]\n",
      " 20%|██        | 6023/30000 [00:00<00:00, 60223.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 62081.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6674.24it/s] \n",
      " 11%|█         | 5496/50000 [00:00<00:00, 54957.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 55301.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12486.44it/s]\n",
      "  8%|▊         | 5611/70000 [00:00<00:01, 56103.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 55148.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8805.75it/s] \n",
      "  6%|▌         | 4980/90000 [00:00<00:01, 49798.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54014.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 19.09it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 24916.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 358.78it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 48033.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3372.82it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 55978.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7063.65it/s] \n",
      " 25%|██▍       | 10184/40950 [00:00<00:00, 51214.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 53551.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 8891.16it/s] \n",
      "  5%|▌         | 5228/99999 [00:00<00:01, 52270.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52359.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2351.70it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 67544.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5951.63it/s]\n",
      " 22%|██▏       | 6459/30000 [00:00<00:00, 64586.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63563.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6550.62it/s] \n",
      " 12%|█▏        | 5830/50000 [00:00<00:00, 58299.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 57942.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12499.73it/s]\n",
      "  8%|▊         | 5492/70000 [00:00<00:01, 54915.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 55106.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8842.45it/s] \n",
      " 12%|█▏        | 10707/90000 [00:00<00:01, 53622.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53187.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 19.26it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 54740.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 855/855 [00:02<00:00, 358.30it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 43502.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3330.00it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 57699.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7074.58it/s] \n",
      " 13%|█▎        | 5418/40950 [00:00<00:00, 54177.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 56255.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:09<00:00, 10396.21it/s]\n",
      "  5%|▌         | 5378/99999 [00:00<00:01, 53772.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 55308.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2366.81it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 71042.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5951.29it/s]\n",
      " 22%|██▏       | 6527/30000 [00:00<00:00, 65265.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 66250.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6948.81it/s] \n",
      " 12%|█▏        | 6119/50000 [00:00<00:00, 61188.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61438.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12359.34it/s]\n",
      "  8%|▊         | 5543/70000 [00:00<00:01, 55423.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 58635.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8503.55it/s] \n",
      "  6%|▌         | 5546/90000 [00:00<00:01, 55450.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 56311.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 19.37it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 55496.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 361.13it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 44374.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3377.05it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 58551.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10464.84it/s]\n",
      " 27%|██▋       | 10985/40950 [00:00<00:00, 54559.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 56762.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:09<00:00, 10299.57it/s]\n",
      "  5%|▌         | 5210/99999 [00:00<00:01, 52095.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 55511.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3834.28it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 72441.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5892.15it/s]\n",
      " 21%|██        | 6313/30000 [00:00<00:00, 63126.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65926.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6679.00it/s] \n",
      " 25%|██▍       | 12484/50000 [00:00<00:00, 62214.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61858.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12647.13it/s]\n",
      " 17%|█▋        | 11752/70000 [00:00<00:00, 58957.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 58374.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8476.27it/s] \n",
      " 12%|█▏        | 11117/90000 [00:00<00:01, 55581.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54477.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 18.82it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 25253.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 364.24it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 42759.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3321.10it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56625.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10321.44it/s]\n",
      " 27%|██▋       | 11044/40950 [00:00<00:00, 54902.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 57118.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 8957.79it/s] \n",
      "  5%|▌         | 5151/99999 [00:00<00:01, 51506.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 54504.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3869.81it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 73411.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9151.23it/s]\n",
      " 22%|██▏       | 6561/30000 [00:00<00:00, 65601.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 62315.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 8282.23it/s] \n",
      " 24%|██▍       | 12022/50000 [00:00<00:00, 59161.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61727.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 8157.43it/s] \n",
      "  8%|▊         | 5839/70000 [00:00<00:01, 58384.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 58766.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8737.46it/s] \n",
      " 12%|█▏        | 10797/90000 [00:00<00:01, 53825.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 52598.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 18.93it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 28284.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 855/855 [00:02<00:00, 360.74it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 43338.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3331.22it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 54680.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10406.58it/s]\n",
      " 27%|██▋       | 11034/40950 [00:00<00:00, 54866.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 56903.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:09<00:00, 10406.06it/s]\n",
      "  5%|▌         | 5286/99999 [00:00<00:01, 52857.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52954.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3884.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 70369.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9028.75it/s]\n",
      " 21%|██        | 6335/30000 [00:00<00:00, 63345.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63377.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:06<00:00, 8305.93it/s] \n",
      " 12%|█▏        | 5847/50000 [00:00<00:00, 58460.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 59648.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 7905.87it/s] \n",
      "  8%|▊         | 5644/70000 [00:00<00:01, 56436.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 55552.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8557.89it/s] \n",
      " 12%|█▏        | 10712/90000 [00:00<00:01, 53522.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 54339.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 19.17it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 29514.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 855/855 [00:02<00:00, 362.25it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 43750.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3339.68it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 54934.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10400.69it/s]\n",
      " 13%|█▎        | 5197/40950 [00:00<00:00, 51965.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54824.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:11<00:00, 9042.02it/s] \n",
      "  5%|▌         | 5036/99999 [00:00<00:01, 50351.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52203.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3865.55it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 68317.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9338.51it/s]\n",
      " 43%|████▎     | 12902/30000 [00:00<00:00, 63352.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65798.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11732.49it/s]\n",
      " 25%|██▌       | 12663/50000 [00:00<00:00, 63269.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61342.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:07<00:00, 9278.97it/s] \n",
      "  8%|▊         | 5663/70000 [00:00<00:01, 56621.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 56746.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12409.25it/s]\n",
      "  6%|▌         | 5459/90000 [00:00<00:01, 54582.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53988.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:02<00:00, 18.85it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 30059.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:02<00:00, 361.67it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 48084.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:02<00:00, 3390.40it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 54959.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:03<00:00, 10333.06it/s]\n",
      " 13%|█▎        | 5211/40950 [00:00<00:00, 52104.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54827.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12864.13it/s]\n",
      "  5%|▍         | 4956/99999 [00:00<00:01, 49553.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52078.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3823.00it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 64585.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9091.39it/s]\n",
      " 44%|████▍     | 13156/30000 [00:00<00:00, 65934.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65846.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 12004.65it/s]\n",
      " 12%|█▏        | 6113/50000 [00:00<00:00, 61122.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61048.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:08<00:00, 8476.76it/s] \n",
      "  8%|▊         | 5840/70000 [00:00<00:01, 58397.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 59614.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12589.18it/s]\n",
      "  6%|▌         | 5521/90000 [00:00<00:01, 55207.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 56860.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:04<00:00, 11.24it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 30049.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855/855 [00:03<00:00, 217.77it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 42413.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1951.44it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 57139.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:06<00:00, 5857.32it/s] \n",
      " 13%|█▎        | 5356/40950 [00:00<00:00, 53556.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 56046.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12836.21it/s]\n",
      "  6%|▌         | 5541/99999 [00:00<00:01, 55407.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 55351.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3825.29it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 73108.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:03<00:00, 9138.93it/s]\n",
      " 22%|██▏       | 6677/30000 [00:00<00:00, 66766.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 67243.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11874.13it/s]\n",
      " 24%|██▍       | 12211/50000 [00:00<00:00, 61105.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 61889.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:07<00:00, 9397.27it/s] \n",
      " 17%|█▋        | 11594/70000 [00:00<00:01, 58042.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 58065.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:07<00:00, 12491.95it/s]\n",
      "  6%|▌         | 5150/90000 [00:00<00:01, 51495.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 53245.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.42it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 28032.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 855/855 [00:04<00:00, 212.71it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 46761.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1996.42it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 56280.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7112.27it/s] \n",
      " 13%|█▎        | 5251/40950 [00:00<00:00, 52507.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 54786.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12853.35it/s]\n",
      "  5%|▌         | 5236/99999 [00:00<00:01, 52351.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 52354.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2289.70it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 69689.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:06<00:00, 4602.55it/s]\n",
      " 20%|█▉        | 5972/30000 [00:00<00:00, 59713.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 63698.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:04<00:00, 11894.83it/s]\n",
      " 12%|█▏        | 5953/50000 [00:00<00:00, 59521.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 60218.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12409.86it/s]\n",
      "  9%|▊         | 5959/70000 [00:00<00:01, 59587.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 60313.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8614.88it/s] \n",
      "  6%|▌         | 5300/90000 [00:00<00:01, 52995.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 56554.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "Building Protein Graph For subset of length 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.45it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 27776.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n",
      "Building Protein Graph For subset of length 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 855/855 [00:03<00:00, 214.85it/s]\n",
      "100%|██████████| 855/855 [00:00<00:00, 52227.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8145/8145 [00:04<00:00, 1961.41it/s]\n",
      "100%|██████████| 8145/8145 [00:00<00:00, 57666.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 40950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:05<00:00, 7040.60it/s] \n",
      " 13%|█▎        | 5420/40950 [00:00<00:00, 54191.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40950/40950 [00:00<00:00, 56365.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 99999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:07<00:00, 12907.32it/s]\n",
      "  5%|▌         | 5447/99999 [00:00<00:01, 54461.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99999/99999 [00:01<00:00, 54611.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2343.28it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 72669.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:05<00:00, 5990.55it/s]\n",
      " 22%|██▏       | 6521/30000 [00:00<00:00, 65209.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 65697.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6912.03it/s] \n",
      " 13%|█▎        | 6261/50000 [00:00<00:00, 62605.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 62994.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:05<00:00, 12455.94it/s]\n",
      "  8%|▊         | 5851/70000 [00:00<00:01, 58499.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 58868.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Protein Graph For subset of length 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8682.79it/s] \n",
      "  6%|▋         | 5636/90000 [00:00<00:01, 56357.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the number of extrema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:01<00:00, 56723.67it/s]\n"
     ]
    }
   ],
   "source": [
    "Extrema_ext = np.zeros((5,5,5))\n",
    "Extrema_abl = np.zeros((5,5,5))\n",
    "\n",
    "for i,key in enumerate(saved_nk_landscapes.keys()):\n",
    "    for j,landscape in enumerate(saved_nk_landscapes[key]):\n",
    "        print(i,j)\n",
    "        for k,distance in enumerate([range(1,6)[:x+1] for x in range(5)]):\n",
    "            Extrema_ext[i,j,k] = landscape.extrema_ruggedness_subset(landscape.complex_indexing(distances=distance))\n",
    "        for k,split in enumerate([0.1,0.3,0.5,0.7,0.9]):\n",
    "            Extrema_abl[i,j,k] = landscape.extrema_ruggedness_subset(landscape.complex_indexing(percentage=split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Averaged_RS_ext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-98a7ef316627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m arrays = {\"RS Extrapolation\" : Averaged_RS_ext,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0;34m\"RS Ablation\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mAveraged_RS_abl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0;34m\"Extrema Extrapolation\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mAveraged_Extrema_ext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \"Extrema Ablation\" : Averaged_Extrema_abl}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Averaged_RS_ext' is not defined"
     ]
    }
   ],
   "source": [
    "arrays = {\"RS Extrapolation\" : Averaged_RS_ext,\n",
    "          \"RS Ablation\" : Averaged_RS_abl,\n",
    "          \"Extrema Extrapolation\" : Averaged_Extrema_ext,\n",
    "          \"Extrema Ablation\" : Averaged_Extrema_abl}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11.03609536,   15.29297151,   18.68143677,   22.11676113,\n",
       "          25.92917313],\n",
       "       [   8.36815024,   16.31921791,   27.91952318,   45.32793568,\n",
       "          71.56736098],\n",
       "       [  13.67354955,   31.41834941,   63.6470313 ,  106.10157244,\n",
       "         113.906533  ],\n",
       "       [   8.3798377 ,   34.1036536 ,  123.00580177,  327.82261236,\n",
       "         358.7484147 ],\n",
       "       [  13.10992022,  126.46059133,  404.26737559, 1283.22621804,\n",
       "        1381.57934441]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Averaged_RS_ext = np.mean(RS_ext,axis=1)\n",
    "Averaged_RS_abl = np.load(\"RS Ablation.npy\")\n",
    "Averaged_Extrema_ext = np.load(\"Extrema Extrapolation.npy\")\n",
    "Averaged_Extrema_abl = np.load(\"Extrema Ablation.npy\")\n",
    "Averaged_RS_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns ; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "RS_experimental = {key : np.log10(value[0].RS_ruggedness) for key, value in experimental_dict.items()}\n",
    "Extrema_experimental = {key : value[0].extrema_ruggedness for key, value in experimental_dict.items()}\n",
    "\n",
    "labels = [\"GB1-2 ($M_1$)\", \"GB1-2 ($M_1$ - $M_2$)\", \"GB1-4\", \"BLac\"]\n",
    "RS_values = [np.log10(value[0].RS_ruggedness) for value in experimental_dict.values()]\n",
    "Extrema_Values = [np.log10(value[0].extrema_ruggedness) for value in experimental_dict.values()]\n",
    "\n",
    "\n",
    "Averaged_RS_ext = np.mean(RS_ext,axis=1)\n",
    "Averaged_RS_abl = np.load(\"RS Ablation.npy\")\n",
    "Averaged_Extrema_ext = np.load(\"Extrema Extrapolation.npy\")\n",
    "Averaged_Extrema_abl = np.load(\"Extrema Ablation.npy\")\n",
    "\n",
    "arrays = {\"Averaged RS\" : (Averaged_RS, [0,1,2,3,4]),\n",
    "          \"RS Extrapolation\" : (Averaged_RS_ext, [\"$M_{1-1}$\",\"$M_{1-2}$\",\"$M_{1-3}$\",\"$M_{1-4}$\",\"$M_{1-5}$\"]),\n",
    "          \"RS Ablation\" : (Averaged_RS_abl, [\"10%\",\"30%\",\"50%\",\"70%\",\"90%\"]),\n",
    "          \"Experimental RS\" : (RS_values, labels),\n",
    "          \"Averaged Extrema\" : (Averaged_Extrema, [0,1,2,3,4]),\n",
    "          \"Extrema Extrapolation\" : (Averaged_Extrema_ext, [\"$M_{1-1}$\",\"$M_{1-2}$\",\"$M_{1-3}$\",\"$M_{1-4}$\",\"$M_{1-5}$\"]),\n",
    "          \"Extrema Ablation\" : (Averaged_Extrema_abl, [\"10%\",\"30%\",\"50%\",\"70%\",\"90%\"]),\n",
    "          \"Experimental Extrema\" : (Extrema_Values, labels)  }\n",
    "\n",
    "\n",
    "fig, ((ax1, ax2, ax3,ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(2, 4, figsize=(16,8))\n",
    "fig.subplots_adjust(right=1.2)\n",
    "axes = [ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8]\n",
    "i=0\n",
    "for name, result in arrays.items():\n",
    "    if name == \"Experimental RS\":\n",
    "        sns.barplot(x=result[1],y=result[0],ax=axes[i])\n",
    "        sns.despine(offset=10, trim=True)\n",
    "        axes[i].set_xticklabels([])\n",
    "        axes[i].set_ylim(-12,2)\n",
    "        axes[i].set_ylabel(\"$\\log({r/s})$\")\n",
    "        axes[i].set_yticks([-12,-10,-8,-6,-4,-2,0,2])\n",
    "    elif name == \"Experimental Extrema\":\n",
    "        sns.barplot(x=result[1],y=result[0],ax=axes[i])\n",
    "        sns.despine(offset=10, trim=True)\n",
    "        axes[i].set_ylim(-3,0)\n",
    "        axes[i].set_ylabel(\"$\\log({extrema})$\")\n",
    "        axes[i].set_yticks([-3,-2,-1,0])\n",
    "        plt.xticks(rotation=45)\n",
    "    else:\n",
    "        sns.lineplot(data=result[0].T,ax=axes[i])\n",
    "        sns.despine(offset=10, trim=True)\n",
    "        axes[i].set_xticks(range(len(result[1])))\n",
    "        axes[i].set_xticklabels(result[1])\n",
    "        if \"Averaged RS\" == name:\n",
    "            axes[i].set_ylim(0,1400)\n",
    "            axes[i].set_yticks([0,200,400,600,800,1000,1200,1400])\n",
    "        elif \"RS\" in name:\n",
    "            axes[i].legend(title=\"K\")\n",
    "            axes[i].set_ylim(0,1400)\n",
    "            axes[i].set_yticks([0,200,400,600,800,1000,1200,1400])\n",
    "        elif name == \"Averaged Extrema\":\n",
    "            axes[i].set_ylim(0,0.05)\n",
    "            axes[i].set_yticks([0,0.01,0.02,0.03,0.04,0.05])\n",
    "        elif name == \"Extrema Extrapolation\":\n",
    "            axes[i].legend(title=\"K\")\n",
    "            axes[i].set_ylim(0,0.25)\n",
    "            axes[i].set_yticks([0,0.05,0.1,0.15,0.2,0.25])\n",
    "        elif name == \"Extrema Ablation\":\n",
    "            axes[i].legend(title=\"K\")\n",
    "            axes[i].set_ylim(0,0.55)\n",
    "            axes[i].set_yticks([0,0.1,0.2,0.3,0.4,0.5])\n",
    "    i+=1\n",
    "plt.savefig(\"Ruggedness_Estimators_New.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.9492760788966276\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.948480970373267\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.9475203921577834\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.9499373195067622\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.9481027615516094\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.9487759736127652\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.9484886903706267\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.9485592678008409\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.9490173232397501\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.9476126434752431\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.9361807358550167\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.9365457020256509\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.9347695239837269\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.934738603986358\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.9359079705607232\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.9351367103180416\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.933867419414008\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.9331535751549811\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.933860804394858\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.9350071835159216\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.9840062223154744\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.9831921818220208\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.983220495000027\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.9824628921240853\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.9826900160897174\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.9830014322102902\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.9833963348941909\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.9831594258342721\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.9843435483598884\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.9835599332910919\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.9879724068348906\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.9866502165474079\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.9877197482223307\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.9876216664987476\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.9873358075447042\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.9874802032105914\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.9873747077321346\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.9870134067333473\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.9874459263366723\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.9870525181027684\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.9683056760251915\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.9689602058943665\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.9680289584559646\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.9676486624586428\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.9680618997120468\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.9684033651989321\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.9687045872182574\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.9684828080596777\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.9689058151440268\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.9678075807125847\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.8226994325466461\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.8272602841850931\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.8348875326790023\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.8401707637001605\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.8358300774875768\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.8311195281846646\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.8347995688911507\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.8295575053384484\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.8302675786899767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.8314872756108453\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.8175098552756491\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.8211190165557961\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.8161115331630449\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.8214869587547888\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.8216959527731548\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.8193307109728233\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.8202734716068235\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.8191754651291645\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.8165677423047759\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.8172929237532822\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.8986587671223014\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.8955803621049014\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.9022352986993988\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.896861218382952\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.8976117913875854\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.8984019641488558\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.8957800167543684\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.8964659242436332\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.9046642471267586\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.9028123139027604\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.8949030886664703\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.8980766034649691\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.8956288864989779\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.8948071828272292\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.8962413534139055\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.89776292679926\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.8971720224162808\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.898059698219745\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.8954898671129057\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.8958076619535228\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.9131478569092459\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.9119906334516523\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.9130463232718062\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.911754796284355\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.9134074827365648\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.9100919459217697\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.9090559683067514\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.9116442814961834\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.9096829870523748\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.911950232108361\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.8784981023902498\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.8832411334225311\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.884081710562601\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.8815203253015215\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.8804697935159774\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.880386579647539\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.8811802553451591\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.8809753709940515\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.8790921426681895\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.8818436401096349\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.6600979992351139\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.6675719873301627\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.6680440183218828\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.6696095900327903\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.6688313610893044\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.6690682823901599\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.6648915159500995\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.6673227910795752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.6681123614766002\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.6683900567715029\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.689010678625835\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.6911396682068287\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.6870648672813793\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.6861381317620308\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.6923222182070444\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.6854916745424757\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.6855064240725467\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.6799086374988463\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.6881914707943574\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.6882987163075889\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.6749532779939689\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.6742640001742525\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.6686661812501918\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.6645862442627183\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.6729277442845584\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.6703248118887433\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.6701192494677726\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.6669854990597672\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.6750685219116367\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.6620324126687793\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.8807366956413057\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.8835807941233322\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.8824354487619857\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.882623585598647\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.8831299928306522\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.8820816606430364\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.8848420556012789\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.8829775332788822\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.8842328145653737\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.8864547701118887\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.4430153782419681\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.44153579867130677\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.4346082390446847\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.4373472202703589\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.4399768457561809\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.4477579506889653\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.44634285864969336\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.451961114592176\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.4341946792571306\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.44250045442695785\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.474249877101824\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.48209338292712156\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.4637081322169244\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.471916124257279\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.47143846224953967\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.47567875979721497\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.47016572033475745\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.4683966460395208\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.47346861308642174\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.4722925555708001\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.43301936295712407\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.4206864000309142\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.43872365760340937\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.43037584111571003\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.41839696749900723\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.42797353912070113\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 70, RF returned an R-squared of 0.39969186075500907\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.43439233842917535\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.4341821900804028\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.42562900453775376\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.41886700502359364\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.4046854446116894\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.4136138389501046\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.4209900530872025\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.42529287093298007\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.4118658336601402\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.4027622237566415\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.3974033789634951\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.4169461354358669\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.3994067429533754\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of 0.7405351990579256\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of 0.7316643116345065\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of 0.7358222262842775\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of 0.7371775402994933\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of 0.7413134751890167\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of 0.723549823473975\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of 0.7282571254938371\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of 0.7407484492549359\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of 0.738831335362434\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of 0.737631640584538\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of -0.13864884947957035\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of -0.13928570341471347\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of -0.1340392545672664\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of -0.1348497036949805\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of -0.1359705131746225\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of -0.1372827161125203\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of -0.13361383458001153\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of -0.13613111725330374\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of -0.13208335158599893\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of -0.1341911140942036\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of -0.13864970491058903\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of -0.14014369924949688\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of -0.13870300011908832\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of -0.13984781198769203\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of -0.1387186703429375\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of -0.1358285789973026\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of -0.12941247801197298\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of -0.1338650548448015\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of -0.14165044342417654\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of -0.14505937622721032\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of -0.13379159818502306\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of -0.13463142654809812\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of -0.13646345507338187\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of -0.1408099409779311\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of -0.14051356205695442\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of -0.1377662755394986\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of -0.14102511790212335\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of -0.1439898715476109\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of -0.13994335915224143\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of -0.13686494028844698\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of -0.13860689358420375\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of -0.14297950030737105\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of -0.13709685960551865\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of -0.13637857513982188\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of -0.14196478450131345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of -0.14027468417491984\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of -0.13868337911990336\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of -0.14483982041410015\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of -0.13770084390872928\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of -0.13388878006070493\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, RF returned an R-squared of -0.13334614569687497\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, RF returned an R-squared of -0.12453661120126558\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, RF returned an R-squared of -0.1262085158377504\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, RF returned an R-squared of -0.12847361847033079\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, RF returned an R-squared of -0.14058433450036412\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, RF returned an R-squared of -0.1355324451474902\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, RF returned an R-squared of -0.1381913437534701\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, RF returned an R-squared of -0.1308301075241567\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, RF returned an R-squared of -0.12897097190095486\n",
      "Training model RandomForestRegressor(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, RF returned an R-squared of -0.12602947671709064\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.06349945884083352\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.05767808604445768\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.06440521649829889\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.06115381100810957\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.058345560529085794\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.058538566119453295\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.06206986136880255\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.06440730017799279\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.06281049207469047\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.06210905157636115\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.04099033277432995\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.046376624552607715\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.04646267645434299\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0432075069060468\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.04451676807289551\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.04598882475353394\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.042684507612923084\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.04745133875814844\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.03946153220379145\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.04540057121154428\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.23095851743820073\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.22351787420505098\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.2237142444819532\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.2274551468230369\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.21258430624188107\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.2135665030904803\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.2154061080361478\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.2170619984580746\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.22238896870560465\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.22650218485876605\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.1963788105138543\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.203462070078272\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.1956759538414603\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.20343953997814956\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.18863303132864273\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.1989898437050952\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.1906707464759302\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.19678317649737653\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.19634359380116806\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.20478189894849208\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.04298139291336567\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.04371925238866392\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.04229360394151793\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.045688142508355956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.0453638961423517\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.04410320202373463\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.04523846646436658\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.043628656875762006\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.04158621257495998\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.04615391413049952\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.006843546759548569\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.007840416356697966\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.010487608198086473\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.008502279149520287\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.008235266284900677\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.008208076679004583\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.006046776301635637\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.008665852930753504\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.00886538643255197\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.005836492936669768\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.006153910948413888\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.007010170449065378\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.006812408554883742\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0067158253717386485\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.005244757548442225\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.003815210708150274\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.0067882276317414725\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.005706163762630334\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.005400486029359275\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.007621689295491563\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.020763315020146056\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.019193920668820974\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.020602700444593447\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.018898924579441756\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.022482474808605102\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.01783079597945425\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.023421612509924028\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.019748869129212365\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.022402978318453615\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.021232440831378985\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.010072111868573819\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.011446139887449025\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.007080079032002917\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0071048294645761345\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.008326548543690304\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.007755624552495344\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.007453594505549277\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.008206938478249048\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.0077361283444038476\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.007977160290068008\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.047340427334724766\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.044081278580138106\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0441694716886778\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.04729867150024436\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.05177037172996357\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.04589218485740931\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.04532211344903703\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.0405097806548711\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.04811360971886591\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.04841089805522325\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.001994542126663501\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.0021638020065626318\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0008219461131284689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0018753157005336751\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.0019957426248156107\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.002378587705287316\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.002194772234097475\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.0020803616453394236\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.00264616247700189\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.0018287633658681246\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.008742455733717014\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.009186351569290707\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.010422997741055928\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.008506532855473092\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.011058712900849477\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.010671294690350996\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.008602457717375778\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.009291003378863905\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.007838667636495544\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.010829607775742156\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.008583245139426099\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.009953319525533466\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.009890805494215282\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.008461126120914764\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.007725574284499159\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.010245498488306803\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.009000412536753877\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.009892799549905074\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.008294426509901531\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.007759559697691487\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.0034691900145242816\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.002487118401321209\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0036072918102576512\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0021822745522382947\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.0040398300502829665\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.0033666422537875773\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.0016895199088307056\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.0026120463049651432\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.0034710478450186155\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.002443100186192426\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.006326575121047173\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.004615867529548234\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0050422905583890065\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.007410191217331885\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.0039604882714161516\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.005491627775759178\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.004574526824038694\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.004926799250073333\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.004732974648728683\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.0060708831467801705\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -0.00012418346006759506\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.00043511831427367476\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0007488510467661413\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0007125869108175475\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.0006984686513175076\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.0008325817217487153\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.0004122299332290913\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.0004974538369089121\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 3.9797876784763986e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 8.444680045127662e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -0.0005590025805748411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.0005007742544488325\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0006357715202320868\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.0008496446031303062\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.000832918364639279\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 3.5869546713418465e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.00024832018525977606\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.0005884851044100081\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.0005953774414012569\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.0013534203709261172\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -0.0004104574126158056\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of -9.73652191960106e-06\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of -0.00015980128522996573\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 6.379803210321278e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of -8.032535811652863e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.00017699377763957802\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 4.960589033498053e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of -5.376999467610055e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of 0.00010978695619301426\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 5.992987842051711e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.0005091726760682036\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 0.00015254587102886408\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 0.0003222620174897495\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of 0.00038793043066498445\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of -0.00024183341637096234\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.0005676662984962499\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of -0.00023522835979061796\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 0.00018401334441364714\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -0.0002472814242051591\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 0.00018932137034310959\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -3.399705080520121e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of -7.478910351244039e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of -0.00030048673047078367\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of -0.00010619580686044294\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 0.0003163812799937826\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 0.00016832817097822694\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of 0.00031856246811168276\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of -0.00011650015701114569\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -6.650853927991918e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 9.156499830098586e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -0.00012962096792823452\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of -0.00038721446019063244\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of -0.0003416961127395979\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of -0.0003016817148207007\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of -0.0010954616608280787\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of -6.141184252150822e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of -0.00014913798134896794\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of -0.00019149696550790907\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -0.00029056916046621417\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of -0.00018396260988651214\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -0.00013555159106903858\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of -0.0003900193725516754\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of -0.0004015473566325589\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of -0.00010934218583269484\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of -0.0003476591399051987\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of -0.00018316434981358043\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of -0.00026111268357076334\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of -0.0002033484807237862\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -7.553221451206404e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of -3.577878792637712e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 0.00014973140741858515\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of 7.873759572607497e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of 1.1010765875396622e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of -7.330546088613055e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of -0.00014187995808989662\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of 8.345302187551251e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of -0.00021113226854718725\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of 3.848173561360468e-06\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -0.00022447181264961813\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of 4.147364143336496e-06\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of -5.9088977743870785e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of -0.0005961544382739881\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of -0.00034263638577813893\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of -0.00032798864980776443\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of -0.00027878731144181934\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of -3.285072169689762e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of -0.000446029143817972\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of -0.00012020045870286822\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -0.00038436172878952846\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of -0.0004906117561958023\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 10, Linear returned an R-squared of 3.172922057903538e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 20, Linear returned an R-squared of -0.00020619913708364024\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 30, Linear returned an R-squared of -0.0010680188025693482\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 40, Linear returned an R-squared of -6.930423349227333e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 50, Linear returned an R-squared of 9.04909781501706e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 60, Linear returned an R-squared of -6.408997048623633e-05\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 70, Linear returned an R-squared of -0.0001224591780979445\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 80, Linear returned an R-squared of -0.00028375762228227686\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 90, Linear returned an R-squared of -0.0005365964690193792\n",
      "Training model LinearRegression(n_jobs=32) on 80000 data points\n",
      "For sequence length 100, Linear returned an R-squared of -0.00018687460212296614\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.9987745106273556\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.9988457752024184\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.998910377612033\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.9987781169220166\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.9986886227257656\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.9987349917566947\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.9987872281764021\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.9988011298981501\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.9988886204294458\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.9989215599697766\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.9979689740280234\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.9981732309093372\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.9981076199964317\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.9979924057166228\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.9981251817204869\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.9981425420227091\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.9981085605552061\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.99829000687968\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.9980770355761894\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.9979608643355765\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.9989191726056641\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.9988726281530472\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.9989226356834702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.9989482657575783\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.9988223997592255\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.9988483149163397\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.9991004256145811\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.9989326953537373\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.9989717717428604\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.9988935202869255\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.9982625624790528\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.9983869794916037\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.998537864653337\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.9981574860505236\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.9981496572682634\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.9982222996465366\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.9983637984718007\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.9982424486563645\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.9981845941841998\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.9980235405079597\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.9990516998076887\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.9990599988233341\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.9988844919105971\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.9990012709505094\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.9990154057363745\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.9989339922150304\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.99896668815383\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.9991521061746472\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.9990427062170136\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.9989918869632727\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.833790298683907\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.8421569098036297\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.8310760617363755\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.8314969016274862\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.8399595963639256\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.8360345869184427\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.8318429737246409\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.840999660851096\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.8236291842459207\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.8467817437817317\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.8440480377397186\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.8498994139465554\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.8546789102771581\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.8501289009572935\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.8510851648769637\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.8661180134284525\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.851155231634606\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.8630772532895682\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.8626416715255574\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.8627884760291922\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.8637561384207652\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.8602412063929639\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.867705684897543\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 40, GB returned an R-squared of 0.8537040761615757\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.844240099878169\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.8403037086590586\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.8590242162167757\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.8572466335997703\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.8510649562929817\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.8522563003248999\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.8835355518702825\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.8798093281724956\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.8833465229472993\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.8891612902602226\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.8798011894569668\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.8827302236867272\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.8875962156121641\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.8829497721020738\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.8768418017079451\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.8791923997303707\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.9113050370750739\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.9200163540423114\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.9268066487777157\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.9285673182198015\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.9183264014969387\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.9263853820442323\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.9253024471394287\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.9228166781264124\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.9255886801520385\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.9254658466297545\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.39520903851416256\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.401857624656328\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.41056601835336015\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.40121460814408605\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.4063530830349321\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.392516971530917\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.39730043426040484\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.39089246252167564\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.3912543614496379\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.39457877731316504\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.30940945472516956\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.3152046784920338\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.3015473260795559\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.3121098837768612\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.32330748366964646\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.32007230913851126\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.3155830268335844\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.31677210536196143\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.31460191725903797\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.31526532159581244\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.32305770265916667\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.31317131770572537\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.30725847635961\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 40, GB returned an R-squared of 0.30502153239657626\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.3145659452687609\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.3087585064804754\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.3090490855742385\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.3156835081214311\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.30757370412486706\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.3102648388902238\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.3530352109102377\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.3537201951233203\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.3551820535324002\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.3528618739840972\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.3502302657003712\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.3414380560027226\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.3458530988175962\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.3457703397268912\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.35943098422149455\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.3489256872740135\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.378108299608645\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.3736815857702731\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.3877342547655209\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.3785066020448924\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.37182888195446817\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.36625871045946523\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.3830905269824081\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.3748378778820711\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.37904119691826066\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.37259552167206833\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.0692956954377032\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.07035645261694057\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.06682569633395585\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.06619957051307657\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.06767895764807164\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.06785392913638011\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.07103310709603694\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.06441738926607199\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.06631424866603497\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.06859721399406504\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.07047651163997426\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.06692680192460054\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.07243651006691432\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.06867230392012535\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.07057237322927068\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.06808003770258242\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.06909709679507092\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.07359973840989098\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.06732304815999712\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.06855711194942704\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.06357337737677216\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.06579465161986753\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.06551660258164393\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 40, GB returned an R-squared of 0.06975996410359264\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.06870298277923437\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.0674946423148528\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.06708129135249075\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.07175130757877679\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.06562547293115661\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.07099754761043597\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.05651104041709465\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.0550785535482915\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.05902315162496374\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.05879396295121431\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.0562122536472901\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.055439048485987574\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.056159268715619826\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.05494827771347732\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.054563966787758766\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.06146178592410989\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of 0.08523063227277838\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of 0.08072324926682506\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of 0.08115648818472787\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of 0.08489252512648848\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of 0.0848137045853109\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of 0.08455587602164427\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of 0.08276244771488439\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of 0.07982345272020652\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of 0.08130336075528954\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of 0.08614289623629567\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of -0.007791115462014941\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of -0.005282682870010769\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of -0.004666639171726716\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of -0.009824626294996142\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of -0.008249299837469692\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of -0.007189960532629325\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of -0.007936439449824917\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of -0.005899122936677603\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of -0.009337257881804062\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of -0.0053346033659107395\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of -0.011958621519466694\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of -0.00685437599583083\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of -0.00487246299463906\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of -0.007035483220336092\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of -0.007282774780808854\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of -0.009030188560566588\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of -0.007228290804462523\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of -0.0066921234425108\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of -0.007466863483007069\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of -0.005848088039499322\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of -0.0077730507737769905\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of -0.005629790578367544\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of -0.006401288909152036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of -0.008962893074153877\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of -0.007657901349422014\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of -0.00686205953855179\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of -0.005934173715396174\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of -0.007595992449211497\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of -0.007307829703786073\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of -0.00941965398735789\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of -0.006789700880435179\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of -0.0076031091473205414\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of -0.007764612747913846\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of -0.00643980269690525\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of -0.008237218037950411\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of -0.007909602306655161\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of -0.00787918420886502\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of -0.00588178223701652\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of -0.009132874881109032\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of -0.008364985127148561\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 10, GB returned an R-squared of -0.006584971037079113\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 20, GB returned an R-squared of -0.006669372878377766\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 30, GB returned an R-squared of -0.006205657741634507\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 40, GB returned an R-squared of -0.0068711718391276655\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 50, GB returned an R-squared of -0.00908208870230598\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 60, GB returned an R-squared of -0.00851922450704623\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 70, GB returned an R-squared of -0.009129918793947756\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 80, GB returned an R-squared of -0.008670792886339651\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 90, GB returned an R-squared of -0.006566618129288271\n",
      "Training model GradientBoostingRegressor(max_depth=5, n_estimators=200) on 80000 data points\n",
      "For sequence length 100, GB returned an R-squared of -0.008376652227758186\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0146\u001b[0m        \u001b[32m0.0138\u001b[0m  8.6145\n",
      "      2        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0127\u001b[0m  8.6305\n",
      "      3        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0107\u001b[0m  8.4537\n",
      "      4        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0095\u001b[0m  8.5085\n",
      "      5        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0090\u001b[0m  8.5696\n",
      "      6        \u001b[36m0.0089\u001b[0m        \u001b[32m0.0083\u001b[0m  8.5048\n",
      "      7        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0079\u001b[0m  8.0299\n",
      "      8        \u001b[36m0.0083\u001b[0m        0.0079  7.5250\n",
      "      9        \u001b[36m0.0082\u001b[0m        0.0086  7.5993\n",
      "     10        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0077\u001b[0m  8.3552\n",
      "     11        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0077\u001b[0m  7.4515\n",
      "     12        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0076\u001b[0m  7.9498\n",
      "     13        \u001b[36m0.0079\u001b[0m        \u001b[32m0.0076\u001b[0m  8.0231\n",
      "     14        \u001b[36m0.0079\u001b[0m        0.0078  7.6817\n",
      "     15        \u001b[36m0.0079\u001b[0m        \u001b[32m0.0076\u001b[0m  7.9554\n",
      "     16        \u001b[36m0.0079\u001b[0m        0.0077  7.8160\n",
      "     17        \u001b[36m0.0079\u001b[0m        \u001b[32m0.0076\u001b[0m  8.2333\n",
      "     18        \u001b[36m0.0078\u001b[0m        \u001b[32m0.0076\u001b[0m  7.7128\n",
      "     19        \u001b[36m0.0078\u001b[0m        0.0076  7.4212\n",
      "     20        \u001b[36m0.0078\u001b[0m        0.0076  7.5712\n",
      "     21        0.0078        0.0076  7.4462\n",
      "     22        \u001b[36m0.0078\u001b[0m        0.0078  7.7283\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.4730198585649589\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0143\u001b[0m  8.6577\n",
      "      2        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0137\u001b[0m  8.4041\n",
      "      3        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0131\u001b[0m  8.6961\n",
      "      4        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0129\u001b[0m  8.6843\n",
      "      5        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0124\u001b[0m  8.5174\n",
      "      6        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0122\u001b[0m  8.3602\n",
      "      7        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0121\u001b[0m  7.1374\n",
      "      8        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0120\u001b[0m  7.9812\n",
      "      9        \u001b[36m0.0122\u001b[0m        0.0121  8.2678\n",
      "     10        \u001b[36m0.0121\u001b[0m        0.0122  7.7467\n",
      "     11        0.0122        \u001b[32m0.0120\u001b[0m  7.9410\n",
      "     12        \u001b[36m0.0121\u001b[0m        0.0121  7.9168\n",
      "     13        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0120\u001b[0m  7.5169\n",
      "     14        0.0121        0.0120  8.4150\n",
      "     15        0.0121        0.0120  8.2373\n",
      "     16        \u001b[36m0.0121\u001b[0m        0.0120  7.5764\n",
      "     17        \u001b[36m0.0121\u001b[0m        0.0123  7.0135\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.20520681478437997\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0150\u001b[0m  8.2806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0151\u001b[0m        0.0150  8.3488\n",
      "      3        0.0151        0.0150  8.5818\n",
      "      4        0.0151        \u001b[32m0.0150\u001b[0m  8.7709\n",
      "      5        0.0151        0.0150  8.5085\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0005410762160895644\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0147\u001b[0m  8.7105\n",
      "      2        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0126\u001b[0m  8.7477\n",
      "      3        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0107\u001b[0m  8.5801\n",
      "      4        \u001b[36m0.0111\u001b[0m        0.0107  8.4976\n",
      "      5        \u001b[36m0.0107\u001b[0m        0.0107  8.5139\n",
      "      6        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0106\u001b[0m  8.6915\n",
      "      7        \u001b[36m0.0104\u001b[0m        0.0110  8.3908\n",
      "      8        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0100\u001b[0m  8.6874\n",
      "      9        \u001b[36m0.0101\u001b[0m        0.0104  8.6302\n",
      "     10        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0099\u001b[0m  8.4895\n",
      "     11        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0098\u001b[0m  7.9781\n",
      "     12        \u001b[36m0.0099\u001b[0m        0.0107  8.3785\n",
      "     13        \u001b[36m0.0098\u001b[0m        0.0099  8.2911\n",
      "     14        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0096\u001b[0m  8.2520\n",
      "     15        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0083\u001b[0m  8.3389\n",
      "     16        \u001b[36m0.0086\u001b[0m        0.0094  8.6821\n",
      "     17        \u001b[36m0.0082\u001b[0m        \u001b[32m0.0071\u001b[0m  8.4668\n",
      "     18        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0067\u001b[0m  8.5440\n",
      "     19        \u001b[36m0.0072\u001b[0m        0.0083  8.7028\n",
      "     20        \u001b[36m0.0067\u001b[0m        0.0070  8.7528\n",
      "     21        \u001b[36m0.0062\u001b[0m        \u001b[32m0.0058\u001b[0m  8.4817\n",
      "     22        \u001b[36m0.0058\u001b[0m        \u001b[32m0.0045\u001b[0m  8.6553\n",
      "     23        \u001b[36m0.0053\u001b[0m        \u001b[32m0.0041\u001b[0m  8.7808\n",
      "     24        \u001b[36m0.0050\u001b[0m        0.0045  8.4637\n",
      "     25        \u001b[36m0.0045\u001b[0m        0.0047  8.7874\n",
      "     26        \u001b[36m0.0043\u001b[0m        0.0049  8.5361\n",
      "     27        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0036\u001b[0m  8.5032\n",
      "     28        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0026\u001b[0m  8.4579\n",
      "     29        \u001b[36m0.0033\u001b[0m        0.0031  8.5417\n",
      "     30        \u001b[36m0.0030\u001b[0m        0.0027  8.4336\n",
      "     31        \u001b[36m0.0028\u001b[0m        \u001b[32m0.0020\u001b[0m  8.5772\n",
      "     32        \u001b[36m0.0025\u001b[0m        0.0021  8.5957\n",
      "     33        \u001b[36m0.0023\u001b[0m        0.0028  8.6409\n",
      "     34        \u001b[36m0.0021\u001b[0m        \u001b[32m0.0017\u001b[0m  8.4833\n",
      "     35        \u001b[36m0.0019\u001b[0m        \u001b[32m0.0015\u001b[0m  8.2773\n",
      "     36        \u001b[36m0.0017\u001b[0m        \u001b[32m0.0013\u001b[0m  8.5639\n",
      "     37        \u001b[36m0.0016\u001b[0m        0.0014  8.5760\n",
      "     38        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0007\u001b[0m  8.5287\n",
      "     39        \u001b[36m0.0013\u001b[0m        0.0015  8.5752\n",
      "     40        \u001b[36m0.0011\u001b[0m        0.0013  8.7931\n",
      "     41        \u001b[36m0.0011\u001b[0m        0.0014  8.5844\n",
      "     42        \u001b[36m0.0009\u001b[0m        0.0009  8.4727\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.9399983302668908\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0152\u001b[0m  8.5608\n",
      "      2        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4362\n",
      "      3        \u001b[36m0.0152\u001b[0m        0.0155  8.4959\n",
      "      4        \u001b[36m0.0151\u001b[0m        0.0158  8.5288\n",
      "      5        0.0152        \u001b[32m0.0151\u001b[0m  8.4759\n",
      "      6        \u001b[36m0.0151\u001b[0m        0.0163  8.5017\n",
      "      7        0.0151        \u001b[32m0.0151\u001b[0m  8.5351\n",
      "      8        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0151\u001b[0m  8.4871\n",
      "      9        \u001b[36m0.0151\u001b[0m        0.0154  8.6026\n",
      "     10        \u001b[36m0.0151\u001b[0m        0.0151  8.4615\n",
      "     11        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0150\u001b[0m  8.6165\n",
      "     12        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0144\u001b[0m  8.4421\n",
      "     13        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0137\u001b[0m  8.5417\n",
      "     14        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0123\u001b[0m  8.3857\n",
      "     15        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0110\u001b[0m  8.6313\n",
      "     16        \u001b[36m0.0099\u001b[0m        0.0119  8.5762\n",
      "     17        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0079\u001b[0m  8.5364\n",
      "     18        \u001b[36m0.0085\u001b[0m        \u001b[32m0.0078\u001b[0m  8.6390\n",
      "     19        \u001b[36m0.0077\u001b[0m        0.0087  8.5342\n",
      "     20        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0057\u001b[0m  8.4866\n",
      "     21        \u001b[36m0.0065\u001b[0m        0.0062  8.4631\n",
      "     22        \u001b[36m0.0060\u001b[0m        \u001b[32m0.0052\u001b[0m  8.5961\n",
      "     23        \u001b[36m0.0057\u001b[0m        0.0056  8.5616\n",
      "     24        \u001b[36m0.0051\u001b[0m        \u001b[32m0.0045\u001b[0m  8.5914\n",
      "     25        \u001b[36m0.0047\u001b[0m        0.0048  8.3757\n",
      "     26        \u001b[36m0.0044\u001b[0m        \u001b[32m0.0036\u001b[0m  8.5960\n",
      "     27        \u001b[36m0.0040\u001b[0m        0.0065  8.5252\n",
      "     28        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0030\u001b[0m  8.6005\n",
      "     29        \u001b[36m0.0033\u001b[0m        0.0040  8.6387\n",
      "     30        \u001b[36m0.0031\u001b[0m        \u001b[32m0.0027\u001b[0m  8.5402\n",
      "     31        \u001b[36m0.0028\u001b[0m        0.0043  8.5952\n",
      "     32        \u001b[36m0.0026\u001b[0m        \u001b[32m0.0014\u001b[0m  8.4682\n",
      "     33        \u001b[36m0.0023\u001b[0m        \u001b[32m0.0013\u001b[0m  8.5671\n",
      "     34        \u001b[36m0.0020\u001b[0m        0.0029  8.5677\n",
      "     35        \u001b[36m0.0017\u001b[0m        \u001b[32m0.0009\u001b[0m  8.6083\n",
      "     36        \u001b[36m0.0015\u001b[0m        0.0017  8.6085\n",
      "     37        \u001b[36m0.0013\u001b[0m        0.0013  8.5320\n",
      "     38        \u001b[36m0.0011\u001b[0m        0.0012  8.3862\n",
      "     39        \u001b[36m0.0010\u001b[0m        0.0010  8.5611\n",
      "     40        \u001b[36m0.0009\u001b[0m        \u001b[32m0.0004\u001b[0m  8.4811\n",
      "     41        \u001b[36m0.0008\u001b[0m        0.0009  8.5744\n",
      "     42        \u001b[36m0.0007\u001b[0m        0.0009  7.3673\n",
      "     43        \u001b[36m0.0007\u001b[0m        0.0005  7.6715\n",
      "     44        \u001b[36m0.0006\u001b[0m        0.0005  8.0175\n",
      "     45        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0004\u001b[0m  8.4692\n",
      "     46        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0003\u001b[0m  7.7665\n",
      "     47        \u001b[36m0.0005\u001b[0m        0.0004  8.0947\n",
      "     48        \u001b[36m0.0005\u001b[0m        0.0007  7.3658\n",
      "     49        \u001b[36m0.0005\u001b[0m        0.0004  7.6749\n",
      "     50        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0002\u001b[0m  8.0059\n",
      "     51        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  7.6897\n",
      "     52        \u001b[36m0.0004\u001b[0m        0.0003  7.9862\n",
      "     53        \u001b[36m0.0004\u001b[0m        0.0006  7.6436\n",
      "     54        0.0004        0.0004  7.6068\n",
      "     55        0.0004        0.0002  8.3201\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.9674376457906263\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0149\u001b[0m  7.9591\n",
      "      2        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0148\u001b[0m  7.5175\n",
      "      3        \u001b[36m0.0153\u001b[0m        0.0148  7.1607\n",
      "      4        \u001b[36m0.0153\u001b[0m        0.0148  8.2817\n",
      "      5        \u001b[36m0.0153\u001b[0m        0.0148  7.9952\n",
      "      6        0.0153        0.0148  7.5352\n",
      "      7        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0148\u001b[0m  8.2735\n",
      "      8        0.0152        0.0148  8.8346\n",
      "      9        0.0152        0.0148  8.7423\n",
      "     10        0.0153        \u001b[32m0.0148\u001b[0m  8.7356\n",
      "     11        \u001b[36m0.0152\u001b[0m        0.0149  8.6805\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0035175187887050985\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0131\u001b[0m  8.5952\n",
      "      2        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0113\u001b[0m  8.7993\n",
      "      3        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0107\u001b[0m  8.7860\n",
      "      4        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0103\u001b[0m  8.7976\n",
      "      5        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0103\u001b[0m  8.6321\n",
      "      6        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0100\u001b[0m  8.4978\n",
      "      7        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0096\u001b[0m  8.8236\n",
      "      8        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0094\u001b[0m  8.6914\n",
      "      9        \u001b[36m0.0094\u001b[0m        0.0104  8.4353\n",
      "     10        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0089\u001b[0m  8.8082\n",
      "     11        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0071\u001b[0m  8.6447\n",
      "     12        \u001b[36m0.0056\u001b[0m        \u001b[32m0.0037\u001b[0m  8.5869\n",
      "     13        \u001b[36m0.0037\u001b[0m        \u001b[32m0.0029\u001b[0m  8.7822\n",
      "     14        \u001b[36m0.0031\u001b[0m        0.0035  8.6076\n",
      "     15        \u001b[36m0.0027\u001b[0m        \u001b[32m0.0020\u001b[0m  8.7896\n",
      "     16        \u001b[36m0.0024\u001b[0m        0.0021  8.7050\n",
      "     17        \u001b[36m0.0019\u001b[0m        0.0024  8.7639\n",
      "     18        \u001b[36m0.0018\u001b[0m        \u001b[32m0.0014\u001b[0m  8.7598\n",
      "     19        \u001b[36m0.0016\u001b[0m        \u001b[32m0.0013\u001b[0m  8.7632\n",
      "     20        0.0016        \u001b[32m0.0011\u001b[0m  8.6652\n",
      "     21        \u001b[36m0.0015\u001b[0m        0.0018  8.6413\n",
      "     22        0.0104        0.0098  8.7457\n",
      "     23        0.0086        0.0081  8.6547\n",
      "     24        0.0081        0.0075  8.7346\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of 0.6098962434057349\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0178\u001b[0m        \u001b[32m0.0150\u001b[0m  8.6311\n",
      "      2        \u001b[36m0.0151\u001b[0m        0.0150  8.7637\n",
      "      3        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0148\u001b[0m  8.7283\n",
      "      4        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0145\u001b[0m  8.6458\n",
      "      5        \u001b[36m0.0146\u001b[0m        \u001b[32m0.0140\u001b[0m  8.7675\n",
      "      6        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0137\u001b[0m  8.8029\n",
      "      7        \u001b[36m0.0139\u001b[0m        0.0137  8.5753\n",
      "      8        \u001b[36m0.0137\u001b[0m        0.0151  8.8029\n",
      "      9        \u001b[36m0.0135\u001b[0m        0.0138  8.7576\n",
      "     10        \u001b[36m0.0131\u001b[0m        0.0149  8.7155\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.02158750832028078\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0151\u001b[0m  8.7600\n",
      "      2        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0151\u001b[0m  8.7920\n",
      "      3        \u001b[36m0.0152\u001b[0m        0.0151  8.5630\n",
      "      4        \u001b[36m0.0152\u001b[0m        0.0151  8.7841\n",
      "      5        0.0152        0.0151  8.6037\n",
      "      6        \u001b[36m0.0152\u001b[0m        0.0153  8.7842\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.01835793104865302\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0151\u001b[0m  8.7891\n",
      "      2        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0151\u001b[0m  8.6508\n",
      "      3        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0150\u001b[0m  8.6010\n",
      "      4        \u001b[36m0.0152\u001b[0m        0.0150  7.1955\n",
      "      5        \u001b[36m0.0152\u001b[0m        0.0150  7.5559\n",
      "      6        0.0152        0.0151  8.2778\n",
      "      7        \u001b[36m0.0152\u001b[0m        0.0151  8.1200\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.0012934667046493598\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0136\u001b[0m  7.2346\n",
      "      2        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0134\u001b[0m  8.6964\n",
      "      3        0.0135        \u001b[32m0.0134\u001b[0m  8.7469\n",
      "      4        0.0135        \u001b[32m0.0133\u001b[0m  8.7198\n",
      "      5        \u001b[36m0.0135\u001b[0m        0.0135  8.8086\n",
      "      6        \u001b[36m0.0135\u001b[0m        0.0134  8.6486\n",
      "      7        \u001b[36m0.0135\u001b[0m        0.0134  8.6173\n",
      "      8        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0133\u001b[0m  8.7790\n",
      "      9        \u001b[36m0.0134\u001b[0m        0.0137  8.7502\n",
      "     10        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0132\u001b[0m  8.7922\n",
      "     11        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0114\u001b[0m  8.6834\n",
      "     12        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0111\u001b[0m  8.6585\n",
      "     13        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0106\u001b[0m  8.5179\n",
      "     14        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0099\u001b[0m  8.7698\n",
      "     15        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0095\u001b[0m  8.7422\n",
      "     16        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0091\u001b[0m  8.7017\n",
      "     17        \u001b[36m0.0090\u001b[0m        0.0125  8.7942\n",
      "     18        \u001b[36m0.0075\u001b[0m        \u001b[32m0.0066\u001b[0m  8.7912\n",
      "     19        \u001b[36m0.0056\u001b[0m        \u001b[32m0.0054\u001b[0m  8.8144\n",
      "     20        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0034\u001b[0m  8.7293\n",
      "     21        \u001b[36m0.0040\u001b[0m        0.0050  8.7061\n",
      "     22        \u001b[36m0.0037\u001b[0m        \u001b[32m0.0025\u001b[0m  8.6154\n",
      "     23        \u001b[36m0.0032\u001b[0m        0.0032  8.7900\n",
      "     24        \u001b[36m0.0028\u001b[0m        \u001b[32m0.0017\u001b[0m  8.8260\n",
      "     25        0.0029        0.0104  8.8080\n",
      "     26        \u001b[36m0.0023\u001b[0m        0.0024  8.1898\n",
      "     27        \u001b[36m0.0019\u001b[0m        \u001b[32m0.0011\u001b[0m  6.6995\n",
      "     28        \u001b[36m0.0016\u001b[0m        0.0022  6.9196\n",
      "     29        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0009\u001b[0m  6.1490\n",
      "     30        \u001b[36m0.0012\u001b[0m        0.0015  6.9396\n",
      "     31        \u001b[36m0.0011\u001b[0m        \u001b[32m0.0006\u001b[0m  7.7825\n",
      "     32        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0005\u001b[0m  6.6593\n",
      "     33        \u001b[36m0.0009\u001b[0m        0.0021  6.8101\n",
      "     34        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0004\u001b[0m  8.3523\n",
      "     35        \u001b[36m0.0008\u001b[0m        0.0007  8.5496\n",
      "     36        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0003\u001b[0m  8.6403\n",
      "     37        0.0006        \u001b[32m0.0003\u001b[0m  8.5816\n",
      "     38        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0002\u001b[0m  8.7478\n",
      "     39        \u001b[36m0.0005\u001b[0m        0.0003  8.6691\n",
      "     40        \u001b[36m0.0005\u001b[0m        0.0003  8.8216\n",
      "     41        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  8.5711\n",
      "     42        \u001b[36m0.0004\u001b[0m        0.0003  8.7526\n",
      "     43        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  8.6477\n",
      "     44        \u001b[36m0.0004\u001b[0m        0.0002  8.5869\n",
      "     45        \u001b[36m0.0004\u001b[0m        0.0003  8.8206\n",
      "     46        \u001b[36m0.0004\u001b[0m        0.0002  8.6291\n",
      "     47        0.0004        0.0003  8.8123\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.9806616557964809\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0132\u001b[0m  8.5321\n",
      "      2        \u001b[36m0.0135\u001b[0m        0.0132  8.5529\n",
      "      3        0.0135        \u001b[32m0.0131\u001b[0m  8.6249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.0134\u001b[0m        0.0132  8.8235\n",
      "      5        \u001b[36m0.0134\u001b[0m        0.0131  8.6089\n",
      "      6        \u001b[36m0.0134\u001b[0m        0.0137  8.8341\n",
      "      7        0.0134        \u001b[32m0.0131\u001b[0m  8.6789\n",
      "      8        \u001b[36m0.0134\u001b[0m        0.0132  8.6824\n",
      "      9        \u001b[36m0.0134\u001b[0m        0.0132  8.5957\n",
      "     10        0.0134        0.0131  8.5215\n",
      "     11        0.0134        0.0131  8.3390\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.00717159332332562\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0134\u001b[0m  8.3799\n",
      "      2        \u001b[36m0.0133\u001b[0m        0.0135  8.5695\n",
      "      3        \u001b[36m0.0133\u001b[0m        0.0137  8.4891\n",
      "      4        \u001b[36m0.0133\u001b[0m        0.0135  8.4652\n",
      "      5        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0134\u001b[0m  8.5830\n",
      "      6        0.0133        0.0135  8.4462\n",
      "      7        0.0133        0.0135  8.3590\n",
      "      8        0.0133        0.0134  8.4242\n",
      "      9        0.0133        0.0136  8.5820\n",
      "     10        0.0133        \u001b[32m0.0134\u001b[0m  8.3126\n",
      "     11        \u001b[36m0.0133\u001b[0m        0.0135  8.5487\n",
      "     12        0.0133        0.0134  8.3289\n",
      "     13        0.0133        \u001b[32m0.0134\u001b[0m  7.5445\n",
      "     14        0.0133        0.0134  8.5787\n",
      "     15        0.0133        0.0135  8.4905\n",
      "     16        \u001b[36m0.0133\u001b[0m        0.0135  8.5375\n",
      "     17        0.0133        0.0135  8.6912\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.020723082176703933\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0117\u001b[0m  8.7699\n",
      "      2        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0115\u001b[0m  8.7858\n",
      "      3        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0114\u001b[0m  8.6653\n",
      "      4        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0113\u001b[0m  8.7747\n",
      "      5        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0112\u001b[0m  8.6053\n",
      "      6        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0112\u001b[0m  8.6297\n",
      "      7        \u001b[36m0.0113\u001b[0m        0.0112  8.6208\n",
      "      8        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0110\u001b[0m  8.7921\n",
      "      9        \u001b[36m0.0111\u001b[0m        0.0110  8.5282\n",
      "     10        \u001b[36m0.0110\u001b[0m        0.0112  8.6847\n",
      "     11        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0107\u001b[0m  8.5811\n",
      "     12        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0105\u001b[0m  8.7205\n",
      "     13        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0104\u001b[0m  8.6722\n",
      "     14        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0098\u001b[0m  8.5737\n",
      "     15        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0096\u001b[0m  8.7950\n",
      "     16        \u001b[36m0.0095\u001b[0m        0.0096  8.6447\n",
      "     17        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0091\u001b[0m  8.6416\n",
      "     18        \u001b[36m0.0093\u001b[0m        0.0093  8.6056\n",
      "     19        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0090\u001b[0m  8.6774\n",
      "     20        \u001b[36m0.0092\u001b[0m        0.0092  8.8582\n",
      "     21        \u001b[36m0.0092\u001b[0m        0.0093  8.7754\n",
      "     22        \u001b[36m0.0092\u001b[0m        0.0091  8.8302\n",
      "     23        0.0092        0.0091  8.7744\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.31772043568358754\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0135\u001b[0m  8.8341\n",
      "      2        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0134\u001b[0m  8.7511\n",
      "      3        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0134\u001b[0m  8.7616\n",
      "      4        0.0133        \u001b[32m0.0133\u001b[0m  8.7502\n",
      "      5        \u001b[36m0.0133\u001b[0m        0.0134  8.5208\n",
      "      6        0.0133        \u001b[32m0.0133\u001b[0m  8.5676\n",
      "      7        \u001b[36m0.0133\u001b[0m        0.0137  8.5599\n",
      "      8        0.0133        0.0133  8.8542\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.003840021971990737\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0135\u001b[0m  8.8182\n",
      "      2        \u001b[36m0.0133\u001b[0m        0.0136  8.8391\n",
      "      3        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0135\u001b[0m  8.6123\n",
      "      4        0.0133        \u001b[32m0.0135\u001b[0m  8.6840\n",
      "      5        0.0133        0.0135  8.5620\n",
      "      6        0.0133        0.0135  8.6795\n",
      "      7        0.0133        0.0136  8.6949\n",
      "      8        0.0133        0.0136  8.6928\n",
      "      9        0.0133        \u001b[32m0.0135\u001b[0m  8.8158\n",
      "     10        0.0133        \u001b[32m0.0135\u001b[0m  8.7827\n",
      "     11        0.0133        0.0135  8.6167\n",
      "     12        0.0133        0.0135  8.6834\n",
      "     13        \u001b[36m0.0133\u001b[0m        0.0135  8.8011\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -5.955938578328102e-06\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0137\u001b[0m        \u001b[32m0.0129\u001b[0m  8.7971\n",
      "      2        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0128\u001b[0m  8.5183\n",
      "      3        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0115\u001b[0m  8.8260\n",
      "      4        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0114\u001b[0m  8.6347\n",
      "      5        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0112\u001b[0m  8.7974\n",
      "      6        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0111\u001b[0m  8.7622\n",
      "      7        \u001b[36m0.0115\u001b[0m        0.0111  8.5135\n",
      "      8        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0108\u001b[0m  8.8318\n",
      "      9        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0102\u001b[0m  8.6348\n",
      "     10        \u001b[36m0.0107\u001b[0m        0.0103  8.5929\n",
      "     11        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0079\u001b[0m  8.6498\n",
      "     12        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0060\u001b[0m  8.8120\n",
      "     13        \u001b[36m0.0059\u001b[0m        \u001b[32m0.0058\u001b[0m  8.6433\n",
      "     14        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0050\u001b[0m  8.6827\n",
      "     15        \u001b[36m0.0042\u001b[0m        \u001b[32m0.0045\u001b[0m  8.4542\n",
      "     16        \u001b[36m0.0038\u001b[0m        \u001b[32m0.0042\u001b[0m  8.5211\n",
      "     17        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0032\u001b[0m  8.3390\n",
      "     18        \u001b[36m0.0032\u001b[0m        0.0036  7.3781\n",
      "     19        \u001b[36m0.0031\u001b[0m        \u001b[32m0.0028\u001b[0m  7.3337\n",
      "     20        \u001b[36m0.0029\u001b[0m        \u001b[32m0.0023\u001b[0m  7.1552\n",
      "     21        \u001b[36m0.0026\u001b[0m        0.0030  8.3204\n",
      "     22        \u001b[36m0.0025\u001b[0m        0.0028  7.6636\n",
      "     23        \u001b[36m0.0023\u001b[0m        \u001b[32m0.0016\u001b[0m  8.5486\n",
      "     24        \u001b[36m0.0021\u001b[0m        0.0022  7.1998\n",
      "     25        \u001b[36m0.0020\u001b[0m        0.0016  8.4350\n",
      "     26        \u001b[36m0.0019\u001b[0m        0.0018  7.5482\n",
      "     27        \u001b[36m0.0018\u001b[0m        0.0018  8.5272\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of 0.8785564016641234\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0132\u001b[0m  8.5781\n",
      "      2        \u001b[36m0.0132\u001b[0m        0.0133  7.3002\n",
      "      3        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0131\u001b[0m  7.8725\n",
      "      4        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0129\u001b[0m  7.1529\n",
      "      5        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0121\u001b[0m  8.6398\n",
      "      6        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0107\u001b[0m  8.8008\n",
      "      7        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0104\u001b[0m  8.6440\n",
      "      8        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0103\u001b[0m  8.6155\n",
      "      9        \u001b[36m0.0103\u001b[0m        0.0105  8.7597\n",
      "     10        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0103\u001b[0m  8.7892\n",
      "     11        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0099\u001b[0m  8.7539\n",
      "     12        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0098\u001b[0m  8.8550\n",
      "     13        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0095\u001b[0m  8.5840\n",
      "     14        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0092\u001b[0m  7.7779\n",
      "     15        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0088\u001b[0m  7.1932\n",
      "     16        \u001b[36m0.0089\u001b[0m        0.0099  8.5886\n",
      "     17        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0080\u001b[0m  7.2030\n",
      "     18        \u001b[36m0.0085\u001b[0m        \u001b[32m0.0079\u001b[0m  8.6899\n",
      "     19        \u001b[36m0.0084\u001b[0m        \u001b[32m0.0073\u001b[0m  7.3512\n",
      "     20        \u001b[36m0.0080\u001b[0m        0.0083  8.6348\n",
      "     21        \u001b[36m0.0078\u001b[0m        \u001b[32m0.0072\u001b[0m  7.1772\n",
      "     22        \u001b[36m0.0073\u001b[0m        \u001b[32m0.0071\u001b[0m  8.5421\n",
      "     23        \u001b[36m0.0068\u001b[0m        0.0136  7.2083\n",
      "     24        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0058\u001b[0m  8.5559\n",
      "     25        \u001b[36m0.0058\u001b[0m        \u001b[32m0.0047\u001b[0m  7.1858\n",
      "     26        \u001b[36m0.0051\u001b[0m        0.0048  8.3267\n",
      "     27        \u001b[36m0.0044\u001b[0m        0.0079  8.0481\n",
      "     28        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0026\u001b[0m  7.4755\n",
      "     29        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0015\u001b[0m  7.6806\n",
      "     30        \u001b[36m0.0025\u001b[0m        0.0020  7.9707\n",
      "     31        \u001b[36m0.0021\u001b[0m        \u001b[32m0.0011\u001b[0m  7.4829\n",
      "     32        \u001b[36m0.0017\u001b[0m        0.0013  8.0467\n",
      "     33        \u001b[36m0.0015\u001b[0m        0.0024  7.4593\n",
      "     34        \u001b[36m0.0013\u001b[0m        0.0013  7.8950\n",
      "     35        \u001b[36m0.0013\u001b[0m        0.0015  8.0246\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.9127563372865314\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0122\u001b[0m  8.1875\n",
      "      2        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0120\u001b[0m  8.0639\n",
      "      3        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0116\u001b[0m  7.6162\n",
      "      4        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0114\u001b[0m  8.0143\n",
      "      5        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0112\u001b[0m  7.4402\n",
      "      6        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0107\u001b[0m  8.0116\n",
      "      7        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0103\u001b[0m  7.4462\n",
      "      8        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0100\u001b[0m  7.8836\n",
      "      9        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0089\u001b[0m  7.4585\n",
      "     10        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0087\u001b[0m  7.8737\n",
      "     11        \u001b[36m0.0086\u001b[0m        \u001b[32m0.0082\u001b[0m  8.2932\n",
      "     12        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0068\u001b[0m  8.3984\n",
      "     13        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0053\u001b[0m  7.4394\n",
      "     14        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0051\u001b[0m  7.8809\n",
      "     15        \u001b[36m0.0055\u001b[0m        \u001b[32m0.0051\u001b[0m  8.6375\n",
      "     16        \u001b[36m0.0051\u001b[0m        \u001b[32m0.0047\u001b[0m  7.8946\n",
      "     17        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0037\u001b[0m  7.4741\n",
      "     18        \u001b[36m0.0043\u001b[0m        0.0057  7.8638\n",
      "     19        \u001b[36m0.0041\u001b[0m        0.0039  7.8535\n",
      "     20        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0037\u001b[0m  8.0379\n",
      "     21        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0036\u001b[0m  7.4551\n",
      "     22        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0027\u001b[0m  7.6997\n",
      "     23        \u001b[36m0.0032\u001b[0m        0.0033  7.4404\n",
      "     24        \u001b[36m0.0031\u001b[0m        \u001b[32m0.0021\u001b[0m  8.1447\n",
      "     25        \u001b[36m0.0028\u001b[0m        0.0023  7.6522\n",
      "     26        \u001b[36m0.0026\u001b[0m        0.0023  7.6662\n",
      "     27        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0019\u001b[0m  8.4395\n",
      "     28        \u001b[36m0.0023\u001b[0m        0.0029  8.4473\n",
      "     29        \u001b[36m0.0019\u001b[0m        0.0021  8.6147\n",
      "     30        \u001b[36m0.0018\u001b[0m        \u001b[32m0.0011\u001b[0m  8.4359\n",
      "     31        \u001b[36m0.0015\u001b[0m        0.0014  8.5701\n",
      "     32        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0010\u001b[0m  8.4145\n",
      "     33        \u001b[36m0.0013\u001b[0m        0.0016  8.6045\n",
      "     34        \u001b[36m0.0012\u001b[0m        0.0030  8.6244\n",
      "     35        \u001b[36m0.0011\u001b[0m        0.0010  8.5964\n",
      "     36        \u001b[36m0.0010\u001b[0m        0.0012  8.3375\n",
      "     37        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0006\u001b[0m  8.8319\n",
      "     38        \u001b[36m0.0009\u001b[0m        0.0009  8.7933\n",
      "     39        \u001b[36m0.0009\u001b[0m        \u001b[32m0.0005\u001b[0m  8.6742\n",
      "     40        \u001b[36m0.0008\u001b[0m        0.0008  8.8027\n",
      "     41        \u001b[36m0.0008\u001b[0m        0.0006  8.6463\n",
      "     42        \u001b[36m0.0008\u001b[0m        0.0007  8.8098\n",
      "     43        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0004\u001b[0m  8.8215\n",
      "     44        \u001b[36m0.0007\u001b[0m        0.0005  8.8013\n",
      "     45        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0003\u001b[0m  8.8624\n",
      "     46        \u001b[36m0.0006\u001b[0m        0.0007  8.7043\n",
      "     47        \u001b[36m0.0006\u001b[0m        0.0005  7.5295\n",
      "     48        \u001b[36m0.0005\u001b[0m        0.0006  6.6636\n",
      "     49        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0003\u001b[0m  5.8659\n",
      "     50        \u001b[36m0.0005\u001b[0m        0.0003  6.8557\n",
      "     51        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  7.4133\n",
      "     52        \u001b[36m0.0004\u001b[0m        0.0003  6.2593\n",
      "     53        \u001b[36m0.0004\u001b[0m        0.0006  5.9940\n",
      "     54        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  6.1295\n",
      "     55        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  6.6599\n",
      "     56        0.0003        \u001b[32m0.0002\u001b[0m  6.9271\n",
      "     57        \u001b[36m0.0003\u001b[0m        0.0003  7.4129\n",
      "     58        \u001b[36m0.0003\u001b[0m        0.0004  6.6751\n",
      "     59        \u001b[36m0.0002\u001b[0m        0.0003  5.9871\n",
      "     60        \u001b[36m0.0002\u001b[0m        0.0004  5.8634\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.9871765756385743\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0137\u001b[0m        \u001b[32m0.0122\u001b[0m  6.7644\n",
      "      2        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0121\u001b[0m  7.2196\n",
      "      3        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0118\u001b[0m  6.2526\n",
      "      4        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0111\u001b[0m  6.0996\n",
      "      5        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0087\u001b[0m  5.8673\n",
      "      6        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0069\u001b[0m  6.8255\n",
      "      7        \u001b[36m0.0069\u001b[0m        \u001b[32m0.0057\u001b[0m  6.8158\n",
      "      8        \u001b[36m0.0053\u001b[0m        \u001b[32m0.0040\u001b[0m  6.3598\n",
      "      9        \u001b[36m0.0041\u001b[0m        0.0061  5.9665\n",
      "     10        \u001b[36m0.0032\u001b[0m        \u001b[32m0.0027\u001b[0m  5.8667\n",
      "     11        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0015\u001b[0m  6.8205\n",
      "     12        \u001b[36m0.0021\u001b[0m        0.0023  5.8633\n",
      "     13        \u001b[36m0.0018\u001b[0m        \u001b[32m0.0013\u001b[0m  7.1663\n",
      "     14        \u001b[36m0.0016\u001b[0m        0.0018  6.1625\n",
      "     15        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0012\u001b[0m  5.8813\n",
      "     16        \u001b[36m0.0013\u001b[0m        \u001b[32m0.0010\u001b[0m  8.5013\n",
      "     17        \u001b[36m0.0012\u001b[0m        0.0012  8.6058\n",
      "     18        \u001b[36m0.0011\u001b[0m        0.0010  8.5572\n",
      "     19        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0008\u001b[0m  8.4466\n",
      "     20        \u001b[36m0.0010\u001b[0m        0.0009  8.5343\n",
      "     21        \u001b[36m0.0009\u001b[0m        0.0010  8.5729\n",
      "     22        \u001b[36m0.0009\u001b[0m        0.0010  8.6395\n",
      "     23        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0006\u001b[0m  8.5161\n",
      "     24        \u001b[36m0.0008\u001b[0m        0.0008  8.4367\n",
      "     25        \u001b[36m0.0008\u001b[0m        0.0007  8.5680\n",
      "     26        \u001b[36m0.0007\u001b[0m        0.0006  8.3992\n",
      "     27        \u001b[36m0.0006\u001b[0m        0.0009  8.6038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0005\u001b[0m  8.5906\n",
      "     29        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0004\u001b[0m  8.5332\n",
      "     30        \u001b[36m0.0005\u001b[0m        0.0007  8.4416\n",
      "     31        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0003\u001b[0m  8.5734\n",
      "     32        \u001b[36m0.0005\u001b[0m        0.0007  8.6122\n",
      "     33        \u001b[36m0.0004\u001b[0m        0.0004  8.5748\n",
      "     34        \u001b[36m0.0004\u001b[0m        0.0010  8.5425\n",
      "     35        0.0004        \u001b[32m0.0003\u001b[0m  8.5025\n",
      "     36        \u001b[36m0.0003\u001b[0m        0.0006  8.5728\n",
      "     37        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.5133\n",
      "     38        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.6064\n",
      "     39        \u001b[36m0.0003\u001b[0m        0.0002  8.4978\n",
      "     40        \u001b[36m0.0003\u001b[0m        0.0004  8.6233\n",
      "     41        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0002\u001b[0m  8.5647\n",
      "     42        0.0002        \u001b[32m0.0001\u001b[0m  8.6082\n",
      "     43        \u001b[36m0.0002\u001b[0m        0.0002  8.5058\n",
      "     44        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0001\u001b[0m  8.4360\n",
      "     45        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0001\u001b[0m  8.6119\n",
      "     46        \u001b[36m0.0002\u001b[0m        0.0002  8.6132\n",
      "     47        0.0002        0.0002  8.5660\n",
      "     48        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0001\u001b[0m  8.4672\n",
      "     49        0.0001        0.0001  8.5805\n",
      "     50        \u001b[36m0.0001\u001b[0m        0.0003  8.3800\n",
      "     51        0.0001        \u001b[32m0.0001\u001b[0m  8.4959\n",
      "     52        \u001b[36m0.0001\u001b[0m        0.0002  8.5168\n",
      "     53        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0001\u001b[0m  8.7695\n",
      "     54        0.0001        0.0001  8.6571\n",
      "     55        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0001\u001b[0m  8.8253\n",
      "     56        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0001\u001b[0m  8.6823\n",
      "     57        \u001b[36m0.0001\u001b[0m        0.0001  7.3156\n",
      "     58        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0000\u001b[0m  8.8137\n",
      "     59        0.0001        0.0001  8.8224\n",
      "     60        \u001b[36m0.0001\u001b[0m        0.0001  8.7171\n",
      "     61        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0000\u001b[0m  8.6815\n",
      "     62        0.0001        0.0001  8.6059\n",
      "     63        0.0001        \u001b[32m0.0000\u001b[0m  8.6509\n",
      "     64        0.0001        0.0002  8.7443\n",
      "     65        \u001b[36m0.0001\u001b[0m        0.0001  8.7748\n",
      "     66        0.0001        \u001b[32m0.0000\u001b[0m  8.8064\n",
      "     67        0.0001        \u001b[32m0.0000\u001b[0m  8.7968\n",
      "     68        0.0001        0.0001  8.5965\n",
      "     69        0.0001        0.0001  8.5353\n",
      "     70        \u001b[36m0.0001\u001b[0m        0.0001  8.7230\n",
      "     71        0.0001        0.0002  8.7107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.9962906431179224\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0129\u001b[0m  8.8409\n",
      "      2        \u001b[36m0.0129\u001b[0m        0.0129  8.4936\n",
      "      3        0.0129        0.0129  8.4999\n",
      "      4        \u001b[36m0.0129\u001b[0m        0.0129  8.5359\n",
      "      5        \u001b[36m0.0129\u001b[0m        0.0129  8.6331\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.00045194898115830107\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0129\u001b[0m  8.7087\n",
      "      2        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0127\u001b[0m  8.5980\n",
      "      3        \u001b[36m0.0129\u001b[0m        0.0131  8.6845\n",
      "      4        0.0129        0.0128  8.6366\n",
      "      5        \u001b[36m0.0129\u001b[0m        0.0127  8.8196\n",
      "      6        \u001b[36m0.0129\u001b[0m        0.0128  8.7564\n",
      "      7        0.0129        \u001b[32m0.0127\u001b[0m  8.6544\n",
      "      8        \u001b[36m0.0129\u001b[0m        0.0127  8.5140\n",
      "      9        0.0129        0.0128  8.5450\n",
      "     10        \u001b[36m0.0129\u001b[0m        0.0127  8.6090\n",
      "     11        0.0129        0.0128  8.6947\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.0006767241110623434\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0120\u001b[0m  8.6429\n",
      "      2        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0119\u001b[0m  8.7670\n",
      "      3        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0117\u001b[0m  8.5706\n",
      "      4        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0117\u001b[0m  8.5526\n",
      "      5        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0115\u001b[0m  8.6156\n",
      "      6        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0113\u001b[0m  8.6187\n",
      "      7        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0112\u001b[0m  8.8075\n",
      "      8        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0111\u001b[0m  8.7426\n",
      "      9        \u001b[36m0.0111\u001b[0m        0.0111  8.7845\n",
      "     10        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0110\u001b[0m  8.2663\n",
      "     11        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0109\u001b[0m  8.3173\n",
      "     12        \u001b[36m0.0110\u001b[0m        0.0109  7.9508\n",
      "     13        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0108\u001b[0m  7.5519\n",
      "     14        0.0110        \u001b[32m0.0108\u001b[0m  7.1448\n",
      "     15        \u001b[36m0.0110\u001b[0m        0.0109  7.7947\n",
      "     16        \u001b[36m0.0109\u001b[0m        0.0109  8.0590\n",
      "     17        0.0109        \u001b[32m0.0108\u001b[0m  7.6957\n",
      "     18        0.0110        0.0109  7.6031\n",
      "     19        0.0109        0.0109  8.3770\n",
      "     20        0.0109        0.0110  8.0450\n",
      "     21        0.0109        0.0109  8.0428\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.14808497730008763\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0129\u001b[0m  7.5803\n",
      "      2        \u001b[36m0.0130\u001b[0m        0.0134  7.3345\n",
      "      3        \u001b[36m0.0130\u001b[0m        0.0129  8.7864\n",
      "      4        \u001b[36m0.0130\u001b[0m        0.0132  8.7356\n",
      "      5        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0128\u001b[0m  7.8030\n",
      "      6        0.0130        0.0128  8.7871\n",
      "      7        0.0130        0.0128  8.6642\n",
      "      8        \u001b[36m0.0129\u001b[0m        0.0130  8.7575\n",
      "      9        0.0129        0.0131  8.8119\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.02645118221519338\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0112\u001b[0m  7.8769\n",
      "      2        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0104\u001b[0m  7.1463\n",
      "      3        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0104\u001b[0m  7.8337\n",
      "      4        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0102\u001b[0m  7.7406\n",
      "      5        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0101\u001b[0m  7.1731\n",
      "      6        \u001b[36m0.0104\u001b[0m        0.0106  7.4882\n",
      "      7        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0100\u001b[0m  7.4116\n",
      "      8        \u001b[36m0.0102\u001b[0m        0.0103  7.2158\n",
      "      9        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0100\u001b[0m  7.1879\n",
      "     10        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0098\u001b[0m  7.1612\n",
      "     11        \u001b[36m0.0100\u001b[0m        0.0102  7.1477\n",
      "     12        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0097\u001b[0m  8.4703\n",
      "     13        \u001b[36m0.0099\u001b[0m        0.0100  7.1631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0097\u001b[0m  8.2518\n",
      "     15        \u001b[36m0.0097\u001b[0m        0.0098  8.5914\n",
      "     16        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0096\u001b[0m  8.6284\n",
      "     17        \u001b[36m0.0096\u001b[0m        0.0097  8.6503\n",
      "     18        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0094\u001b[0m  8.7839\n",
      "     19        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0094\u001b[0m  8.7666\n",
      "     20        \u001b[36m0.0095\u001b[0m        0.0094  8.7209\n",
      "     21        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0093\u001b[0m  7.4345\n",
      "     22        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0093\u001b[0m  8.0225\n",
      "     23        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0093\u001b[0m  7.6085\n",
      "     24        \u001b[36m0.0093\u001b[0m        0.0093  8.5120\n",
      "     25        \u001b[36m0.0092\u001b[0m        \u001b[32m0.0092\u001b[0m  8.5110\n",
      "     26        \u001b[36m0.0092\u001b[0m        0.0096  8.4729\n",
      "     27        \u001b[36m0.0092\u001b[0m        0.0094  8.4350\n",
      "     28        \u001b[36m0.0091\u001b[0m        0.0094  8.4920\n",
      "     29        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0090\u001b[0m  8.5105\n",
      "     30        \u001b[36m0.0091\u001b[0m        0.0097  8.4639\n",
      "     31        0.0091        0.0093  8.0855\n",
      "     32        \u001b[36m0.0090\u001b[0m        \u001b[32m0.0089\u001b[0m  8.2701\n",
      "     33        \u001b[36m0.0090\u001b[0m        0.0091  8.1186\n",
      "     34        \u001b[36m0.0090\u001b[0m        0.0094  8.1203\n",
      "     35        \u001b[36m0.0089\u001b[0m        \u001b[32m0.0088\u001b[0m  8.1100\n",
      "     36        \u001b[36m0.0089\u001b[0m        \u001b[32m0.0088\u001b[0m  8.2784\n",
      "     37        \u001b[36m0.0089\u001b[0m        0.0088  8.4829\n",
      "     38        \u001b[36m0.0089\u001b[0m        0.0088  8.5418\n",
      "     39        0.0089        0.0091  8.4610\n",
      "     40        0.0089        \u001b[32m0.0087\u001b[0m  8.4684\n",
      "     41        \u001b[36m0.0089\u001b[0m        0.0087  8.5133\n",
      "     42        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0087\u001b[0m  8.5006\n",
      "     43        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0087\u001b[0m  8.4725\n",
      "     44        \u001b[36m0.0088\u001b[0m        0.0087  8.3063\n",
      "     45        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0086\u001b[0m  8.1686\n",
      "     46        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0085\u001b[0m  8.1869\n",
      "     47        \u001b[36m0.0086\u001b[0m        0.0086  8.2266\n",
      "     48        \u001b[36m0.0083\u001b[0m        0.0087  8.2347\n",
      "     49        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0077\u001b[0m  8.2760\n",
      "     50        \u001b[36m0.0078\u001b[0m        0.0078  8.1558\n",
      "     51        \u001b[36m0.0074\u001b[0m        0.0093  8.3328\n",
      "     52        \u001b[36m0.0066\u001b[0m        \u001b[32m0.0071\u001b[0m  8.2144\n",
      "     53        \u001b[36m0.0058\u001b[0m        \u001b[32m0.0065\u001b[0m  8.2100\n",
      "     54        \u001b[36m0.0050\u001b[0m        \u001b[32m0.0042\u001b[0m  8.2308\n",
      "     55        \u001b[36m0.0043\u001b[0m        \u001b[32m0.0023\u001b[0m  8.2515\n",
      "     56        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0021\u001b[0m  8.3390\n",
      "     57        \u001b[36m0.0030\u001b[0m        0.0030  8.3933\n",
      "     58        \u001b[36m0.0025\u001b[0m        0.0055  8.2043\n",
      "     59        \u001b[36m0.0021\u001b[0m        0.0030  8.2004\n",
      "     60        \u001b[36m0.0018\u001b[0m        \u001b[32m0.0013\u001b[0m  8.2415\n",
      "     61        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0007\u001b[0m  8.3880\n",
      "     62        \u001b[36m0.0014\u001b[0m        0.0012  8.2934\n",
      "     63        \u001b[36m0.0013\u001b[0m        0.0012  8.2146\n",
      "     64        \u001b[36m0.0012\u001b[0m        \u001b[32m0.0006\u001b[0m  8.2179\n",
      "     65        \u001b[36m0.0011\u001b[0m        0.0008  8.2431\n",
      "     66        \u001b[36m0.0010\u001b[0m        0.0007  8.1340\n",
      "     67        \u001b[36m0.0009\u001b[0m        0.0021  8.1390\n",
      "     68        \u001b[36m0.0009\u001b[0m        0.0022  8.1986\n",
      "     69        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0005\u001b[0m  8.1621\n",
      "     70        \u001b[36m0.0008\u001b[0m        0.0014  8.2142\n",
      "     71        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0003\u001b[0m  8.2255\n",
      "     72        \u001b[36m0.0007\u001b[0m        0.0008  8.3048\n",
      "     73        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0003\u001b[0m  8.1721\n",
      "     74        \u001b[36m0.0006\u001b[0m        0.0005  8.1943\n",
      "     75        \u001b[36m0.0006\u001b[0m        0.0005  8.2519\n",
      "     76        0.0006        0.0006  8.2865\n",
      "     77        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0003\u001b[0m  8.2847\n",
      "     78        \u001b[36m0.0005\u001b[0m        0.0005  8.1682\n",
      "     79        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0002\u001b[0m  8.1673\n",
      "     80        0.0005        0.0004  8.1921\n",
      "     81        \u001b[36m0.0004\u001b[0m        0.0005  8.2376\n",
      "     82        0.0004        \u001b[32m0.0002\u001b[0m  8.2646\n",
      "     83        0.0004        0.0007  8.1780\n",
      "     84        \u001b[36m0.0004\u001b[0m        0.0004  8.1898\n",
      "     85        0.0004        0.0002  8.1860\n",
      "     86        0.0004        0.0003  8.4169\n",
      "     87        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.4878\n",
      "     88        \u001b[36m0.0003\u001b[0m        0.0002  8.5041\n",
      "     89        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0001\u001b[0m  8.4860\n",
      "     90        0.0003        \u001b[32m0.0001\u001b[0m  8.4632\n",
      "     91        0.0003        0.0002  8.4527\n",
      "     92        \u001b[36m0.0002\u001b[0m        0.0002  8.4953\n",
      "     93        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0001\u001b[0m  8.5176\n",
      "     94        0.0003        0.0001  8.4873\n",
      "     95        0.0003        0.0003  8.5188\n",
      "     96        \u001b[36m0.0002\u001b[0m        0.0001  8.4685\n",
      "     97        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0001\u001b[0m  8.5018\n",
      "     98        0.0002        0.0002  8.4416\n",
      "     99        0.0002        0.0002  8.4134\n",
      "    100        \u001b[36m0.0002\u001b[0m        0.0003  8.4776\n",
      "    101        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0001\u001b[0m  8.4489\n",
      "    102        0.0002        \u001b[32m0.0001\u001b[0m  8.4453\n",
      "    103        \u001b[36m0.0002\u001b[0m        0.0001  8.4394\n",
      "    104        0.0002        0.0001  8.4905\n",
      "    105        0.0002        \u001b[32m0.0001\u001b[0m  8.4686\n",
      "    106        0.0002        0.0004  8.4576\n",
      "    107        0.0002        0.0002  8.4659\n",
      "    108        0.0002        0.0002  8.4817\n",
      "    109        \u001b[36m0.0002\u001b[0m        0.0002  8.4722\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.9839094126043202\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0118\u001b[0m  8.4745\n",
      "      2        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0116\u001b[0m  8.4923\n",
      "      3        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0115\u001b[0m  8.4576\n",
      "      4        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0114\u001b[0m  8.4742\n",
      "      5        \u001b[36m0.0115\u001b[0m        0.0115  8.4891\n",
      "      6        0.0115        \u001b[32m0.0114\u001b[0m  8.5003\n",
      "      7        \u001b[36m0.0115\u001b[0m        0.0115  8.4410\n",
      "      8        \u001b[36m0.0115\u001b[0m        0.0114  8.4443\n",
      "      9        \u001b[36m0.0115\u001b[0m        0.0115  8.4978\n",
      "     10        \u001b[36m0.0115\u001b[0m        0.0116  8.4814\n",
      "     11        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0114\u001b[0m  8.4598\n",
      "     12        \u001b[36m0.0115\u001b[0m        0.0115  8.4881\n",
      "     13        \u001b[36m0.0114\u001b[0m        0.0114  8.4877\n",
      "     14        \u001b[36m0.0114\u001b[0m        0.0115  8.4557\n",
      "     15        0.0114        \u001b[32m0.0114\u001b[0m  8.5084\n",
      "     16        0.0114        0.0114  8.5111\n",
      "     17        0.0114        \u001b[32m0.0114\u001b[0m  8.4702\n",
      "     18        \u001b[36m0.0114\u001b[0m        0.0114  8.4581\n",
      "     19        \u001b[36m0.0114\u001b[0m        0.0115  8.5138\n",
      "     20        0.0114        \u001b[32m0.0113\u001b[0m  8.4878\n",
      "     21        0.0114        0.0115  8.4472\n",
      "     22        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0113\u001b[0m  8.4858\n",
      "     23        \u001b[36m0.0114\u001b[0m        0.0113  8.4779\n",
      "     24        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0113\u001b[0m  8.5190\n",
      "     25        \u001b[36m0.0114\u001b[0m        0.0115  8.4792\n",
      "     26        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0113\u001b[0m  8.4045\n",
      "     27        \u001b[36m0.0114\u001b[0m        0.0117  8.2051\n",
      "     28        \u001b[36m0.0113\u001b[0m        0.0114  8.2260\n",
      "     29        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0111\u001b[0m  8.0706\n",
      "     30        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0109\u001b[0m  8.4546\n",
      "     31        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0104\u001b[0m  8.5048\n",
      "     32        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0097\u001b[0m  8.4659\n",
      "     33        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0087\u001b[0m  8.4878\n",
      "     34        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0086\u001b[0m  8.4964\n",
      "     35        \u001b[36m0.0078\u001b[0m        \u001b[32m0.0076\u001b[0m  8.4334\n",
      "     36        \u001b[36m0.0075\u001b[0m        \u001b[32m0.0074\u001b[0m  8.4734\n",
      "     37        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0061\u001b[0m  8.5073\n",
      "     38        \u001b[36m0.0068\u001b[0m        0.0063  8.4611\n",
      "     39        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0052\u001b[0m  8.5103\n",
      "     40        \u001b[36m0.0059\u001b[0m        \u001b[32m0.0052\u001b[0m  8.4323\n",
      "     41        \u001b[36m0.0054\u001b[0m        0.0053  8.4704\n",
      "     42        \u001b[36m0.0043\u001b[0m        \u001b[32m0.0045\u001b[0m  8.5035\n",
      "     43        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0031\u001b[0m  8.4921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     44        \u001b[36m0.0032\u001b[0m        \u001b[32m0.0027\u001b[0m  8.4561\n",
      "     45        \u001b[36m0.0030\u001b[0m        0.0044  8.1513\n",
      "     46        \u001b[36m0.0027\u001b[0m        \u001b[32m0.0019\u001b[0m  8.2539\n",
      "     47        \u001b[36m0.0025\u001b[0m        0.0021  8.1735\n",
      "     48        \u001b[36m0.0022\u001b[0m        0.0029  8.1612\n",
      "     49        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0012\u001b[0m  8.1696\n",
      "     50        \u001b[36m0.0019\u001b[0m        \u001b[32m0.0012\u001b[0m  8.1758\n",
      "     51        \u001b[36m0.0016\u001b[0m        \u001b[32m0.0011\u001b[0m  8.3338\n",
      "     52        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0011\u001b[0m  8.3101\n",
      "     53        \u001b[36m0.0013\u001b[0m        \u001b[32m0.0009\u001b[0m  8.1564\n",
      "     54        \u001b[36m0.0012\u001b[0m        0.0013  8.1379\n",
      "     55        \u001b[36m0.0011\u001b[0m        0.0010  8.1326\n",
      "     56        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0009\u001b[0m  8.2144\n",
      "     57        \u001b[36m0.0009\u001b[0m        0.0011  8.1789\n",
      "     58        \u001b[36m0.0008\u001b[0m        0.0013  8.2154\n",
      "     59        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0006\u001b[0m  8.2428\n",
      "     60        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0005\u001b[0m  8.0674\n",
      "     61        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0004\u001b[0m  8.0783\n",
      "     62        \u001b[36m0.0006\u001b[0m        0.0008  8.1658\n",
      "     63        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0003\u001b[0m  8.1147\n",
      "     64        \u001b[36m0.0006\u001b[0m        0.0004  8.3660\n",
      "     65        \u001b[36m0.0005\u001b[0m        0.0008  8.3781\n",
      "     66        \u001b[36m0.0005\u001b[0m        0.0006  8.0392\n",
      "     67        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0003\u001b[0m  8.2482\n",
      "     68        \u001b[36m0.0004\u001b[0m        0.0007  8.0925\n",
      "     69        0.0004        0.0004  8.0882\n",
      "     70        0.0004        0.0010  8.3311\n",
      "     71        0.0004        \u001b[32m0.0002\u001b[0m  8.1535\n",
      "     72        \u001b[36m0.0003\u001b[0m        0.0004  8.1811\n",
      "     73        0.0004        0.0005  8.2282\n",
      "     74        \u001b[36m0.0003\u001b[0m        0.0002  8.0483\n",
      "     75        \u001b[36m0.0003\u001b[0m        0.0002  8.2619\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 0.983735794421903\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0133\u001b[0m  7.4322\n",
      "      2        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0127\u001b[0m  8.2042\n",
      "      3        \u001b[36m0.0130\u001b[0m        0.0128  7.6505\n",
      "      4        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0126\u001b[0m  8.2406\n",
      "      5        \u001b[36m0.0128\u001b[0m        0.0129  8.2196\n",
      "      6        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0117\u001b[0m  8.2553\n",
      "      7        \u001b[36m0.0124\u001b[0m        0.0147  7.4200\n",
      "      8        \u001b[36m0.0110\u001b[0m        0.0121  8.2509\n",
      "      9        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0089\u001b[0m  7.6753\n",
      "     10        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0084\u001b[0m  8.2401\n",
      "     11        \u001b[36m0.0089\u001b[0m        0.0085  7.6791\n",
      "     12        \u001b[36m0.0084\u001b[0m        \u001b[32m0.0075\u001b[0m  8.2278\n",
      "     13        \u001b[36m0.0081\u001b[0m        0.0104  8.2515\n",
      "     14        \u001b[36m0.0076\u001b[0m        \u001b[32m0.0071\u001b[0m  8.2990\n",
      "     15        \u001b[36m0.0075\u001b[0m        \u001b[32m0.0065\u001b[0m  8.0083\n",
      "     16        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0061\u001b[0m  8.3357\n",
      "     17        \u001b[36m0.0068\u001b[0m        \u001b[32m0.0057\u001b[0m  7.4256\n",
      "     18        \u001b[36m0.0062\u001b[0m        \u001b[32m0.0042\u001b[0m  8.3393\n",
      "     19        \u001b[36m0.0050\u001b[0m        \u001b[32m0.0041\u001b[0m  7.6147\n",
      "     20        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0033\u001b[0m  8.3175\n",
      "     21        \u001b[36m0.0031\u001b[0m        0.0045  7.4367\n",
      "     22        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0016\u001b[0m  8.3029\n",
      "     23        \u001b[36m0.0018\u001b[0m        \u001b[32m0.0011\u001b[0m  7.7401\n",
      "     24        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0006\u001b[0m  8.3956\n",
      "     25        \u001b[36m0.0011\u001b[0m        0.0008  7.7901\n",
      "     26        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0004\u001b[0m  7.7781\n",
      "     27        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0004\u001b[0m  7.4263\n",
      "     28        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0004\u001b[0m  8.3129\n",
      "     29        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0003\u001b[0m  7.7052\n",
      "     30        \u001b[36m0.0005\u001b[0m        0.0005  8.3595\n",
      "     31        \u001b[36m0.0005\u001b[0m        0.0005  7.7562\n",
      "     32        \u001b[36m0.0004\u001b[0m        0.0003  7.6926\n",
      "     33        0.0004        \u001b[32m0.0002\u001b[0m  7.7388\n",
      "     34        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  8.2731\n",
      "     35        \u001b[36m0.0003\u001b[0m        0.0002  7.7428\n",
      "     36        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.2906\n",
      "     37        \u001b[36m0.0003\u001b[0m        0.0002  7.7396\n",
      "     38        0.0003        0.0006  8.3333\n",
      "     39        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  7.4538\n",
      "     40        0.0003        0.0003  8.3371\n",
      "     41        \u001b[36m0.0003\u001b[0m        0.0002  7.5272\n",
      "     42        0.0003        0.0004  8.3105\n",
      "     43        0.0003        0.0002  7.4476\n",
      "     44        0.0003        \u001b[32m0.0001\u001b[0m  8.3176\n",
      "     45        \u001b[36m0.0002\u001b[0m        0.0002  7.4389\n",
      "     46        \u001b[36m0.0002\u001b[0m        0.0002  8.3042\n",
      "     47        \u001b[36m0.0002\u001b[0m        0.0002  7.7562\n",
      "     48        \u001b[36m0.0002\u001b[0m        0.0002  8.3152\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of 0.9834765017269229\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0109\u001b[0m  8.2414\n",
      "      2        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0105\u001b[0m  8.5160\n",
      "      3        \u001b[36m0.0104\u001b[0m        0.0110  8.8181\n",
      "      4        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0101\u001b[0m  8.6233\n",
      "      5        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0099\u001b[0m  8.7890\n",
      "      6        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0098\u001b[0m  8.5948\n",
      "      7        \u001b[36m0.0098\u001b[0m        0.0098  8.8037\n",
      "      8        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0097\u001b[0m  8.5289\n",
      "      9        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0095\u001b[0m  8.6817\n",
      "     10        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0094\u001b[0m  8.2686\n",
      "     11        \u001b[36m0.0095\u001b[0m        0.0096  8.1880\n",
      "     12        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0093\u001b[0m  8.3265\n",
      "     13        \u001b[36m0.0093\u001b[0m        0.0096  8.3484\n",
      "     14        \u001b[36m0.0092\u001b[0m        \u001b[32m0.0091\u001b[0m  8.2225\n",
      "     15        \u001b[36m0.0092\u001b[0m        \u001b[32m0.0090\u001b[0m  8.0878\n",
      "     16        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0089\u001b[0m  8.2093\n",
      "     17        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0088\u001b[0m  8.2413\n",
      "     18        \u001b[36m0.0090\u001b[0m        0.0089  8.2823\n",
      "     19        \u001b[36m0.0090\u001b[0m        0.0089  8.2847\n",
      "     20        \u001b[36m0.0089\u001b[0m        \u001b[32m0.0087\u001b[0m  8.1446\n",
      "     21        \u001b[36m0.0089\u001b[0m        0.0089  8.1777\n",
      "     22        \u001b[36m0.0089\u001b[0m        0.0088  8.1524\n",
      "     23        \u001b[36m0.0089\u001b[0m        0.0088  8.1448\n",
      "     24        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0087\u001b[0m  8.2103\n",
      "     25        \u001b[36m0.0088\u001b[0m        0.0088  8.2639\n",
      "     26        \u001b[36m0.0088\u001b[0m        0.0094  8.1746\n",
      "     27        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0087\u001b[0m  8.1755\n",
      "     28        \u001b[36m0.0088\u001b[0m        0.0088  8.3168\n",
      "     29        0.0088        0.0088  8.3604\n",
      "     30        \u001b[36m0.0088\u001b[0m        0.0087  8.1968\n",
      "     31        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0087\u001b[0m  8.2621\n",
      "     32        \u001b[36m0.0088\u001b[0m        0.0087  8.2235\n",
      "     33        \u001b[36m0.0088\u001b[0m        0.0087  8.1490\n",
      "     34        \u001b[36m0.0088\u001b[0m        0.0087  8.2784\n",
      "     35        0.0088        0.0089  8.1869\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.3223486607071189\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0115\u001b[0m  8.3450\n",
      "      2        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0112\u001b[0m  8.2347\n",
      "      3        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0111\u001b[0m  8.2268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0110\u001b[0m  8.2480\n",
      "      5        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0109\u001b[0m  8.2592\n",
      "      6        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0109\u001b[0m  8.2750\n",
      "      7        \u001b[36m0.0109\u001b[0m        0.0110  8.1360\n",
      "      8        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0108\u001b[0m  8.1456\n",
      "      9        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0107\u001b[0m  8.1670\n",
      "     10        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0105\u001b[0m  8.4143\n",
      "     11        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0103\u001b[0m  8.1624\n",
      "     12        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0101\u001b[0m  8.1502\n",
      "     13        \u001b[36m0.0102\u001b[0m        0.0103  8.1926\n",
      "     14        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0100\u001b[0m  8.1602\n",
      "     15        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0097\u001b[0m  8.2635\n",
      "     16        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0096\u001b[0m  8.3262\n",
      "     17        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0095\u001b[0m  8.3389\n",
      "     18        \u001b[36m0.0097\u001b[0m        0.0097  8.3587\n",
      "     19        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0092\u001b[0m  8.2663\n",
      "     20        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0086\u001b[0m  8.1322\n",
      "     21        \u001b[36m0.0089\u001b[0m        \u001b[32m0.0086\u001b[0m  8.2518\n",
      "     22        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0069\u001b[0m  8.1496\n",
      "     23        \u001b[36m0.0067\u001b[0m        \u001b[32m0.0050\u001b[0m  8.2725\n",
      "     24        \u001b[36m0.0047\u001b[0m        \u001b[32m0.0030\u001b[0m  8.1586\n",
      "     25        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0028\u001b[0m  8.3168\n",
      "     26        \u001b[36m0.0029\u001b[0m        \u001b[32m0.0023\u001b[0m  8.1692\n",
      "     27        \u001b[36m0.0026\u001b[0m        0.0025  8.2084\n",
      "     28        \u001b[36m0.0024\u001b[0m        0.0046  8.1921\n",
      "     29        \u001b[36m0.0021\u001b[0m        0.0025  8.2075\n",
      "     30        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0016\u001b[0m  8.2222\n",
      "     31        \u001b[36m0.0017\u001b[0m        \u001b[32m0.0011\u001b[0m  8.2207\n",
      "     32        0.0018        0.0016  8.3811\n",
      "     33        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0009\u001b[0m  8.1596\n",
      "     34        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0008\u001b[0m  8.1538\n",
      "     35        \u001b[36m0.0013\u001b[0m        0.0016  8.1741\n",
      "     36        \u001b[36m0.0012\u001b[0m        0.0019  8.1834\n",
      "     37        \u001b[36m0.0011\u001b[0m        0.0013  8.1456\n",
      "     38        \u001b[36m0.0011\u001b[0m        \u001b[32m0.0006\u001b[0m  8.1824\n",
      "     39        \u001b[36m0.0010\u001b[0m        0.0007  8.1655\n",
      "     40        \u001b[36m0.0010\u001b[0m        0.0009  8.2134\n",
      "     41        \u001b[36m0.0009\u001b[0m        0.0011  8.2320\n",
      "     42        0.0009        0.0010  8.2125\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.9217704066109963\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0089\u001b[0m  8.2566\n",
      "      2        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0083\u001b[0m  8.1315\n",
      "      3        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0080\u001b[0m  8.2876\n",
      "      4        \u001b[36m0.0080\u001b[0m        0.0081  8.0898\n",
      "      5        \u001b[36m0.0080\u001b[0m        0.0081  8.2740\n",
      "      6        \u001b[36m0.0080\u001b[0m        0.0081  8.1415\n",
      "      7        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0080\u001b[0m  8.3008\n",
      "      8        \u001b[36m0.0080\u001b[0m        0.0081  8.1599\n",
      "      9        \u001b[36m0.0080\u001b[0m        0.0080  8.2910\n",
      "     10        \u001b[36m0.0080\u001b[0m        0.0080  8.1578\n",
      "     11        \u001b[36m0.0080\u001b[0m        0.0080  8.1792\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.3811404038531915\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0161\u001b[0m  8.1649\n",
      "      2        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0160\u001b[0m  8.1645\n",
      "      3        \u001b[36m0.0162\u001b[0m        0.0160  8.1462\n",
      "      4        0.0162        \u001b[32m0.0159\u001b[0m  8.1501\n",
      "      5        \u001b[36m0.0162\u001b[0m        0.0159  8.1569\n",
      "      6        \u001b[36m0.0162\u001b[0m        0.0161  8.1604\n",
      "      7        0.0162        0.0159  8.1311\n",
      "      8        \u001b[36m0.0162\u001b[0m        0.0159  8.1388\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0016014146039864396\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0162\u001b[0m  8.1311\n",
      "      2        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  8.2833\n",
      "      3        0.0162        0.0165  8.2469\n",
      "      4        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0161\u001b[0m  8.3051\n",
      "      5        \u001b[36m0.0162\u001b[0m        0.0165  8.1529\n",
      "      6        0.0162        0.0162  8.1609\n",
      "      7        0.0162        \u001b[32m0.0161\u001b[0m  8.1550\n",
      "      8        0.0162        0.0161  8.3005\n",
      "      9        \u001b[36m0.0162\u001b[0m        0.0163  8.1635\n",
      "     10        \u001b[36m0.0162\u001b[0m        0.0161  8.1544\n",
      "     11        0.0162        0.0164  8.1596\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.017845908214464412\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0183\u001b[0m        \u001b[32m0.0160\u001b[0m  8.2960\n",
      "      2        \u001b[36m0.0163\u001b[0m        0.0161  8.1657\n",
      "      3        0.0163        0.0164  8.3339\n",
      "      4        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0160\u001b[0m  8.1676\n",
      "      5        \u001b[36m0.0163\u001b[0m        0.0161  8.2574\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.007085990657770047\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0160\u001b[0m  8.2194\n",
      "      2        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0157\u001b[0m  8.2280\n",
      "      3        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0153\u001b[0m  8.2192\n",
      "      4        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0147\u001b[0m  8.2302\n",
      "      5        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0137\u001b[0m  8.2298\n",
      "      6        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0122\u001b[0m  8.2388\n",
      "      7        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0116\u001b[0m  8.2580\n",
      "      8        \u001b[36m0.0119\u001b[0m        0.0135  8.2764\n",
      "      9        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0104\u001b[0m  8.1175\n",
      "     10        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0102\u001b[0m  8.2560\n",
      "     11        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0093\u001b[0m  8.1226\n",
      "     12        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0089\u001b[0m  8.4164\n",
      "     13        \u001b[36m0.0092\u001b[0m        0.0108  8.1773\n",
      "     14        \u001b[36m0.0085\u001b[0m        0.0129  8.1913\n",
      "     15        \u001b[36m0.0076\u001b[0m        \u001b[32m0.0050\u001b[0m  8.1839\n",
      "     16        \u001b[36m0.0065\u001b[0m        \u001b[32m0.0048\u001b[0m  8.1892\n",
      "     17        \u001b[36m0.0058\u001b[0m        0.0067  8.2249\n",
      "     18        \u001b[36m0.0051\u001b[0m        0.0069  8.1747\n",
      "     19        \u001b[36m0.0045\u001b[0m        \u001b[32m0.0038\u001b[0m  8.1948\n",
      "     20        \u001b[36m0.0041\u001b[0m        \u001b[32m0.0030\u001b[0m  8.3182\n",
      "     21        \u001b[36m0.0036\u001b[0m        0.0036  8.2362\n",
      "     22        \u001b[36m0.0031\u001b[0m        0.0053  8.3055\n",
      "     23        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0018\u001b[0m  8.1266\n",
      "     24        \u001b[36m0.0023\u001b[0m        0.0019  8.4073\n",
      "     25        \u001b[36m0.0020\u001b[0m        0.0021  8.2304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     26        \u001b[36m0.0017\u001b[0m        \u001b[32m0.0015\u001b[0m  8.1501\n",
      "     27        \u001b[36m0.0016\u001b[0m        0.0028  8.2809\n",
      "     28        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0007\u001b[0m  8.3303\n",
      "     29        \u001b[36m0.0013\u001b[0m        0.0021  8.3605\n",
      "     30        \u001b[36m0.0012\u001b[0m        \u001b[32m0.0006\u001b[0m  8.2730\n",
      "     31        \u001b[36m0.0010\u001b[0m        0.0009  8.0987\n",
      "     32        \u001b[36m0.0010\u001b[0m        0.0012  8.4213\n",
      "     33        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0005\u001b[0m  8.1611\n",
      "     34        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0004\u001b[0m  8.1626\n",
      "     35        0.0008        0.0005  8.2087\n",
      "     36        \u001b[36m0.0007\u001b[0m        0.0016  8.1553\n",
      "     37        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0004\u001b[0m  8.2202\n",
      "     38        0.0007        0.0008  8.2233\n",
      "     39        \u001b[36m0.0006\u001b[0m        0.0006  8.2398\n",
      "     40        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0003\u001b[0m  8.2560\n",
      "     41        0.0006        0.0007  8.3909\n",
      "     42        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0002\u001b[0m  8.1703\n",
      "     43        \u001b[36m0.0005\u001b[0m        0.0004  8.1988\n",
      "     44        \u001b[36m0.0004\u001b[0m        0.0006  8.1983\n",
      "     45        0.0005        0.0006  8.3047\n",
      "     46        \u001b[36m0.0004\u001b[0m        0.0007  8.2349\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.9838520355834017\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0134\u001b[0m  8.2979\n",
      "      2        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0131\u001b[0m  8.1834\n",
      "      3        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0130\u001b[0m  8.3149\n",
      "      4        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0129\u001b[0m  8.3160\n",
      "      5        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0129\u001b[0m  8.1844\n",
      "      6        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0127\u001b[0m  8.2113\n",
      "      7        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0127\u001b[0m  8.2057\n",
      "      8        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0126\u001b[0m  8.2156\n",
      "      9        \u001b[36m0.0126\u001b[0m        0.0128  8.2306\n",
      "     10        \u001b[36m0.0126\u001b[0m        0.0128  8.2221\n",
      "     11        \u001b[36m0.0125\u001b[0m        0.0126  8.2230\n",
      "     12        \u001b[36m0.0125\u001b[0m        0.0127  8.3907\n",
      "     13        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0120\u001b[0m  8.1308\n",
      "     14        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0097\u001b[0m  8.2541\n",
      "     15        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0090\u001b[0m  8.4310\n",
      "     16        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0083\u001b[0m  8.1525\n",
      "     17        \u001b[36m0.0067\u001b[0m        \u001b[32m0.0058\u001b[0m  8.1736\n",
      "     18        \u001b[36m0.0052\u001b[0m        \u001b[32m0.0038\u001b[0m  8.3085\n",
      "     19        \u001b[36m0.0042\u001b[0m        \u001b[32m0.0028\u001b[0m  8.3502\n",
      "     20        \u001b[36m0.0034\u001b[0m        \u001b[32m0.0025\u001b[0m  8.2391\n",
      "     21        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0020\u001b[0m  8.2596\n",
      "     22        \u001b[36m0.0019\u001b[0m        \u001b[32m0.0014\u001b[0m  8.1310\n",
      "     23        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0010\u001b[0m  8.2480\n",
      "     24        \u001b[36m0.0011\u001b[0m        \u001b[32m0.0006\u001b[0m  8.2573\n",
      "     25        \u001b[36m0.0009\u001b[0m        0.0010  8.2807\n",
      "     26        \u001b[36m0.0008\u001b[0m        0.0008  8.1619\n",
      "     27        \u001b[36m0.0007\u001b[0m        0.0011  8.3209\n",
      "     28        \u001b[36m0.0006\u001b[0m        0.0006  8.2253\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.9393481394187213\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0186\u001b[0m  8.1878\n",
      "      2        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0161\u001b[0m  8.1833\n",
      "      3        \u001b[36m0.0163\u001b[0m        0.0168  8.2268\n",
      "      4        \u001b[36m0.0162\u001b[0m        0.0171  8.2093\n",
      "      5        \u001b[36m0.0162\u001b[0m        0.0169  8.2132\n",
      "      6        0.0162        0.0161  8.2164\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0003194667013954877\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0165\u001b[0m  8.2116\n",
      "      2        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  8.2165\n",
      "      3        0.0162        0.0163  8.2349\n",
      "      4        0.0162        \u001b[32m0.0162\u001b[0m  8.4534\n",
      "      5        0.0162        0.0162  8.5020\n",
      "      6        0.0162        0.0165  8.4882\n",
      "      7        0.0162        0.0163  8.4827\n",
      "      8        0.0162        0.0162  8.4681\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.015685749023622186\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4616\n",
      "      2        \u001b[36m0.0156\u001b[0m        0.0153  8.5005\n",
      "      3        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0148\u001b[0m  8.5228\n",
      "      4        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0145\u001b[0m  8.4463\n",
      "      5        \u001b[36m0.0149\u001b[0m        0.0146  8.4737\n",
      "      6        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0143\u001b[0m  8.5274\n",
      "      7        \u001b[36m0.0147\u001b[0m        0.0145  8.4942\n",
      "      8        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0140\u001b[0m  8.4917\n",
      "      9        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0140\u001b[0m  8.4769\n",
      "     10        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0135\u001b[0m  8.5008\n",
      "     11        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0130\u001b[0m  8.4895\n",
      "     12        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0119\u001b[0m  8.4600\n",
      "     13        \u001b[36m0.0123\u001b[0m        0.0159  8.5083\n",
      "     14        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0101\u001b[0m  8.4297\n",
      "     15        \u001b[36m0.0106\u001b[0m        0.0193  8.4575\n",
      "     16        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0088\u001b[0m  8.5151\n",
      "     17        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0082\u001b[0m  8.4886\n",
      "     18        \u001b[36m0.0092\u001b[0m        0.0125  8.4591\n",
      "     19        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0068\u001b[0m  8.4429\n",
      "     20        \u001b[36m0.0078\u001b[0m        \u001b[32m0.0061\u001b[0m  8.5029\n",
      "     21        \u001b[36m0.0069\u001b[0m        \u001b[32m0.0057\u001b[0m  8.4930\n",
      "     22        \u001b[36m0.0061\u001b[0m        0.0082  8.1017\n",
      "     23        \u001b[36m0.0055\u001b[0m        \u001b[32m0.0040\u001b[0m  8.2033\n",
      "     24        \u001b[36m0.0048\u001b[0m        0.0057  8.2378\n",
      "     25        \u001b[36m0.0043\u001b[0m        0.0041  8.0527\n",
      "     26        \u001b[36m0.0040\u001b[0m        0.0049  8.2223\n",
      "     27        \u001b[36m0.0033\u001b[0m        \u001b[32m0.0026\u001b[0m  8.0508\n",
      "     28        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0017\u001b[0m  8.2255\n",
      "     29        \u001b[36m0.0029\u001b[0m        0.0053  8.2497\n",
      "     30        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0017\u001b[0m  8.2915\n",
      "     31        \u001b[36m0.0022\u001b[0m        \u001b[32m0.0015\u001b[0m  8.1383\n",
      "     32        \u001b[36m0.0019\u001b[0m        0.0022  8.1206\n",
      "     33        \u001b[36m0.0018\u001b[0m        0.0015  8.1306\n",
      "     34        \u001b[36m0.0016\u001b[0m        \u001b[32m0.0010\u001b[0m  8.0986\n",
      "     35        \u001b[36m0.0015\u001b[0m        0.0011  8.2763\n",
      "     36        \u001b[36m0.0013\u001b[0m        \u001b[32m0.0009\u001b[0m  8.1351\n",
      "     37        \u001b[36m0.0013\u001b[0m        \u001b[32m0.0006\u001b[0m  8.2584\n",
      "     38        \u001b[36m0.0011\u001b[0m        \u001b[32m0.0006\u001b[0m  8.0415\n",
      "     39        \u001b[36m0.0011\u001b[0m        0.0019  8.3680\n",
      "     40        \u001b[36m0.0009\u001b[0m        0.0010  8.2306\n",
      "     41        0.0010        0.0006  8.0556\n",
      "     42        \u001b[36m0.0009\u001b[0m        0.0007  8.0626\n",
      "     43        0.0009        \u001b[32m0.0005\u001b[0m  8.0383\n",
      "     44        \u001b[36m0.0008\u001b[0m        0.0018  8.2291\n",
      "     45        \u001b[36m0.0008\u001b[0m        0.0005  8.2446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     46        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0004\u001b[0m  8.0893\n",
      "     47        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0004\u001b[0m  8.0727\n",
      "     48        \u001b[36m0.0007\u001b[0m        0.0008  8.0885\n",
      "     49        \u001b[36m0.0007\u001b[0m        0.0007  8.0916\n",
      "     50        0.0007        0.0007  8.1029\n",
      "     51        \u001b[36m0.0006\u001b[0m        0.0005  8.0822\n",
      "     52        0.0006        \u001b[32m0.0003\u001b[0m  8.0714\n",
      "     53        \u001b[36m0.0006\u001b[0m        0.0004  8.1023\n",
      "     54        \u001b[36m0.0006\u001b[0m        0.0005  8.0922\n",
      "     55        \u001b[36m0.0005\u001b[0m        0.0005  8.1117\n",
      "     56        0.0006        0.0004  8.0822\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.9682233223722038\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0185\u001b[0m        \u001b[32m0.0162\u001b[0m  8.1339\n",
      "      2        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0157\u001b[0m  8.2928\n",
      "      3        \u001b[36m0.0151\u001b[0m        0.0181  8.1154\n",
      "      4        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0134\u001b[0m  8.2940\n",
      "      5        \u001b[36m0.0129\u001b[0m        0.0135  8.1315\n",
      "      6        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0125\u001b[0m  8.0967\n",
      "      7        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0123\u001b[0m  8.2201\n",
      "      8        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0120\u001b[0m  8.2099\n",
      "      9        \u001b[36m0.0119\u001b[0m        0.0120  8.2780\n",
      "     10        \u001b[36m0.0118\u001b[0m        0.0125  8.3953\n",
      "     11        \u001b[36m0.0117\u001b[0m        0.0122  8.2815\n",
      "     12        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0116\u001b[0m  8.2226\n",
      "     13        \u001b[36m0.0115\u001b[0m        0.0117  8.4042\n",
      "     14        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0115\u001b[0m  8.0272\n",
      "     15        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0115\u001b[0m  8.2268\n",
      "     16        \u001b[36m0.0111\u001b[0m        0.0118  8.4126\n",
      "     17        \u001b[36m0.0110\u001b[0m        0.0117  8.0960\n",
      "     18        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0111\u001b[0m  8.0772\n",
      "     19        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0101\u001b[0m  8.2751\n",
      "     20        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0098\u001b[0m  8.2905\n",
      "     21        \u001b[36m0.0099\u001b[0m        0.0106  8.1336\n",
      "     22        \u001b[36m0.0093\u001b[0m        0.0130  8.1387\n",
      "     23        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0071\u001b[0m  8.1203\n",
      "     24        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0049\u001b[0m  8.1222\n",
      "     25        \u001b[36m0.0066\u001b[0m        0.0102  8.1056\n",
      "     26        \u001b[36m0.0055\u001b[0m        0.0062  8.1162\n",
      "     27        \u001b[36m0.0046\u001b[0m        \u001b[32m0.0043\u001b[0m  8.2711\n",
      "     28        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0025\u001b[0m  8.1174\n",
      "     29        \u001b[36m0.0030\u001b[0m        0.0031  8.1307\n",
      "     30        \u001b[36m0.0024\u001b[0m        \u001b[32m0.0020\u001b[0m  8.1407\n",
      "     31        \u001b[36m0.0019\u001b[0m        0.0022  8.1172\n",
      "     32        \u001b[36m0.0016\u001b[0m        \u001b[32m0.0010\u001b[0m  8.1281\n",
      "     33        \u001b[36m0.0014\u001b[0m        0.0011  8.4561\n",
      "     34        \u001b[36m0.0012\u001b[0m        0.0010  8.4732\n",
      "     35        \u001b[36m0.0012\u001b[0m        0.0012  8.4564\n",
      "     36        \u001b[36m0.0011\u001b[0m        0.0011  8.4754\n",
      "     37        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0007\u001b[0m  8.5370\n",
      "     38        \u001b[36m0.0009\u001b[0m        0.0012  8.4655\n",
      "     39        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0006\u001b[0m  8.5057\n",
      "     40        0.0008        0.0010  8.4586\n",
      "     41        \u001b[36m0.0008\u001b[0m        0.0006  8.4960\n",
      "     42        \u001b[36m0.0007\u001b[0m        0.0008  8.4311\n",
      "     43        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0004\u001b[0m  8.5136\n",
      "     44        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0004\u001b[0m  8.4807\n",
      "     45        0.0006        0.0007  8.4344\n",
      "     46        \u001b[36m0.0006\u001b[0m        0.0008  8.5088\n",
      "     47        \u001b[36m0.0005\u001b[0m        0.0006  8.5180\n",
      "     48        0.0006        0.0010  8.5124\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.9538807670573103\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0161\u001b[0m  8.5131\n",
      "      2        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0160\u001b[0m  8.4103\n",
      "      3        \u001b[36m0.0160\u001b[0m        0.0162  8.4962\n",
      "      4        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0156\u001b[0m  8.4707\n",
      "      5        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4594\n",
      "      6        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0151\u001b[0m  8.4542\n",
      "      7        \u001b[36m0.0152\u001b[0m        0.0158  8.4979\n",
      "      8        \u001b[36m0.0150\u001b[0m        0.0167  8.4634\n",
      "      9        \u001b[36m0.0150\u001b[0m        0.0154  8.4153\n",
      "     10        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0150\u001b[0m  8.7134\n",
      "     11        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0144\u001b[0m  8.7242\n",
      "     12        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0140\u001b[0m  8.6240\n",
      "     13        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0133\u001b[0m  8.4973\n",
      "     14        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0115\u001b[0m  8.5741\n",
      "     15        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0112\u001b[0m  8.8066\n",
      "     16        \u001b[36m0.0127\u001b[0m        0.0116  8.4928\n",
      "     17        \u001b[36m0.0122\u001b[0m        0.0179  8.8474\n",
      "     18        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0091\u001b[0m  8.4550\n",
      "     19        \u001b[36m0.0112\u001b[0m        0.0103  8.6090\n",
      "     20        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0084\u001b[0m  8.7819\n",
      "     21        \u001b[36m0.0101\u001b[0m        0.0106  8.1815\n",
      "     22        \u001b[36m0.0095\u001b[0m        0.0107  7.4696\n",
      "     23        \u001b[36m0.0088\u001b[0m        0.0089  8.0331\n",
      "     24        \u001b[36m0.0080\u001b[0m        \u001b[32m0.0060\u001b[0m  8.3441\n",
      "     25        \u001b[36m0.0073\u001b[0m        0.0127  8.0521\n",
      "     26        \u001b[36m0.0069\u001b[0m        0.0063  7.4681\n",
      "     27        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0049\u001b[0m  8.2663\n",
      "     28        \u001b[36m0.0057\u001b[0m        0.0068  7.8623\n",
      "     29        \u001b[36m0.0051\u001b[0m        \u001b[32m0.0048\u001b[0m  7.5761\n",
      "     30        \u001b[36m0.0045\u001b[0m        \u001b[32m0.0038\u001b[0m  7.9842\n",
      "     31        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0025\u001b[0m  7.5526\n",
      "     32        \u001b[36m0.0032\u001b[0m        0.0038  7.9106\n",
      "     33        \u001b[36m0.0027\u001b[0m        \u001b[32m0.0015\u001b[0m  7.4573\n",
      "     34        \u001b[36m0.0024\u001b[0m        0.0027  7.8649\n",
      "     35        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0010\u001b[0m  8.7544\n",
      "     36        \u001b[36m0.0018\u001b[0m        0.0018  8.7459\n",
      "     37        \u001b[36m0.0014\u001b[0m        0.0018  8.6526\n",
      "     38        \u001b[36m0.0013\u001b[0m        0.0012  8.5682\n",
      "     39        \u001b[36m0.0011\u001b[0m        0.0017  8.4834\n",
      "     40        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0007\u001b[0m  8.7807\n",
      "     41        \u001b[36m0.0009\u001b[0m        \u001b[32m0.0005\u001b[0m  8.5570\n",
      "     42        \u001b[36m0.0008\u001b[0m        0.0006  8.6373\n",
      "     43        \u001b[36m0.0007\u001b[0m        0.0010  8.4944\n",
      "     44        \u001b[36m0.0006\u001b[0m        0.0005  8.7505\n",
      "     45        \u001b[36m0.0006\u001b[0m        0.0012  8.4382\n",
      "     46        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0004\u001b[0m  8.6913\n",
      "     47        \u001b[36m0.0004\u001b[0m        0.0006  8.7669\n",
      "     48        0.0005        \u001b[32m0.0002\u001b[0m  8.5731\n",
      "     49        \u001b[36m0.0004\u001b[0m        0.0003  8.6780\n",
      "     50        0.0004        \u001b[32m0.0002\u001b[0m  8.5916\n",
      "     51        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  8.6764\n",
      "     52        \u001b[36m0.0003\u001b[0m        0.0003  8.5726\n",
      "     53        \u001b[36m0.0003\u001b[0m        0.0003  8.7907\n",
      "     54        0.0003        0.0003  8.4820\n",
      "     55        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.4014\n",
      "     56        \u001b[36m0.0003\u001b[0m        0.0003  8.2844\n",
      "     57        0.0003        0.0004  8.3614\n",
      "     58        \u001b[36m0.0002\u001b[0m        \u001b[32m0.0001\u001b[0m  8.1542\n",
      "     59        \u001b[36m0.0002\u001b[0m        0.0004  8.1724\n",
      "     60        \u001b[36m0.0002\u001b[0m        0.0002  8.1930\n",
      "     61        \u001b[36m0.0002\u001b[0m        0.0002  8.2283\n",
      "     62        \u001b[36m0.0002\u001b[0m        0.0002  8.2180\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.9902414542823619\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0160\u001b[0m  8.2659\n",
      "      2        \u001b[36m0.0159\u001b[0m        0.0160  8.4173\n",
      "      3        \u001b[36m0.0158\u001b[0m        0.0160  8.1496\n",
      "      4        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0159\u001b[0m  8.2196\n",
      "      5        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0158\u001b[0m  8.1524\n",
      "      6        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0155\u001b[0m  8.1496\n",
      "      7        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0152\u001b[0m  8.1307\n",
      "      8        \u001b[36m0.0151\u001b[0m        0.0155  8.1485\n",
      "      9        \u001b[36m0.0146\u001b[0m        0.0157  8.2489\n",
      "     10        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0143\u001b[0m  8.1413\n",
      "     11        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0113\u001b[0m  8.2534\n",
      "     12        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0097\u001b[0m  8.1790\n",
      "     13        \u001b[36m0.0097\u001b[0m        0.0145  8.2884\n",
      "     14        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0079\u001b[0m  8.1532\n",
      "     15        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0068\u001b[0m  8.3252\n",
      "     16        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0061\u001b[0m  8.1796\n",
      "     17        \u001b[36m0.0063\u001b[0m        \u001b[32m0.0054\u001b[0m  8.2401\n",
      "     18        \u001b[36m0.0057\u001b[0m        \u001b[32m0.0050\u001b[0m  8.2022\n",
      "     19        \u001b[36m0.0054\u001b[0m        \u001b[32m0.0046\u001b[0m  8.2198\n",
      "     20        \u001b[36m0.0050\u001b[0m        \u001b[32m0.0046\u001b[0m  8.2193\n",
      "     21        \u001b[36m0.0048\u001b[0m        0.0054  8.1550\n",
      "     22        \u001b[36m0.0045\u001b[0m        \u001b[32m0.0040\u001b[0m  8.2558\n",
      "     23        \u001b[36m0.0044\u001b[0m        \u001b[32m0.0038\u001b[0m  8.2592\n",
      "     24        \u001b[36m0.0042\u001b[0m        \u001b[32m0.0037\u001b[0m  8.1341\n",
      "     25        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0033\u001b[0m  8.1839\n",
      "     26        \u001b[36m0.0037\u001b[0m        0.0064  8.1216\n",
      "     27        \u001b[36m0.0035\u001b[0m        \u001b[32m0.0024\u001b[0m  8.1010\n",
      "     28        \u001b[36m0.0033\u001b[0m        0.0030  8.1154\n",
      "     29        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0022\u001b[0m  8.2896\n",
      "     30        \u001b[36m0.0028\u001b[0m        \u001b[32m0.0021\u001b[0m  8.1486\n",
      "     31        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0020\u001b[0m  8.1969\n",
      "     32        \u001b[36m0.0023\u001b[0m        0.0022  8.2160\n",
      "     33        \u001b[36m0.0021\u001b[0m        \u001b[32m0.0015\u001b[0m  8.1753\n",
      "     34        \u001b[36m0.0019\u001b[0m        0.0023  8.1790\n",
      "     35        \u001b[36m0.0017\u001b[0m        \u001b[32m0.0010\u001b[0m  8.1958\n",
      "     36        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0009\u001b[0m  8.2155\n",
      "     37        \u001b[36m0.0015\u001b[0m        \u001b[32m0.0007\u001b[0m  8.3975\n",
      "     38        \u001b[36m0.0013\u001b[0m        0.0016  8.2745\n",
      "     39        \u001b[36m0.0012\u001b[0m        0.0011  8.1590\n",
      "     40        \u001b[36m0.0011\u001b[0m        \u001b[32m0.0007\u001b[0m  8.2576\n",
      "     41        \u001b[36m0.0011\u001b[0m        0.0010  8.1787\n",
      "     42        \u001b[36m0.0010\u001b[0m        0.0008  8.2921\n",
      "     43        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0006\u001b[0m  8.2160\n",
      "     44        \u001b[36m0.0009\u001b[0m        0.0013  8.3713\n",
      "     45        \u001b[36m0.0008\u001b[0m        0.0011  8.2810\n",
      "     46        \u001b[36m0.0008\u001b[0m        0.0009  8.2814\n",
      "     47        \u001b[36m0.0008\u001b[0m        0.0007  8.1814\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.9387063008586499\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0146\u001b[0m  8.1865\n",
      "      2        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0143\u001b[0m  8.1897\n",
      "      3        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0139\u001b[0m  8.2107\n",
      "      4        \u001b[36m0.0136\u001b[0m        0.0145  8.2207\n",
      "      5        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0131\u001b[0m  8.2064\n",
      "      6        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0130\u001b[0m  8.2134\n",
      "      7        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0129\u001b[0m  8.1930\n",
      "      8        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0129\u001b[0m  8.2266\n",
      "      9        \u001b[36m0.0128\u001b[0m        0.0129  8.2403\n",
      "     10        \u001b[36m0.0128\u001b[0m        0.0131  8.2442\n",
      "     11        0.0128        0.0130  8.2465\n",
      "     12        0.0128        \u001b[32m0.0128\u001b[0m  8.3316\n",
      "     13        0.0128        0.0129  8.2809\n",
      "     14        0.0128        0.0129  8.1550\n",
      "     15        \u001b[36m0.0127\u001b[0m        0.0130  8.1510\n",
      "     16        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0128\u001b[0m  8.1631\n",
      "     17        \u001b[36m0.0127\u001b[0m        0.0129  8.2450\n",
      "     18        \u001b[36m0.0127\u001b[0m        0.0129  8.1727\n",
      "     19        0.0127        0.0129  8.2068\n",
      "     20        \u001b[36m0.0127\u001b[0m        0.0129  8.1536\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.19549426304073947\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0154\u001b[0m  8.2648\n",
      "      2        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0148\u001b[0m  8.1883\n",
      "      3        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0137\u001b[0m  8.3000\n",
      "      4        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0135\u001b[0m  8.1953\n",
      "      5        \u001b[36m0.0139\u001b[0m        0.0137  8.2173\n",
      "      6        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0134\u001b[0m  8.2078\n",
      "      7        \u001b[36m0.0137\u001b[0m        0.0136  8.2010\n",
      "      8        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0131\u001b[0m  8.2264\n",
      "      9        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0130\u001b[0m  8.0830\n",
      "     10        \u001b[36m0.0134\u001b[0m        0.0135  8.2869\n",
      "     11        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0124\u001b[0m  8.2447\n",
      "     12        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0119\u001b[0m  8.2472\n",
      "     13        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0118\u001b[0m  8.1155\n",
      "     14        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0116\u001b[0m  8.2599\n",
      "     15        \u001b[36m0.0120\u001b[0m        0.0117  8.1238\n",
      "     16        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0112\u001b[0m  8.3815\n",
      "     17        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0107\u001b[0m  8.3355\n",
      "     18        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0098\u001b[0m  8.3451\n",
      "     19        \u001b[36m0.0105\u001b[0m        0.0127  8.3008\n",
      "     20        \u001b[36m0.0098\u001b[0m        0.0136  8.1949\n",
      "     21        \u001b[36m0.0089\u001b[0m        0.0114  8.0119\n",
      "     22        \u001b[36m0.0076\u001b[0m        \u001b[32m0.0048\u001b[0m  8.1565\n",
      "     23        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0040\u001b[0m  8.1421\n",
      "     24        \u001b[36m0.0045\u001b[0m        0.0046  8.4495\n",
      "     25        \u001b[36m0.0034\u001b[0m        \u001b[32m0.0022\u001b[0m  8.4608\n",
      "     26        \u001b[36m0.0029\u001b[0m        0.0024  8.5097\n",
      "     27        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0020\u001b[0m  8.4154\n",
      "     28        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0019\u001b[0m  8.4307\n",
      "     29        \u001b[36m0.0018\u001b[0m        0.0029  8.4301\n",
      "     30        \u001b[36m0.0015\u001b[0m        0.0019  8.3982\n",
      "     31        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0018\u001b[0m  8.4302\n",
      "     32        \u001b[36m0.0012\u001b[0m        \u001b[32m0.0015\u001b[0m  8.4900\n",
      "     33        \u001b[36m0.0011\u001b[0m        \u001b[32m0.0009\u001b[0m  8.4923\n",
      "     34        \u001b[36m0.0010\u001b[0m        \u001b[32m0.0008\u001b[0m  8.5139\n",
      "     35        \u001b[36m0.0009\u001b[0m        0.0010  8.4456\n",
      "     36        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0005\u001b[0m  8.4486\n",
      "     37        \u001b[36m0.0008\u001b[0m        0.0012  8.4887\n",
      "     38        \u001b[36m0.0007\u001b[0m        \u001b[32m0.0004\u001b[0m  8.5189\n",
      "     39        \u001b[36m0.0007\u001b[0m        0.0004  8.4969\n",
      "     40        \u001b[36m0.0006\u001b[0m        0.0006  8.3970\n",
      "     41        \u001b[36m0.0006\u001b[0m        0.0005  8.4950\n",
      "     42        0.0006        \u001b[32m0.0003\u001b[0m  8.4575\n",
      "     43        \u001b[36m0.0005\u001b[0m        0.0003  8.4355\n",
      "     44        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0002\u001b[0m  8.4710\n",
      "     45        0.0005        0.0007  8.4457\n",
      "     46        \u001b[36m0.0005\u001b[0m        0.0008  8.5047\n",
      "     47        0.0005        0.0005  8.4693\n",
      "     48        \u001b[36m0.0004\u001b[0m        \u001b[32m0.0002\u001b[0m  8.4946\n",
      "     49        \u001b[36m0.0004\u001b[0m        0.0010  8.4796\n",
      "     50        0.0004        0.0002  8.4766\n",
      "     51        \u001b[36m0.0003\u001b[0m        0.0006  8.4694\n",
      "     52        0.0003        0.0002  8.4778\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.9796894060325189\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.5320\n",
      "      2        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0160\u001b[0m  8.5116\n",
      "      3        0.0160        \u001b[32m0.0159\u001b[0m  8.4507\n",
      "      4        \u001b[36m0.0160\u001b[0m        0.0163  8.5558\n",
      "      5        \u001b[36m0.0160\u001b[0m        0.0159  8.5054\n",
      "      6        0.0160        0.0159  8.4641\n",
      "      7        \u001b[36m0.0159\u001b[0m        0.0160  8.5197\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.0009075002289631318\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0159\u001b[0m  8.4654\n",
      "      2        \u001b[36m0.0159\u001b[0m        0.0159  8.4856\n",
      "      3        0.0159        0.0159  8.5194\n",
      "      4        \u001b[36m0.0159\u001b[0m        0.0162  8.4989\n",
      "      5        0.0159        0.0161  8.4566\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.019042268300407006\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0160\u001b[0m  8.4862\n",
      "      2        \u001b[36m0.0160\u001b[0m        0.0161  8.4679\n",
      "      3        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0159\u001b[0m  8.4144\n",
      "      4        \u001b[36m0.0159\u001b[0m        0.0159  8.4779\n",
      "      5        0.0159        0.0162  8.4457\n",
      "      6        0.0159        0.0160  8.4639\n",
      "      7        0.0159        0.0162  8.5066\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.02207414995412904\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0160\u001b[0m  8.4901\n",
      "      2        \u001b[36m0.0158\u001b[0m        0.0160  8.4478\n",
      "      3        \u001b[36m0.0158\u001b[0m        0.0162  8.4053\n",
      "      4        0.0158        0.0162  8.4548\n",
      "      5        \u001b[36m0.0158\u001b[0m        0.0165  8.4827\n",
      "      6        0.0158        \u001b[32m0.0159\u001b[0m  8.4526\n",
      "      7        0.0158        0.0159  8.4766\n",
      "      8        \u001b[36m0.0158\u001b[0m        0.0159  8.4734\n",
      "      9        \u001b[36m0.0158\u001b[0m        0.0159  8.4005\n",
      "     10        \u001b[36m0.0158\u001b[0m        0.0159  8.4990\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -5.514224717662053e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0161\u001b[0m  8.4813\n",
      "      2        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0159\u001b[0m  8.4434\n",
      "      3        0.0159        0.0166  8.4987\n",
      "      4        0.0159        0.0165  8.4382\n",
      "      5        \u001b[36m0.0159\u001b[0m        0.0162  8.4491\n",
      "      6        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0159\u001b[0m  8.4217\n",
      "      7        0.0159        0.0159  8.4999\n",
      "      8        \u001b[36m0.0159\u001b[0m        0.0159  8.4112\n",
      "      9        0.0159        0.0164  8.4487\n",
      "     10        0.0159        0.0160  8.4370\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.0012867998776209255\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0144\u001b[0m  8.0726\n",
      "      2        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0141\u001b[0m  8.0538\n",
      "      3        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0122\u001b[0m  8.2273\n",
      "      4        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0102\u001b[0m  8.2206\n",
      "      5        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0089\u001b[0m  8.1265\n",
      "      6        \u001b[36m0.0087\u001b[0m        0.0097  8.2155\n",
      "      7        \u001b[36m0.0084\u001b[0m        \u001b[32m0.0081\u001b[0m  8.0236\n",
      "      8        \u001b[36m0.0082\u001b[0m        \u001b[32m0.0079\u001b[0m  8.1960\n",
      "      9        \u001b[36m0.0081\u001b[0m        \u001b[32m0.0078\u001b[0m  8.3595\n",
      "     10        \u001b[36m0.0080\u001b[0m        0.0079  8.1608\n",
      "     11        \u001b[36m0.0078\u001b[0m        \u001b[32m0.0077\u001b[0m  8.0431\n",
      "     12        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0074\u001b[0m  8.2207\n",
      "     13        \u001b[36m0.0075\u001b[0m        0.0077  8.4752\n",
      "     14        \u001b[36m0.0073\u001b[0m        0.0078  8.4863\n",
      "     15        \u001b[36m0.0072\u001b[0m        \u001b[32m0.0072\u001b[0m  8.4574\n",
      "     16        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0071\u001b[0m  8.4005\n",
      "     17        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0067\u001b[0m  8.4958\n",
      "     18        \u001b[36m0.0069\u001b[0m        \u001b[32m0.0064\u001b[0m  8.4910\n",
      "     19        \u001b[36m0.0067\u001b[0m        \u001b[32m0.0063\u001b[0m  8.4608\n",
      "     20        \u001b[36m0.0066\u001b[0m        0.0064  8.4638\n",
      "     21        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0061\u001b[0m  8.4629\n",
      "     22        \u001b[36m0.0063\u001b[0m        \u001b[32m0.0059\u001b[0m  8.4849\n",
      "     23        0.0063        0.0067  8.4491\n",
      "     24        \u001b[36m0.0060\u001b[0m        \u001b[32m0.0056\u001b[0m  8.4524\n",
      "     25        \u001b[36m0.0058\u001b[0m        \u001b[32m0.0050\u001b[0m  8.4865\n",
      "     26        \u001b[36m0.0056\u001b[0m        0.0068  8.4620\n",
      "     27        \u001b[36m0.0053\u001b[0m        0.0074  8.4499\n",
      "     28        \u001b[36m0.0050\u001b[0m        \u001b[32m0.0043\u001b[0m  8.5019\n",
      "     29        \u001b[36m0.0044\u001b[0m        \u001b[32m0.0024\u001b[0m  8.4874\n",
      "     30        \u001b[36m0.0036\u001b[0m        0.0057  8.4796\n",
      "     31        \u001b[36m0.0028\u001b[0m        0.0035  8.4298\n",
      "     32        \u001b[36m0.0020\u001b[0m        \u001b[32m0.0024\u001b[0m  8.4155\n",
      "     33        \u001b[36m0.0016\u001b[0m        \u001b[32m0.0011\u001b[0m  8.4586\n",
      "     34        \u001b[36m0.0013\u001b[0m        \u001b[32m0.0008\u001b[0m  8.4986\n",
      "     35        \u001b[36m0.0012\u001b[0m        0.0012  8.4654\n",
      "     36        \u001b[36m0.0011\u001b[0m        0.0009  8.4972\n",
      "     37        \u001b[36m0.0010\u001b[0m        0.0013  8.4964\n",
      "     38        \u001b[36m0.0009\u001b[0m        0.0010  8.4564\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.9274112597032071\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0156\u001b[0m  8.4467\n",
      "      2        \u001b[36m0.0157\u001b[0m        0.0158  8.4405\n",
      "      3        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0154\u001b[0m  8.4858\n",
      "      4        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4881\n",
      "      5        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0144\u001b[0m  8.5114\n",
      "      6        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0132\u001b[0m  8.4292\n",
      "      7        \u001b[36m0.0130\u001b[0m        0.0150  8.4853\n",
      "      8        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0118\u001b[0m  8.4515\n",
      "      9        \u001b[36m0.0122\u001b[0m        0.0122  8.4348\n",
      "     10        \u001b[36m0.0093\u001b[0m        0.0127  8.5004\n",
      "     11        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0098\u001b[0m  8.4197\n",
      "     12        \u001b[36m0.0074\u001b[0m        \u001b[32m0.0067\u001b[0m  8.4478\n",
      "     13        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0060\u001b[0m  8.4479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        \u001b[36m0.0066\u001b[0m        0.0066  8.5177\n",
      "     15        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0056\u001b[0m  8.4675\n",
      "     16        \u001b[36m0.0055\u001b[0m        \u001b[32m0.0044\u001b[0m  8.4217\n",
      "     17        \u001b[36m0.0050\u001b[0m        0.0046  8.4909\n",
      "     18        \u001b[36m0.0045\u001b[0m        \u001b[32m0.0044\u001b[0m  8.4745\n",
      "     19        \u001b[36m0.0040\u001b[0m        \u001b[32m0.0031\u001b[0m  8.5194\n",
      "     20        \u001b[36m0.0036\u001b[0m        0.0032  8.4895\n",
      "     21        \u001b[36m0.0033\u001b[0m        \u001b[32m0.0027\u001b[0m  8.4474\n",
      "     22        \u001b[36m0.0030\u001b[0m        \u001b[32m0.0023\u001b[0m  8.4638\n",
      "     23        \u001b[36m0.0027\u001b[0m        0.0040  8.5066\n",
      "     24        \u001b[36m0.0025\u001b[0m        \u001b[32m0.0021\u001b[0m  8.3542\n",
      "     25        \u001b[36m0.0022\u001b[0m        0.0025  7.9793\n",
      "     26        \u001b[36m0.0019\u001b[0m        0.0025  8.2044\n",
      "     27        \u001b[36m0.0017\u001b[0m        0.0024  8.1779\n",
      "     28        \u001b[36m0.0016\u001b[0m        \u001b[32m0.0015\u001b[0m  8.3026\n",
      "     29        \u001b[36m0.0014\u001b[0m        \u001b[32m0.0009\u001b[0m  8.3317\n",
      "     30        \u001b[36m0.0012\u001b[0m        \u001b[32m0.0008\u001b[0m  8.0382\n",
      "     31        \u001b[36m0.0012\u001b[0m        0.0017  8.1638\n",
      "     32        \u001b[36m0.0010\u001b[0m        0.0017  8.0671\n",
      "     33        0.0010        \u001b[32m0.0006\u001b[0m  8.2490\n",
      "     34        \u001b[36m0.0009\u001b[0m        0.0011  8.2418\n",
      "     35        \u001b[36m0.0008\u001b[0m        \u001b[32m0.0006\u001b[0m  8.0980\n",
      "     36        \u001b[36m0.0008\u001b[0m        0.0006  8.2782\n",
      "     37        \u001b[36m0.0007\u001b[0m        0.0009  8.1217\n",
      "     38        \u001b[36m0.0007\u001b[0m        0.0009  8.4887\n",
      "     39        \u001b[36m0.0006\u001b[0m        \u001b[32m0.0005\u001b[0m  8.1628\n",
      "     40        \u001b[36m0.0005\u001b[0m        0.0007  8.4807\n",
      "     41        \u001b[36m0.0005\u001b[0m        \u001b[32m0.0003\u001b[0m  7.9918\n",
      "     42        0.0005        0.0003  8.0196\n",
      "     43        \u001b[36m0.0004\u001b[0m        0.0004  8.1087\n",
      "     44        \u001b[36m0.0004\u001b[0m        0.0006  8.0165\n",
      "     45        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.1217\n",
      "     46        \u001b[36m0.0003\u001b[0m        0.0007  8.0610\n",
      "     47        \u001b[36m0.0003\u001b[0m        0.0002  8.4044\n",
      "     48        \u001b[36m0.0003\u001b[0m        \u001b[32m0.0002\u001b[0m  8.4423\n",
      "     49        \u001b[36m0.0003\u001b[0m        0.0003  8.5077\n",
      "     50        0.0003        0.0006  8.4759\n",
      "     51        \u001b[36m0.0002\u001b[0m        0.0004  8.4541\n",
      "     52        0.0003        0.0003  8.4652\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.9881482459462941\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0186\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4454\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4622\n",
      "      3        \u001b[36m0.0168\u001b[0m        0.0166  8.5359\n",
      "      4        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0164\u001b[0m  8.4641\n",
      "      5        \u001b[36m0.0168\u001b[0m        0.0166  8.4308\n",
      "      6        0.0168        0.0165  8.4676\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0165  8.5068\n",
      "      8        0.0168        0.0165  8.4718\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0029601172461806158\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0183\u001b[0m  8.4270\n",
      "      2        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  8.4712\n",
      "      3        \u001b[36m0.0166\u001b[0m        0.0165  8.5001\n",
      "      4        \u001b[36m0.0166\u001b[0m        0.0165  8.4248\n",
      "      5        \u001b[36m0.0166\u001b[0m        0.0165  8.5206\n",
      "      6        0.0166        \u001b[32m0.0164\u001b[0m  8.4516\n",
      "      7        \u001b[36m0.0166\u001b[0m        0.0164  8.4712\n",
      "      8        \u001b[36m0.0166\u001b[0m        0.0166  8.4573\n",
      "      9        0.0166        \u001b[32m0.0164\u001b[0m  8.4544\n",
      "     10        \u001b[36m0.0165\u001b[0m        0.0165  8.4764\n",
      "     11        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  8.4521\n",
      "     12        0.0165        0.0164  8.5469\n",
      "     13        0.0165        0.0166  8.3636\n",
      "     14        0.0165        0.0164  8.4698\n",
      "     15        \u001b[36m0.0165\u001b[0m        0.0164  8.3961\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.016590296564038653\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0171\u001b[0m  8.4317\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0169\u001b[0m  8.4611\n",
      "      3        \u001b[36m0.0168\u001b[0m        0.0170  8.4290\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0169  8.4501\n",
      "      5        \u001b[36m0.0168\u001b[0m        0.0171  8.4752\n",
      "      6        \u001b[36m0.0167\u001b[0m        0.0170  8.4794\n",
      "      7        0.0168        \u001b[32m0.0169\u001b[0m  8.6995\n",
      "      8        0.0168        0.0169  8.4235\n",
      "      9        \u001b[36m0.0167\u001b[0m        0.0169  8.4341\n",
      "     10        \u001b[36m0.0167\u001b[0m        0.0169  8.5315\n",
      "     11        0.0168        \u001b[32m0.0169\u001b[0m  8.4699\n",
      "     12        \u001b[36m0.0167\u001b[0m        0.0169  8.4714\n",
      "     13        0.0167        0.0170  8.4444\n",
      "     14        \u001b[36m0.0167\u001b[0m        0.0169  8.4727\n",
      "     15        0.0167        \u001b[32m0.0169\u001b[0m  8.4693\n",
      "     16        \u001b[36m0.0167\u001b[0m        0.0172  8.4516\n",
      "     17        0.0167        0.0169  8.4771\n",
      "     18        0.0167        0.0170  8.4108\n",
      "     19        \u001b[36m0.0167\u001b[0m        0.0171  8.4736\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.000793325409537804\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0171\u001b[0m  8.4213\n",
      "      2        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0169\u001b[0m  8.4318\n",
      "      3        0.0168        \u001b[32m0.0166\u001b[0m  8.4447\n",
      "      4        0.0168        \u001b[32m0.0164\u001b[0m  8.4894\n",
      "      5        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  8.6110\n",
      "      6        \u001b[36m0.0167\u001b[0m        0.0166  8.4237\n",
      "      7        0.0167        \u001b[32m0.0164\u001b[0m  8.5556\n",
      "      8        \u001b[36m0.0167\u001b[0m        0.0166  8.4862\n",
      "      9        \u001b[36m0.0167\u001b[0m        0.0164  8.2323\n",
      "     10        0.0167        0.0164  8.3247\n",
      "     11        0.0167        \u001b[32m0.0164\u001b[0m  8.4631\n",
      "     12        \u001b[36m0.0167\u001b[0m        0.0167  8.4090\n",
      "     13        \u001b[36m0.0167\u001b[0m        0.0164  8.4381\n",
      "     14        \u001b[36m0.0167\u001b[0m        0.0166  8.6835\n",
      "     15        0.0167        0.0164  8.3910\n",
      "     16        0.0167        \u001b[32m0.0164\u001b[0m  8.4630\n",
      "     17        \u001b[36m0.0167\u001b[0m        0.0164  8.3326\n",
      "     18        0.0167        0.0166  8.5695\n",
      "     19        \u001b[36m0.0167\u001b[0m        0.0165  8.6827\n",
      "     20        \u001b[36m0.0167\u001b[0m        0.0164  8.5379\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.0054964065331385425\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0170\u001b[0m  7.5404\n",
      "      2        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  7.1677\n",
      "      3        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.0166\u001b[0m        0.0166  7.9603\n",
      "      5        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  8.1202\n",
      "      6        \u001b[36m0.0165\u001b[0m        0.0163  8.2660\n",
      "      7        \u001b[36m0.0165\u001b[0m        0.0170  7.7701\n",
      "      8        \u001b[36m0.0165\u001b[0m        0.0164  8.0026\n",
      "      9        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  8.2948\n",
      "     10        \u001b[36m0.0165\u001b[0m        0.0164  8.3284\n",
      "     11        \u001b[36m0.0165\u001b[0m        0.0164  8.4215\n",
      "     12        \u001b[36m0.0165\u001b[0m        0.0163  8.4270\n",
      "     13        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3439\n",
      "     14        \u001b[36m0.0164\u001b[0m        0.0165  8.3870\n",
      "     15        0.0164        0.0163  8.5475\n",
      "     16        \u001b[36m0.0164\u001b[0m        0.0164  8.4121\n",
      "     17        0.0164        \u001b[32m0.0163\u001b[0m  8.3905\n",
      "     18        \u001b[36m0.0164\u001b[0m        0.0167  8.4518\n",
      "     19        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3889\n",
      "     20        \u001b[36m0.0164\u001b[0m        0.0163  8.7132\n",
      "     21        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.4365\n",
      "     22        0.0164        \u001b[32m0.0163\u001b[0m  8.7121\n",
      "     23        \u001b[36m0.0164\u001b[0m        0.0163  8.3538\n",
      "     24        0.0164        0.0164  8.4166\n",
      "     25        \u001b[36m0.0164\u001b[0m        0.0163  8.6481\n",
      "     26        \u001b[36m0.0164\u001b[0m        0.0164  8.6135\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.01814317542185029\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4482\n",
      "      2        \u001b[36m0.0162\u001b[0m        0.0165  8.3506\n",
      "      3        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0164\u001b[0m  8.1151\n",
      "      4        \u001b[36m0.0162\u001b[0m        0.0165  8.3936\n",
      "      5        \u001b[36m0.0162\u001b[0m        0.0164  8.6285\n",
      "      6        \u001b[36m0.0162\u001b[0m        0.0164  8.4411\n",
      "      7        0.0162        \u001b[32m0.0164\u001b[0m  8.2654\n",
      "      8        \u001b[36m0.0162\u001b[0m        0.0164  8.0664\n",
      "      9        \u001b[36m0.0161\u001b[0m        0.0164  8.0812\n",
      "     10        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0164\u001b[0m  8.0941\n",
      "     11        \u001b[36m0.0161\u001b[0m        0.0165  8.0835\n",
      "     12        0.0161        0.0164  8.5620\n",
      "     13        \u001b[36m0.0161\u001b[0m        0.0164  8.3843\n",
      "     14        \u001b[36m0.0161\u001b[0m        0.0164  7.7324\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 0.03674275560109275\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3816\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0166\u001b[0m  8.4795\n",
      "      3        0.0169        0.0167  8.2836\n",
      "      4        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3952\n",
      "      5        0.0168        0.0166  8.3339\n",
      "      6        0.0168        0.0167  8.3373\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0169  8.3928\n",
      "      8        0.0168        0.0170  8.3058\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0010318596725158447\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3920\n",
      "      2        \u001b[36m0.0166\u001b[0m        0.0168  8.5053\n",
      "      3        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0167\u001b[0m  8.4327\n",
      "      4        \u001b[36m0.0165\u001b[0m        0.0170  8.3738\n",
      "      5        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3837\n",
      "      6        \u001b[36m0.0164\u001b[0m        0.0167  8.4301\n",
      "      7        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0166\u001b[0m  8.4302\n",
      "      8        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0165\u001b[0m  8.1736\n",
      "      9        \u001b[36m0.0164\u001b[0m        0.0166  8.3903\n",
      "     10        \u001b[36m0.0164\u001b[0m        0.0167  8.4621\n",
      "     11        \u001b[36m0.0164\u001b[0m        0.0165  8.3693\n",
      "     12        \u001b[36m0.0164\u001b[0m        0.0165  8.5205\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.017453162690007185\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0169\u001b[0m  8.4388\n",
      "      2        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0168\u001b[0m  8.5013\n",
      "      3        \u001b[36m0.0165\u001b[0m        0.0168  8.3774\n",
      "      4        \u001b[36m0.0165\u001b[0m        0.0172  8.2597\n",
      "      5        \u001b[36m0.0164\u001b[0m        0.0169  7.7000\n",
      "      6        0.0164        0.0169  7.6253\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.012962747481068293\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4247\n",
      "      2        \u001b[36m0.0169\u001b[0m        0.0166  8.2579\n",
      "      3        0.0169        0.0166  8.3806\n",
      "      4        \u001b[36m0.0169\u001b[0m        0.0172  8.3456\n",
      "      5        0.0169        \u001b[32m0.0165\u001b[0m  8.3318\n",
      "      6        0.0169        \u001b[32m0.0165\u001b[0m  8.3479\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0166  8.3344\n",
      "      8        0.0169        0.0166  8.3147\n",
      "      9        \u001b[36m0.0168\u001b[0m        0.0168  8.1189\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.016459562167437713\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0156\u001b[0m  8.3343\n",
      "      2        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0155\u001b[0m  8.3329\n",
      "      3        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0151\u001b[0m  8.3103\n",
      "      4        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0150\u001b[0m  8.3580\n",
      "      5        0.0152        \u001b[32m0.0150\u001b[0m  8.3261\n",
      "      6        \u001b[36m0.0152\u001b[0m        0.0150  8.3418\n",
      "      7        \u001b[36m0.0151\u001b[0m        0.0150  8.4453\n",
      "      8        \u001b[36m0.0151\u001b[0m        0.0154  8.4897\n",
      "      9        \u001b[36m0.0151\u001b[0m        0.0150  8.4616\n",
      "     10        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0149\u001b[0m  8.4583\n",
      "     11        \u001b[36m0.0150\u001b[0m        0.0149  8.4436\n",
      "     12        0.0150        \u001b[32m0.0149\u001b[0m  8.4877\n",
      "     13        \u001b[36m0.0150\u001b[0m        0.0149  8.3523\n",
      "     14        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0149\u001b[0m  8.3456\n",
      "     15        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0148\u001b[0m  8.3396\n",
      "     16        \u001b[36m0.0149\u001b[0m        0.0148  8.3562\n",
      "     17        \u001b[36m0.0149\u001b[0m        0.0149  8.3472\n",
      "     18        \u001b[36m0.0149\u001b[0m        0.0150  8.3274\n",
      "     19        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0148\u001b[0m  8.3642\n",
      "     20        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0148\u001b[0m  8.3738\n",
      "     21        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0148\u001b[0m  8.3641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22        \u001b[36m0.0149\u001b[0m        0.0148  8.3485\n",
      "     23        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0148\u001b[0m  8.6929\n",
      "     24        \u001b[36m0.0149\u001b[0m        0.0149  8.5148\n",
      "     25        0.0149        \u001b[32m0.0148\u001b[0m  8.3945\n",
      "     26        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0148\u001b[0m  8.5397\n",
      "     27        \u001b[36m0.0148\u001b[0m        0.0148  7.4726\n",
      "     28        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0147\u001b[0m  6.9259\n",
      "     29        \u001b[36m0.0148\u001b[0m        0.0147  6.7495\n",
      "     30        \u001b[36m0.0148\u001b[0m        0.0150  8.2272\n",
      "     31        \u001b[36m0.0148\u001b[0m        0.0150  8.2740\n",
      "     32        0.0148        0.0148  7.2204\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.09618812566626278\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0162\u001b[0m  5.9687\n",
      "      2        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0162\u001b[0m  7.3203\n",
      "      3        \u001b[36m0.0160\u001b[0m        0.0162  7.1927\n",
      "      4        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0162\u001b[0m  7.8365\n",
      "      5        \u001b[36m0.0160\u001b[0m        0.0164  7.2421\n",
      "      6        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0161\u001b[0m  7.5598\n",
      "      7        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0161\u001b[0m  7.2400\n",
      "      8        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0161\u001b[0m  7.2974\n",
      "      9        \u001b[36m0.0159\u001b[0m        0.0161  8.2961\n",
      "     10        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0160\u001b[0m  6.9606\n",
      "     11        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0158\u001b[0m  6.8585\n",
      "     12        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0156\u001b[0m  8.2826\n",
      "     13        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0155\u001b[0m  6.8558\n",
      "     14        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0149\u001b[0m  8.1051\n",
      "     15        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0147\u001b[0m  7.3638\n",
      "     16        \u001b[36m0.0146\u001b[0m        0.0147  7.9510\n",
      "     17        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0145\u001b[0m  6.7169\n",
      "     18        \u001b[36m0.0144\u001b[0m        0.0157  7.1489\n",
      "     19        0.0145        \u001b[32m0.0143\u001b[0m  8.3379\n",
      "     20        \u001b[36m0.0144\u001b[0m        0.0145  7.6725\n",
      "     21        \u001b[36m0.0143\u001b[0m        0.0146  7.3142\n",
      "     22        \u001b[36m0.0143\u001b[0m        0.0145  7.6095\n",
      "     23        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0143\u001b[0m  8.1129\n",
      "     24        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0142\u001b[0m  8.2362\n",
      "     25        \u001b[36m0.0142\u001b[0m        0.0142  8.6558\n",
      "     26        0.0142        0.0146  8.4784\n",
      "     27        \u001b[36m0.0141\u001b[0m        0.0144  8.0447\n",
      "     28        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0141\u001b[0m  8.4868\n",
      "     29        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0140\u001b[0m  8.1180\n",
      "     30        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0139\u001b[0m  6.7988\n",
      "     31        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0139\u001b[0m  7.8546\n",
      "     32        0.0139        \u001b[32m0.0138\u001b[0m  7.3801\n",
      "     33        \u001b[36m0.0138\u001b[0m        0.0140  8.1418\n",
      "     34        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0137\u001b[0m  6.1881\n",
      "     35        \u001b[36m0.0137\u001b[0m        0.0137  5.8896\n",
      "     36        \u001b[36m0.0137\u001b[0m        \u001b[32m0.0137\u001b[0m  6.7197\n",
      "     37        \u001b[36m0.0136\u001b[0m        0.0137  6.4835\n",
      "     38        \u001b[36m0.0136\u001b[0m        0.0137  6.8060\n",
      "     39        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0137\u001b[0m  6.4718\n",
      "     40        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0134\u001b[0m  7.1228\n",
      "     41        \u001b[36m0.0134\u001b[0m        0.0134  7.4904\n",
      "     42        \u001b[36m0.0133\u001b[0m        0.0134  7.2128\n",
      "     43        \u001b[36m0.0133\u001b[0m        0.0141  6.6506\n",
      "     44        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0133\u001b[0m  8.0578\n",
      "     45        \u001b[36m0.0132\u001b[0m        0.0134  7.7228\n",
      "     46        \u001b[36m0.0131\u001b[0m        0.0136  7.6595\n",
      "     47        \u001b[36m0.0131\u001b[0m        0.0142  6.7753\n",
      "     48        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0128\u001b[0m  7.0958\n",
      "     49        \u001b[36m0.0129\u001b[0m        0.0133  7.5497\n",
      "     50        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0128\u001b[0m  6.4826\n",
      "     51        \u001b[36m0.0128\u001b[0m        0.0133  7.8602\n",
      "     52        \u001b[36m0.0128\u001b[0m        0.0129  7.3335\n",
      "     53        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0126\u001b[0m  6.4742\n",
      "     54        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0126\u001b[0m  6.2063\n",
      "     55        \u001b[36m0.0126\u001b[0m        0.0140  6.2519\n",
      "     56        \u001b[36m0.0125\u001b[0m        0.0132  7.9811\n",
      "     57        0.0125        \u001b[32m0.0123\u001b[0m  6.4082\n",
      "     58        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0123\u001b[0m  8.0335\n",
      "     59        \u001b[36m0.0124\u001b[0m        0.0125  8.0704\n",
      "     60        \u001b[36m0.0124\u001b[0m        0.0123  8.2719\n",
      "     61        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0122\u001b[0m  8.0146\n",
      "     62        \u001b[36m0.0123\u001b[0m        0.0123  8.2361\n",
      "     63        \u001b[36m0.0121\u001b[0m        0.0129  7.6254\n",
      "     64        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0120\u001b[0m  8.2411\n",
      "     65        \u001b[36m0.0120\u001b[0m        0.0120  8.2815\n",
      "     66        \u001b[36m0.0120\u001b[0m        0.0124  8.2793\n",
      "     67        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0119\u001b[0m  8.2546\n",
      "     68        \u001b[36m0.0119\u001b[0m        0.0124  8.3741\n",
      "     69        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0117\u001b[0m  8.3788\n",
      "     70        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0117\u001b[0m  8.3972\n",
      "     71        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0116\u001b[0m  8.5257\n",
      "     72        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0115\u001b[0m  8.6424\n",
      "     73        \u001b[36m0.0116\u001b[0m        0.0116  8.3021\n",
      "     74        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0114\u001b[0m  8.3680\n",
      "     75        \u001b[36m0.0114\u001b[0m        0.0124  8.1545\n",
      "     76        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0113\u001b[0m  8.2483\n",
      "     77        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0112\u001b[0m  8.1515\n",
      "     78        \u001b[36m0.0112\u001b[0m        0.0114  8.4290\n",
      "     79        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0109\u001b[0m  8.2364\n",
      "     80        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0108\u001b[0m  8.2281\n",
      "     81        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0107\u001b[0m  8.1291\n",
      "     82        0.0108        \u001b[32m0.0106\u001b[0m  8.5028\n",
      "     83        \u001b[36m0.0107\u001b[0m        0.0106  8.1236\n",
      "     84        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0105\u001b[0m  7.8930\n",
      "     85        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0105\u001b[0m  8.1276\n",
      "     86        \u001b[36m0.0106\u001b[0m        0.0109  7.0982\n",
      "     87        \u001b[36m0.0105\u001b[0m        0.0109  7.8686\n",
      "     88        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0102\u001b[0m  7.9552\n",
      "     89        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0102\u001b[0m  8.2552\n",
      "     90        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0100\u001b[0m  8.0206\n",
      "     91        \u001b[36m0.0103\u001b[0m        0.0104  8.0996\n",
      "     92        \u001b[36m0.0102\u001b[0m        0.0102  8.3416\n",
      "     93        \u001b[36m0.0102\u001b[0m        0.0101  8.6648\n",
      "     94        \u001b[36m0.0102\u001b[0m        0.0102  8.6437\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.36693436077586816\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0178\u001b[0m        \u001b[32m0.0163\u001b[0m  8.4412\n",
      "      2        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0162\u001b[0m  8.4746\n",
      "      3        0.0164        0.0163  8.3897\n",
      "      4        \u001b[36m0.0163\u001b[0m        0.0164  8.4439\n",
      "      5        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0162\u001b[0m  8.6207\n",
      "      6        0.0163        \u001b[32m0.0162\u001b[0m  8.3978\n",
      "      7        \u001b[36m0.0163\u001b[0m        0.0162  8.3275\n",
      "      8        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0161\u001b[0m  8.4458\n",
      "      9        \u001b[36m0.0162\u001b[0m        0.0162  8.5182\n",
      "     10        0.0162        0.0162  8.5668\n",
      "     11        \u001b[36m0.0162\u001b[0m        0.0162  8.3729\n",
      "     12        \u001b[36m0.0162\u001b[0m        0.0164  8.5918\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.00921795743426801\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0166\u001b[0m  8.5279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0165\u001b[0m  8.5391\n",
      "      3        \u001b[36m0.0164\u001b[0m        0.0165  8.1387\n",
      "      4        0.0164        0.0165  7.1507\n",
      "      5        0.0165        0.0165  6.7333\n",
      "      6        0.0165        \u001b[32m0.0165\u001b[0m  6.6140\n",
      "      7        0.0165        0.0165  6.6657\n",
      "      8        0.0165        0.0165  6.8179\n",
      "      9        0.0165        0.0165  8.0641\n",
      "     10        0.0165        0.0165  8.3955\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 1.7771717941705845e-07\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0181\u001b[0m        \u001b[32m0.0163\u001b[0m  8.4761\n",
      "      2        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0162\u001b[0m  7.8052\n",
      "      3        \u001b[36m0.0166\u001b[0m        0.0162  8.4827\n",
      "      4        0.0167        \u001b[32m0.0162\u001b[0m  8.5322\n",
      "      5        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0162\u001b[0m  8.2494\n",
      "      6        \u001b[36m0.0166\u001b[0m        0.0163  7.2872\n",
      "      7        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0162\u001b[0m  7.4467\n",
      "      8        \u001b[36m0.0165\u001b[0m        0.0163  8.1169\n",
      "      9        0.0165        0.0164  7.7361\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.009628068199678541\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0166\u001b[0m  8.5254\n",
      "      2        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4352\n",
      "      3        \u001b[36m0.0164\u001b[0m        0.0165  8.5194\n",
      "      4        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3715\n",
      "      5        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0164\u001b[0m  8.0579\n",
      "      6        \u001b[36m0.0163\u001b[0m        0.0165  8.0842\n",
      "      7        0.0163        \u001b[32m0.0164\u001b[0m  8.0763\n",
      "      8        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0164\u001b[0m  8.0906\n",
      "      9        0.0163        \u001b[32m0.0164\u001b[0m  8.4423\n",
      "     10        \u001b[36m0.0162\u001b[0m        0.0164  8.0473\n",
      "     11        0.0163        \u001b[32m0.0163\u001b[0m  8.2801\n",
      "     12        \u001b[36m0.0162\u001b[0m        0.0165  7.8640\n",
      "     13        0.0162        \u001b[32m0.0163\u001b[0m  8.5035\n",
      "     14        0.0162        0.0164  7.8645\n",
      "     15        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0163\u001b[0m  7.2533\n",
      "     16        \u001b[36m0.0162\u001b[0m        0.0163  7.7678\n",
      "     17        0.0162        0.0163  7.8648\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 0.011618890922700076\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0167\u001b[0m  7.2891\n",
      "      2        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0165\u001b[0m  8.2726\n",
      "      3        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0165\u001b[0m  7.9798\n",
      "      4        0.0164        \u001b[32m0.0164\u001b[0m  7.7828\n",
      "      5        0.0164        0.0164  7.7149\n",
      "      6        0.0165        0.0165  7.7231\n",
      "      7        \u001b[36m0.0164\u001b[0m        0.0167  8.3250\n",
      "      8        0.0164        0.0165  7.6600\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0019716865285717144\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0164\u001b[0m  7.4643\n",
      "      2        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  8.0453\n",
      "      3        0.0165        0.0165  7.8564\n",
      "      4        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  7.7781\n",
      "      5        0.0165        \u001b[32m0.0163\u001b[0m  8.2486\n",
      "      6        \u001b[36m0.0165\u001b[0m        0.0163  8.3354\n",
      "      7        0.0165        \u001b[32m0.0163\u001b[0m  8.2401\n",
      "      8        \u001b[36m0.0165\u001b[0m        0.0163  8.1979\n",
      "      9        0.0165        0.0163  8.4045\n",
      "     10        0.0165        0.0163  7.4436\n",
      "     11        0.0165        0.0163  6.7537\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.0008275103885933\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0166\u001b[0m  7.5465\n",
      "      2        \u001b[36m0.0165\u001b[0m        0.0166  8.4752\n",
      "      3        \u001b[36m0.0165\u001b[0m        0.0166  8.5003\n",
      "      4        0.0165        0.0166  8.6004\n",
      "      5        0.0165        0.0167  8.6186\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.003143691932176562\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0178\u001b[0m        \u001b[32m0.0165\u001b[0m  8.5742\n",
      "      2        \u001b[36m0.0167\u001b[0m        0.0166  8.2154\n",
      "      3        \u001b[36m0.0166\u001b[0m        0.0167  8.4081\n",
      "      4        \u001b[36m0.0166\u001b[0m        0.0170  8.3688\n",
      "      5        0.0166        \u001b[32m0.0165\u001b[0m  8.3771\n",
      "      6        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  8.0086\n",
      "      7        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  7.9907\n",
      "      8        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  7.0332\n",
      "      9        \u001b[36m0.0164\u001b[0m        0.0165  8.1757\n",
      "     10        \u001b[36m0.0164\u001b[0m        0.0166  8.1347\n",
      "     11        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  7.8535\n",
      "     12        0.0164        0.0167  7.3567\n",
      "     13        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  7.9632\n",
      "     14        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  6.8384\n",
      "     15        \u001b[36m0.0163\u001b[0m        0.0163  7.6448\n",
      "     16        \u001b[36m0.0163\u001b[0m        0.0167  7.8705\n",
      "     17        0.0163        0.0163  7.6971\n",
      "     18        0.0163        \u001b[32m0.0162\u001b[0m  7.2121\n",
      "     19        \u001b[36m0.0163\u001b[0m        0.0162  7.4467\n",
      "     20        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0162\u001b[0m  7.9138\n",
      "     21        \u001b[36m0.0163\u001b[0m        0.0165  7.9045\n",
      "     22        \u001b[36m0.0163\u001b[0m        0.0162  8.4835\n",
      "     23        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  8.4270\n",
      "     24        0.0163        0.0163  8.4380\n",
      "     25        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  8.4055\n",
      "     26        0.0162        0.0163  8.3612\n",
      "     27        0.0163        0.0163  8.4022\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.01613934489122748\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0177\u001b[0m        \u001b[32m0.0171\u001b[0m  8.3543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0173\u001b[0m        0.0181  7.9599\n",
      "      3        \u001b[36m0.0173\u001b[0m        0.0177  7.9905\n",
      "      4        \u001b[36m0.0172\u001b[0m        0.0171  8.2484\n",
      "      5        \u001b[36m0.0172\u001b[0m        0.0171  8.2005\n",
      "      6        0.0172        \u001b[32m0.0171\u001b[0m  8.3760\n",
      "      7        \u001b[36m0.0171\u001b[0m        0.0171  8.4524\n",
      "      8        0.0171        0.0171  8.0987\n",
      "      9        \u001b[36m0.0171\u001b[0m        0.0171  8.3193\n",
      "     10        0.0171        0.0171  8.0957\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -1.0758604849225861e-06\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0186\u001b[0m        \u001b[32m0.0170\u001b[0m  8.0362\n",
      "      2        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0168\u001b[0m  8.0058\n",
      "      3        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0164\u001b[0m  7.9618\n",
      "      4        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0159\u001b[0m  8.1803\n",
      "      5        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0156\u001b[0m  8.1450\n",
      "      6        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0155\u001b[0m  8.4198\n",
      "      7        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0154\u001b[0m  8.4253\n",
      "      8        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0153\u001b[0m  8.4881\n",
      "      9        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0153\u001b[0m  8.2686\n",
      "     10        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0152\u001b[0m  8.2644\n",
      "     11        0.0153        \u001b[32m0.0151\u001b[0m  7.9001\n",
      "     12        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0151\u001b[0m  8.0360\n",
      "     13        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0150\u001b[0m  8.3397\n",
      "     14        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0150\u001b[0m  8.3553\n",
      "     15        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0150\u001b[0m  8.4153\n",
      "     16        \u001b[36m0.0150\u001b[0m        0.0150  8.4133\n",
      "     17        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0150\u001b[0m  8.2101\n",
      "     18        0.0150        0.0162  8.3208\n",
      "     19        \u001b[36m0.0149\u001b[0m        0.0150  8.2535\n",
      "     20        \u001b[36m0.0148\u001b[0m        0.0150  8.3986\n",
      "     21        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0148\u001b[0m  8.2397\n",
      "     22        \u001b[36m0.0148\u001b[0m        0.0150  8.4249\n",
      "     23        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0147\u001b[0m  8.3981\n",
      "     24        0.0148        \u001b[32m0.0146\u001b[0m  8.2828\n",
      "     25        \u001b[36m0.0147\u001b[0m        0.0152  8.3302\n",
      "     26        0.0147        \u001b[32m0.0145\u001b[0m  8.2283\n",
      "     27        \u001b[36m0.0146\u001b[0m        0.0147  8.3102\n",
      "     28        \u001b[36m0.0145\u001b[0m        0.0146  8.5539\n",
      "     29        0.0145        \u001b[32m0.0144\u001b[0m  8.2671\n",
      "     30        \u001b[36m0.0145\u001b[0m        0.0145  8.4543\n",
      "     31        \u001b[36m0.0145\u001b[0m        0.0149  8.5001\n",
      "     32        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0143\u001b[0m  8.4144\n",
      "     33        \u001b[36m0.0143\u001b[0m        0.0143  8.5216\n",
      "     34        \u001b[36m0.0143\u001b[0m        0.0155  8.3972\n",
      "     35        \u001b[36m0.0142\u001b[0m        0.0146  8.5001\n",
      "     36        \u001b[36m0.0142\u001b[0m        0.0148  8.1764\n",
      "     37        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0140\u001b[0m  8.6555\n",
      "     38        \u001b[36m0.0141\u001b[0m        0.0148  8.2295\n",
      "     39        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0139\u001b[0m  7.8627\n",
      "     40        0.0141        \u001b[32m0.0139\u001b[0m  8.3815\n",
      "     41        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0139\u001b[0m  8.4083\n",
      "     42        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0138\u001b[0m  7.2628\n",
      "     43        0.0140        0.0139  8.3853\n",
      "     44        \u001b[36m0.0139\u001b[0m        0.0144  8.4075\n",
      "     45        0.0139        \u001b[32m0.0138\u001b[0m  8.3177\n",
      "     46        \u001b[36m0.0139\u001b[0m        0.0143  8.3741\n",
      "     47        \u001b[36m0.0138\u001b[0m        0.0143  8.2834\n",
      "     48        0.0139        0.0138  8.4120\n",
      "     49        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0136\u001b[0m  8.4103\n",
      "     50        \u001b[36m0.0137\u001b[0m        0.0138  8.4735\n",
      "     51        \u001b[36m0.0137\u001b[0m        0.0139  8.3253\n",
      "     52        0.0137        \u001b[32m0.0135\u001b[0m  8.2765\n",
      "     53        0.0137        0.0136  7.9072\n",
      "     54        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0135\u001b[0m  8.2521\n",
      "     55        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0135\u001b[0m  7.3575\n",
      "     56        \u001b[36m0.0136\u001b[0m        0.0137  8.1170\n",
      "     57        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0134\u001b[0m  8.2131\n",
      "     58        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0134\u001b[0m  7.3015\n",
      "     59        \u001b[36m0.0134\u001b[0m        0.0134  8.1235\n",
      "     60        \u001b[36m0.0134\u001b[0m        0.0136  8.3167\n",
      "     61        \u001b[36m0.0134\u001b[0m        0.0141  7.8935\n",
      "     62        \u001b[36m0.0133\u001b[0m        0.0134  7.8772\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.17602169778693966\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0163\u001b[0m  7.9607\n",
      "      2        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0161\u001b[0m  7.8658\n",
      "      3        \u001b[36m0.0162\u001b[0m        0.0162  7.2942\n",
      "      4        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0159\u001b[0m  7.9872\n",
      "      5        \u001b[36m0.0160\u001b[0m        0.0159  7.7245\n",
      "      6        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0158\u001b[0m  7.8534\n",
      "      7        \u001b[36m0.0159\u001b[0m        0.0159  7.4250\n",
      "      8        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0157\u001b[0m  7.7224\n",
      "      9        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0157\u001b[0m  7.5140\n",
      "     10        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0155\u001b[0m  7.6529\n",
      "     11        \u001b[36m0.0157\u001b[0m        0.0158  7.7662\n",
      "     12        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0155\u001b[0m  7.9279\n",
      "     13        \u001b[36m0.0156\u001b[0m        0.0156  8.0300\n",
      "     14        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0154\u001b[0m  7.9325\n",
      "     15        \u001b[36m0.0155\u001b[0m        0.0158  7.6821\n",
      "     16        0.0155        0.0155  7.5671\n",
      "     17        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0153\u001b[0m  8.3750\n",
      "     18        \u001b[36m0.0155\u001b[0m        0.0159  8.3604\n",
      "     19        0.0155        \u001b[32m0.0153\u001b[0m  8.4337\n",
      "     20        \u001b[36m0.0154\u001b[0m        0.0154  8.4107\n",
      "     21        0.0155        0.0156  8.4818\n",
      "     22        \u001b[36m0.0154\u001b[0m        0.0155  7.8581\n",
      "     23        \u001b[36m0.0154\u001b[0m        0.0156  7.7733\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.05887858510325572\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0170\u001b[0m  7.3913\n",
      "      2        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0168\u001b[0m  7.2871\n",
      "      3        0.0171        \u001b[32m0.0168\u001b[0m  7.0627\n",
      "      4        \u001b[36m0.0170\u001b[0m        0.0169  7.1033\n",
      "      5        \u001b[36m0.0170\u001b[0m        0.0175  8.0190\n",
      "      6        \u001b[36m0.0170\u001b[0m        0.0180  7.1314\n",
      "      7        0.0170        \u001b[32m0.0168\u001b[0m  8.5074\n",
      "      8        \u001b[36m0.0170\u001b[0m        0.0168  8.6575\n",
      "      9        0.0170        0.0174  8.4406\n",
      "     10        0.0170        \u001b[32m0.0168\u001b[0m  8.4139\n",
      "     11        0.0170        0.0171  8.5574\n",
      "     12        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0168\u001b[0m  8.4138\n",
      "     13        0.0170        \u001b[32m0.0168\u001b[0m  8.3672\n",
      "     14        \u001b[36m0.0169\u001b[0m        0.0168  8.5071\n",
      "     15        0.0169        0.0168  8.7214\n",
      "     16        0.0169        0.0169  8.4698\n",
      "     17        \u001b[36m0.0169\u001b[0m        0.0169  8.7038\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.0051934023415145525\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0169\u001b[0m  8.5358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0168\u001b[0m  8.6811\n",
      "      3        0.0168        \u001b[32m0.0168\u001b[0m  8.7041\n",
      "      4        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.5424\n",
      "      5        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0165\u001b[0m  8.2815\n",
      "      6        \u001b[36m0.0167\u001b[0m        0.0171  8.3627\n",
      "      7        \u001b[36m0.0167\u001b[0m        0.0166  8.2549\n",
      "      8        \u001b[36m0.0167\u001b[0m        0.0166  7.0577\n",
      "      9        \u001b[36m0.0167\u001b[0m        0.0166  7.3118\n",
      "     10        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0165\u001b[0m  6.7775\n",
      "     11        0.0167        0.0166  6.8974\n",
      "     12        0.0167        0.0166  6.7747\n",
      "     13        \u001b[36m0.0166\u001b[0m        0.0166  7.3690\n",
      "     14        0.0167        \u001b[32m0.0165\u001b[0m  8.2303\n",
      "     15        0.0167        \u001b[32m0.0165\u001b[0m  7.9015\n",
      "     16        0.0167        0.0165  8.5440\n",
      "     17        \u001b[36m0.0166\u001b[0m        0.0166  8.4819\n",
      "     18        0.0167        0.0166  8.5566\n",
      "     19        0.0166        \u001b[32m0.0165\u001b[0m  8.4016\n",
      "     20        0.0167        0.0165  7.7429\n",
      "     21        \u001b[36m0.0166\u001b[0m        0.0165  8.4700\n",
      "     22        0.0166        \u001b[32m0.0165\u001b[0m  8.4786\n",
      "     23        0.0166        0.0167  8.5492\n",
      "     24        \u001b[36m0.0166\u001b[0m        0.0173  7.7920\n",
      "     25        \u001b[36m0.0166\u001b[0m        0.0166  7.1019\n",
      "     26        0.0166        0.0165  7.3257\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.028120246756818124\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0182\u001b[0m        \u001b[32m0.0169\u001b[0m  7.5798\n",
      "      2        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0168\u001b[0m  7.4458\n",
      "      3        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0167\u001b[0m  8.0096\n",
      "      4        \u001b[36m0.0171\u001b[0m        0.0171  7.3649\n",
      "      5        \u001b[36m0.0170\u001b[0m        0.0167  7.6989\n",
      "      6        \u001b[36m0.0170\u001b[0m        0.0172  7.1965\n",
      "      7        \u001b[36m0.0170\u001b[0m        0.0171  7.7846\n",
      "      8        0.0170        \u001b[32m0.0166\u001b[0m  7.5892\n",
      "      9        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0166\u001b[0m  7.7600\n",
      "     10        0.0169        0.0167  7.5440\n",
      "     11        0.0169        0.0166  8.2125\n",
      "     12        0.0169        0.0171  8.2777\n",
      "     13        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0165\u001b[0m  8.1642\n",
      "     14        0.0169        0.0167  8.2092\n",
      "     15        0.0169        0.0168  8.2733\n",
      "     16        0.0169        0.0166  8.2638\n",
      "     17        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0165\u001b[0m  8.2183\n",
      "     18        0.0169        0.0166  8.2378\n",
      "     19        0.0169        0.0166  8.3018\n",
      "     20        0.0169        \u001b[32m0.0165\u001b[0m  8.2192\n",
      "     21        \u001b[36m0.0169\u001b[0m        0.0165  8.2460\n",
      "     22        0.0169        \u001b[32m0.0165\u001b[0m  8.1994\n",
      "     23        0.0169        0.0167  8.2770\n",
      "     24        0.0169        0.0166  8.3019\n",
      "     25        \u001b[36m0.0168\u001b[0m        0.0166  8.4151\n",
      "     26        0.0168        0.0165  7.8440\n",
      "     27        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0165\u001b[0m  7.8108\n",
      "     28        \u001b[36m0.0168\u001b[0m        0.0170  7.5561\n",
      "     29        0.0168        0.0165  8.4073\n",
      "     30        0.0168        \u001b[32m0.0165\u001b[0m  8.0241\n",
      "     31        0.0168        0.0166  8.0992\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 0.013556038509400792\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0174\u001b[0m        \u001b[32m0.0173\u001b[0m  8.3290\n",
      "      2        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0173\u001b[0m  8.0165\n",
      "      3        \u001b[36m0.0171\u001b[0m        0.0174  7.9498\n",
      "      4        0.0171        0.0175  8.0264\n",
      "      5        \u001b[36m0.0171\u001b[0m        0.0179  8.0370\n",
      "      6        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0173\u001b[0m  8.0096\n",
      "      7        0.0171        0.0174  8.0271\n",
      "      8        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0173\u001b[0m  8.0009\n",
      "      9        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0172\u001b[0m  8.0221\n",
      "     10        0.0170        0.0175  7.9740\n",
      "     11        0.0170        0.0173  8.2495\n",
      "     12        0.0170        0.0174  8.3093\n",
      "     13        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0172\u001b[0m  8.3451\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0031051780624868908\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0183\u001b[0m        \u001b[32m0.0169\u001b[0m  8.4302\n",
      "      2        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0169\u001b[0m  8.4476\n",
      "      3        \u001b[36m0.0170\u001b[0m        0.0170  8.3584\n",
      "      4        \u001b[36m0.0169\u001b[0m        0.0170  8.3790\n",
      "      5        \u001b[36m0.0168\u001b[0m        0.0170  8.2095\n",
      "      6        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0168\u001b[0m  8.1434\n",
      "      7        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0167\u001b[0m  7.5307\n",
      "      8        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0166\u001b[0m  7.9379\n",
      "      9        \u001b[36m0.0166\u001b[0m        0.0167  7.4507\n",
      "     10        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0165\u001b[0m  7.9208\n",
      "     11        \u001b[36m0.0166\u001b[0m        0.0165  7.7443\n",
      "     12        \u001b[36m0.0165\u001b[0m        0.0165  7.8441\n",
      "     13        0.0165        \u001b[32m0.0164\u001b[0m  8.3113\n",
      "     14        \u001b[36m0.0165\u001b[0m        0.0165  8.3658\n",
      "     15        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  8.2790\n",
      "     16        \u001b[36m0.0165\u001b[0m        0.0166  8.3068\n",
      "     17        \u001b[36m0.0165\u001b[0m        0.0165  8.3676\n",
      "     18        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3313\n",
      "     19        \u001b[36m0.0164\u001b[0m        0.0169  8.3150\n",
      "     20        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3730\n",
      "     21        \u001b[36m0.0164\u001b[0m        0.0165  8.2937\n",
      "     22        \u001b[36m0.0164\u001b[0m        0.0164  7.9977\n",
      "     23        \u001b[36m0.0164\u001b[0m        0.0165  7.5666\n",
      "     24        \u001b[36m0.0164\u001b[0m        0.0164  8.1900\n",
      "     25        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  7.5578\n",
      "     26        \u001b[36m0.0163\u001b[0m        0.0165  7.8613\n",
      "     27        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  6.8143\n",
      "     28        0.0163        \u001b[32m0.0163\u001b[0m  7.3717\n",
      "     29        0.0163        0.0164  6.3732\n",
      "     30        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  6.1572\n",
      "     31        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  7.1722\n",
      "     32        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0162\u001b[0m  6.3514\n",
      "     33        \u001b[36m0.0163\u001b[0m        0.0162  5.9787\n",
      "     34        \u001b[36m0.0162\u001b[0m        0.0165  5.8903\n",
      "     35        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  6.1238\n",
      "     36        \u001b[36m0.0162\u001b[0m        0.0163  7.4740\n",
      "     37        \u001b[36m0.0162\u001b[0m        0.0163  8.0079\n",
      "     38        \u001b[36m0.0162\u001b[0m        0.0163  8.0720\n",
      "     39        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  8.0756\n",
      "     40        \u001b[36m0.0162\u001b[0m        0.0164  8.1227\n",
      "     41        \u001b[36m0.0161\u001b[0m        0.0162  8.3640\n",
      "     42        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0162\u001b[0m  8.1715\n",
      "     43        \u001b[36m0.0161\u001b[0m        0.0164  7.4538\n",
      "     44        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0161\u001b[0m  8.2535\n",
      "     45        0.0161        \u001b[32m0.0161\u001b[0m  8.2839\n",
      "     46        \u001b[36m0.0161\u001b[0m        0.0161  8.3162\n",
      "     47        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0161\u001b[0m  8.3035\n",
      "     48        \u001b[36m0.0161\u001b[0m        0.0163  8.2313\n",
      "     49        \u001b[36m0.0161\u001b[0m        0.0167  8.1728\n",
      "     50        \u001b[36m0.0160\u001b[0m        0.0164  8.2528\n",
      "     51        \u001b[36m0.0160\u001b[0m        0.0162  8.2129\n",
      "     52        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0161\u001b[0m  8.2886\n",
      "     53        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0160\u001b[0m  8.2868\n",
      "     54        \u001b[36m0.0160\u001b[0m        0.0161  8.2461\n",
      "     55        \u001b[36m0.0160\u001b[0m        0.0160  8.2417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     56        \u001b[36m0.0160\u001b[0m        0.0160  8.2362\n",
      "     57        0.0160        \u001b[32m0.0160\u001b[0m  8.2946\n",
      "     58        \u001b[36m0.0159\u001b[0m        0.0162  8.2547\n",
      "     59        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0160\u001b[0m  8.2917\n",
      "     60        \u001b[36m0.0159\u001b[0m        0.0166  8.2005\n",
      "     61        \u001b[36m0.0159\u001b[0m        0.0164  8.4089\n",
      "     62        \u001b[36m0.0159\u001b[0m        0.0160  8.4508\n",
      "     63        \u001b[36m0.0158\u001b[0m        0.0161  8.4111\n",
      "     64        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0159\u001b[0m  8.3124\n",
      "     65        0.0158        \u001b[32m0.0158\u001b[0m  8.3197\n",
      "     66        \u001b[36m0.0158\u001b[0m        0.0159  8.2899\n",
      "     67        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0158\u001b[0m  8.1397\n",
      "     68        \u001b[36m0.0157\u001b[0m        0.0158  8.2682\n",
      "     69        \u001b[36m0.0157\u001b[0m        0.0160  8.3494\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.06971926499823922\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0181\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3211\n",
      "      2        \u001b[36m0.0168\u001b[0m        0.0170  8.4917\n",
      "      3        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0167\u001b[0m  8.4110\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0167  8.4357\n",
      "      5        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3665\n",
      "      6        \u001b[36m0.0167\u001b[0m        0.0170  8.3908\n",
      "      7        0.0167        \u001b[32m0.0166\u001b[0m  8.3036\n",
      "      8        \u001b[36m0.0167\u001b[0m        0.0170  8.4925\n",
      "      9        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0165\u001b[0m  8.3181\n",
      "     10        \u001b[36m0.0167\u001b[0m        0.0166  8.3760\n",
      "     11        \u001b[36m0.0166\u001b[0m        0.0166  8.2782\n",
      "     12        \u001b[36m0.0166\u001b[0m        0.0166  8.4223\n",
      "     13        \u001b[36m0.0166\u001b[0m        0.0167  8.2833\n",
      "     14        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4835\n",
      "     15        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3287\n",
      "     16        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3434\n",
      "     17        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  7.8305\n",
      "     18        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3678\n",
      "     19        \u001b[36m0.0165\u001b[0m        0.0165  8.0266\n",
      "     20        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  8.4068\n",
      "     21        \u001b[36m0.0165\u001b[0m        0.0164  8.2412\n",
      "     22        \u001b[36m0.0165\u001b[0m        0.0164  8.3438\n",
      "     23        \u001b[36m0.0164\u001b[0m        0.0168  8.3994\n",
      "     24        \u001b[36m0.0164\u001b[0m        0.0164  8.3819\n",
      "     25        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.2603\n",
      "     26        0.0164        0.0166  8.3738\n",
      "     27        \u001b[36m0.0164\u001b[0m        0.0165  8.6432\n",
      "     28        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3626\n",
      "     29        0.0164        0.0163  8.3649\n",
      "     30        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.0940\n",
      "     31        0.0164        0.0166  8.2698\n",
      "     32        0.0164        0.0163  8.1026\n",
      "     33        0.0164        0.0163  8.2814\n",
      "     34        \u001b[36m0.0164\u001b[0m        0.0164  8.2975\n",
      "     35        0.0164        \u001b[32m0.0163\u001b[0m  8.2831\n",
      "     36        0.0164        \u001b[32m0.0163\u001b[0m  8.4434\n",
      "     37        \u001b[36m0.0164\u001b[0m        0.0163  8.3598\n",
      "     38        \u001b[36m0.0164\u001b[0m        0.0163  8.3879\n",
      "     39        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3136\n",
      "     40        0.0164        0.0163  8.4115\n",
      "     41        0.0164        0.0163  8.3117\n",
      "     42        0.0164        0.0164  8.3970\n",
      "     43        0.0164        0.0163  8.3034\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.026061672734777996\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0180\u001b[0m        \u001b[32m0.0176\u001b[0m  8.3364\n",
      "      2        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0170\u001b[0m  8.4097\n",
      "      3        \u001b[36m0.0173\u001b[0m        0.0175  8.4474\n",
      "      4        \u001b[36m0.0172\u001b[0m        0.0185  8.4081\n",
      "      5        0.0172        \u001b[32m0.0169\u001b[0m  8.4434\n",
      "      6        0.0172        \u001b[32m0.0169\u001b[0m  8.3295\n",
      "      7        \u001b[36m0.0171\u001b[0m        0.0169  8.4441\n",
      "      8        \u001b[36m0.0171\u001b[0m        0.0169  8.3585\n",
      "      9        \u001b[36m0.0171\u001b[0m        0.0181  8.4477\n",
      "     10        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3156\n",
      "     11        \u001b[36m0.0170\u001b[0m        0.0170  8.4332\n",
      "     12        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0168\u001b[0m  8.2696\n",
      "     13        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0167\u001b[0m  8.4113\n",
      "     14        \u001b[36m0.0169\u001b[0m        0.0169  8.4174\n",
      "     15        \u001b[36m0.0169\u001b[0m        0.0175  8.2256\n",
      "     16        \u001b[36m0.0169\u001b[0m        0.0172  8.3080\n",
      "     17        0.0169        0.0169  8.2849\n",
      "     18        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3128\n",
      "     19        \u001b[36m0.0168\u001b[0m        0.0167  8.3067\n",
      "     20        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  8.2878\n",
      "     21        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3902\n",
      "     22        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3524\n",
      "     23        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.3505\n",
      "     24        0.0164        0.0165  8.1216\n",
      "     25        0.0164        \u001b[32m0.0162\u001b[0m  8.2258\n",
      "     26        \u001b[36m0.0164\u001b[0m        0.0164  8.2679\n",
      "     27        \u001b[36m0.0163\u001b[0m        0.0165  8.4097\n",
      "     28        0.0163        \u001b[32m0.0160\u001b[0m  7.8614\n",
      "     29        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0160\u001b[0m  7.9679\n",
      "     30        \u001b[36m0.0162\u001b[0m        0.0169  7.9434\n",
      "     31        \u001b[36m0.0162\u001b[0m        0.0161  8.3988\n",
      "     32        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0159\u001b[0m  8.2395\n",
      "     33        \u001b[36m0.0160\u001b[0m        0.0168  8.0257\n",
      "     34        0.0160        0.0165  8.0767\n",
      "     35        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0157\u001b[0m  8.0887\n",
      "     36        \u001b[36m0.0159\u001b[0m        0.0159  8.1209\n",
      "     37        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0156\u001b[0m  8.2199\n",
      "     38        \u001b[36m0.0157\u001b[0m        0.0159  8.3169\n",
      "     39        0.0157        \u001b[32m0.0155\u001b[0m  7.9508\n",
      "     40        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0153\u001b[0m  7.9495\n",
      "     41        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4940\n",
      "     42        \u001b[36m0.0154\u001b[0m        0.0152  7.9036\n",
      "     43        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0151\u001b[0m  7.6428\n",
      "     44        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0151\u001b[0m  7.8195\n",
      "     45        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0150\u001b[0m  7.5542\n",
      "     46        \u001b[36m0.0150\u001b[0m        0.0154  7.7372\n",
      "     47        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0146\u001b[0m  7.4427\n",
      "     48        \u001b[36m0.0149\u001b[0m        0.0146  7.7890\n",
      "     49        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0144\u001b[0m  7.7590\n",
      "     50        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0144\u001b[0m  7.7457\n",
      "     51        \u001b[36m0.0145\u001b[0m        0.0147  7.7006\n",
      "     52        \u001b[36m0.0145\u001b[0m        0.0148  8.1591\n",
      "     53        \u001b[36m0.0144\u001b[0m        0.0146  7.6870\n",
      "     54        \u001b[36m0.0143\u001b[0m        0.0150  7.6565\n",
      "     55        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0139\u001b[0m  7.6125\n",
      "     56        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0137\u001b[0m  7.4996\n",
      "     57        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0134\u001b[0m  7.4200\n",
      "     58        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0133\u001b[0m  7.9197\n",
      "     59        \u001b[36m0.0137\u001b[0m        0.0138  7.9453\n",
      "     60        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0131\u001b[0m  7.8993\n",
      "     61        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0129\u001b[0m  8.3178\n",
      "     62        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0128\u001b[0m  7.4245\n",
      "     63        \u001b[36m0.0133\u001b[0m        0.0152  8.0506\n",
      "     64        \u001b[36m0.0132\u001b[0m        0.0130  7.8459\n",
      "     65        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0128\u001b[0m  7.8622\n",
      "     66        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0124\u001b[0m  8.0138\n",
      "     67        \u001b[36m0.0129\u001b[0m        0.0126  7.8752\n",
      "     68        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0123\u001b[0m  7.8636\n",
      "     69        0.0127        0.0131  8.0744\n",
      "     70        \u001b[36m0.0126\u001b[0m        0.0126  7.8970\n",
      "     71        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0119\u001b[0m  7.3968\n",
      "     72        \u001b[36m0.0124\u001b[0m        0.0129  7.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     73        \u001b[36m0.0123\u001b[0m        0.0124  8.0083\n",
      "     74        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0115\u001b[0m  8.0593\n",
      "     75        \u001b[36m0.0120\u001b[0m        0.0125  7.4159\n",
      "     76        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0114\u001b[0m  8.2954\n",
      "     77        \u001b[36m0.0118\u001b[0m        0.0116  7.4122\n",
      "     78        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0113\u001b[0m  7.7798\n",
      "     79        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0110\u001b[0m  8.0295\n",
      "     80        \u001b[36m0.0115\u001b[0m        0.0116  8.1091\n",
      "     81        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0108\u001b[0m  7.4257\n",
      "     82        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0107\u001b[0m  7.7684\n",
      "     83        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0106\u001b[0m  8.0445\n",
      "     84        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0104\u001b[0m  8.0740\n",
      "     85        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0103\u001b[0m  7.9569\n",
      "     86        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0102\u001b[0m  8.0722\n",
      "     87        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0099\u001b[0m  8.3113\n",
      "     88        \u001b[36m0.0105\u001b[0m        0.0111  7.9646\n",
      "     89        0.0106        \u001b[32m0.0099\u001b[0m  7.9739\n",
      "     90        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0096\u001b[0m  7.6122\n",
      "     91        \u001b[36m0.0103\u001b[0m        0.0100  7.1691\n",
      "     92        \u001b[36m0.0102\u001b[0m        0.0102  7.4030\n",
      "     93        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0093\u001b[0m  7.6635\n",
      "     94        \u001b[36m0.0100\u001b[0m        0.0094  7.7375\n",
      "     95        \u001b[36m0.0099\u001b[0m        0.0102  7.5733\n",
      "     96        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0091\u001b[0m  7.6278\n",
      "     97        \u001b[36m0.0098\u001b[0m        0.0092  7.2664\n",
      "     98        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0090\u001b[0m  7.5933\n",
      "     99        \u001b[36m0.0095\u001b[0m        0.0091  7.4646\n",
      "    100        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0089\u001b[0m  6.9804\n",
      "    101        \u001b[36m0.0094\u001b[0m        0.0090  8.3505\n",
      "    102        \u001b[36m0.0094\u001b[0m        0.0090  8.3777\n",
      "    103        \u001b[36m0.0092\u001b[0m        0.0093  8.4289\n",
      "    104        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0086\u001b[0m  8.4151\n",
      "    105        0.0091        0.0086  8.4550\n",
      "    106        \u001b[36m0.0090\u001b[0m        0.0093  8.5269\n",
      "    107        0.0091        0.0088  8.6863\n",
      "    108        \u001b[36m0.0090\u001b[0m        0.0097  8.6194\n",
      "    109        \u001b[36m0.0088\u001b[0m        \u001b[32m0.0086\u001b[0m  8.6696\n",
      "    110        \u001b[36m0.0088\u001b[0m        0.0094  8.5736\n",
      "    111        \u001b[36m0.0087\u001b[0m        0.0086  8.4795\n",
      "    112        \u001b[36m0.0087\u001b[0m        0.0086  8.4430\n",
      "    113        \u001b[36m0.0086\u001b[0m        \u001b[32m0.0081\u001b[0m  8.3891\n",
      "    114        0.0086        0.0082  8.5339\n",
      "    115        \u001b[36m0.0086\u001b[0m        0.0085  8.2286\n",
      "    116        \u001b[36m0.0085\u001b[0m        0.0081  8.2795\n",
      "    117        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0080\u001b[0m  8.1627\n",
      "    118        0.0084        0.0086  8.1135\n",
      "    119        \u001b[36m0.0083\u001b[0m        \u001b[32m0.0079\u001b[0m  8.1439\n",
      "    120        0.0083        \u001b[32m0.0078\u001b[0m  8.2670\n",
      "    121        \u001b[36m0.0082\u001b[0m        0.0078  8.2110\n",
      "    122        \u001b[36m0.0081\u001b[0m        0.0084  8.3908\n",
      "    123        \u001b[36m0.0081\u001b[0m        0.0092  8.3468\n",
      "    124        \u001b[36m0.0080\u001b[0m        0.0090  8.4930\n",
      "    125        0.0081        \u001b[32m0.0076\u001b[0m  8.2751\n",
      "    126        \u001b[36m0.0080\u001b[0m        0.0077  6.5278\n",
      "    127        \u001b[36m0.0079\u001b[0m        0.0080  6.3310\n",
      "    128        \u001b[36m0.0078\u001b[0m        0.0088  6.4826\n",
      "    129        \u001b[36m0.0078\u001b[0m        0.0088  7.4955\n",
      "    130        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0075\u001b[0m  6.4139\n",
      "    131        \u001b[36m0.0077\u001b[0m        0.0081  7.4707\n",
      "    132        0.0077        0.0078  8.4062\n",
      "    133        \u001b[36m0.0076\u001b[0m        0.0082  8.3172\n",
      "    134        0.0077        \u001b[32m0.0072\u001b[0m  8.3809\n",
      "    135        \u001b[36m0.0076\u001b[0m        0.0074  8.4262\n",
      "    136        \u001b[36m0.0076\u001b[0m        0.0078  8.3118\n",
      "    137        0.0076        \u001b[32m0.0070\u001b[0m  8.4087\n",
      "    138        \u001b[36m0.0074\u001b[0m        \u001b[32m0.0069\u001b[0m  8.4368\n",
      "    139        \u001b[36m0.0074\u001b[0m        0.0072  8.4811\n",
      "    140        0.0074        0.0075  8.4071\n",
      "    141        0.0074        0.0070  8.3214\n",
      "    142        \u001b[36m0.0074\u001b[0m        \u001b[32m0.0069\u001b[0m  8.2924\n",
      "    143        \u001b[36m0.0072\u001b[0m        0.0071  8.2420\n",
      "    144        0.0073        0.0077  8.2595\n",
      "    145        \u001b[36m0.0072\u001b[0m        0.0073  8.4158\n",
      "    146        \u001b[36m0.0072\u001b[0m        0.0073  8.1068\n",
      "    147        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0066\u001b[0m  8.4297\n",
      "    148        0.0072        0.0075  8.3023\n",
      "    149        0.0071        0.0074  8.1297\n",
      "    150        \u001b[36m0.0070\u001b[0m        0.0072  8.1181\n",
      "    151        0.0070        \u001b[32m0.0065\u001b[0m  8.4269\n",
      "    152        0.0070        0.0076  8.4221\n",
      "    153        \u001b[36m0.0070\u001b[0m        0.0078  8.2343\n",
      "    154        \u001b[36m0.0069\u001b[0m        0.0073  8.3950\n",
      "    155        \u001b[36m0.0069\u001b[0m        \u001b[32m0.0063\u001b[0m  8.3386\n",
      "    156        0.0069        0.0073  8.3136\n",
      "    157        \u001b[36m0.0068\u001b[0m        0.0065  8.3202\n",
      "    158        0.0068        0.0066  8.4800\n",
      "    159        \u001b[36m0.0068\u001b[0m        0.0064  8.2701\n",
      "    160        0.0068        \u001b[32m0.0062\u001b[0m  8.3028\n",
      "    161        \u001b[36m0.0068\u001b[0m        0.0077  8.2419\n",
      "    162        \u001b[36m0.0067\u001b[0m        0.0069  8.3228\n",
      "    163        0.0067        0.0066  8.2724\n",
      "    164        \u001b[36m0.0066\u001b[0m        \u001b[32m0.0061\u001b[0m  8.2059\n",
      "    165        0.0067        0.0079  8.3260\n",
      "    166        0.0066        0.0064  8.3257\n",
      "    167        \u001b[36m0.0066\u001b[0m        0.0070  8.2356\n",
      "    168        0.0066        0.0076  8.0877\n",
      "    169        0.0066        \u001b[32m0.0061\u001b[0m  8.0888\n",
      "    170        \u001b[36m0.0065\u001b[0m        0.0064  8.2347\n",
      "    171        0.0065        0.0069  7.9772\n",
      "    172        \u001b[36m0.0064\u001b[0m        0.0067  7.9727\n",
      "    173        \u001b[36m0.0064\u001b[0m        0.0081  8.0843\n",
      "    174        0.0065        \u001b[32m0.0060\u001b[0m  8.1798\n",
      "    175        0.0064        \u001b[32m0.0059\u001b[0m  7.9698\n",
      "    176        0.0064        0.0059  8.1380\n",
      "    177        \u001b[36m0.0063\u001b[0m        0.0061  8.0293\n",
      "    178        0.0064        0.0061  8.1931\n",
      "    179        \u001b[36m0.0063\u001b[0m        0.0059  8.2436\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.6180945455777048\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0171\u001b[0m  8.0408\n",
      "      2        \u001b[36m0.0168\u001b[0m        0.0171  8.1279\n",
      "      3        \u001b[36m0.0168\u001b[0m        0.0173  8.1342\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0172  8.1157\n",
      "      5        0.0168        0.0173  8.3839\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.022916773284008318\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0178\u001b[0m        \u001b[32m0.0162\u001b[0m  8.4989\n",
      "      2        \u001b[36m0.0167\u001b[0m        0.0162  8.4472\n",
      "      3        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0162\u001b[0m  8.4184\n",
      "      4        \u001b[36m0.0166\u001b[0m        0.0163  8.4615\n",
      "      5        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0161\u001b[0m  8.5303\n",
      "      6        \u001b[36m0.0166\u001b[0m        0.0164  8.4504\n",
      "      7        \u001b[36m0.0166\u001b[0m        0.0162  8.1183\n",
      "      8        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0160\u001b[0m  8.4287\n",
      "      9        \u001b[36m0.0165\u001b[0m        0.0161  8.1853\n",
      "     10        0.0165        \u001b[32m0.0160\u001b[0m  8.4672\n",
      "     11        \u001b[36m0.0165\u001b[0m        0.0160  8.4942\n",
      "     12        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0160\u001b[0m  8.3518\n",
      "     13        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0160\u001b[0m  7.9227\n",
      "     14        \u001b[36m0.0165\u001b[0m        0.0161  8.2309\n",
      "     15        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0160\u001b[0m  7.9630\n",
      "     16        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0160\u001b[0m  7.9186\n",
      "     17        \u001b[36m0.0164\u001b[0m        0.0161  7.9390\n",
      "     18        0.0164        \u001b[32m0.0160\u001b[0m  7.8730\n",
      "     19        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0160\u001b[0m  8.1063\n",
      "     20        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0159\u001b[0m  7.7594\n",
      "     21        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0158\u001b[0m  7.9021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0154\u001b[0m  7.8527\n",
      "     23        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0150\u001b[0m  7.7621\n",
      "     24        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0150\u001b[0m  7.7211\n",
      "     25        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0150\u001b[0m  7.7022\n",
      "     26        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0148\u001b[0m  7.4789\n",
      "     27        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0146\u001b[0m  8.0893\n",
      "     28        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0146\u001b[0m  8.5470\n",
      "     29        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0145\u001b[0m  7.6980\n",
      "     30        \u001b[36m0.0150\u001b[0m        0.0146  7.5807\n",
      "     31        \u001b[36m0.0149\u001b[0m        0.0148  8.5740\n",
      "     32        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0144\u001b[0m  7.6609\n",
      "     33        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0143\u001b[0m  7.5981\n",
      "     34        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0142\u001b[0m  8.4254\n",
      "     35        \u001b[36m0.0147\u001b[0m        0.0143  7.5909\n",
      "     36        \u001b[36m0.0146\u001b[0m        \u001b[32m0.0141\u001b[0m  7.9356\n",
      "     37        \u001b[36m0.0146\u001b[0m        0.0141  8.1026\n",
      "     38        0.0146        0.0143  7.5603\n",
      "     39        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0139\u001b[0m  8.0505\n",
      "     40        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0139\u001b[0m  8.6064\n",
      "     41        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0138\u001b[0m  7.7140\n",
      "     42        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0138\u001b[0m  7.6274\n",
      "     43        \u001b[36m0.0143\u001b[0m        0.0142  7.5550\n",
      "     44        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0137\u001b[0m  8.4796\n",
      "     45        \u001b[36m0.0143\u001b[0m        0.0139  7.6075\n",
      "     46        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0136\u001b[0m  8.6108\n",
      "     47        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0136\u001b[0m  7.7324\n",
      "     48        \u001b[36m0.0141\u001b[0m        0.0137  7.5912\n",
      "     49        \u001b[36m0.0141\u001b[0m        0.0140  7.5907\n",
      "     50        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0134\u001b[0m  8.0402\n",
      "     51        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0134\u001b[0m  7.5584\n",
      "     52        \u001b[36m0.0140\u001b[0m        \u001b[32m0.0134\u001b[0m  8.5925\n",
      "     53        \u001b[36m0.0139\u001b[0m        0.0140  7.5997\n",
      "     54        0.0139        0.0140  8.3576\n",
      "     55        \u001b[36m0.0138\u001b[0m        0.0134  8.6933\n",
      "     56        0.0138        \u001b[32m0.0132\u001b[0m  8.4581\n",
      "     57        \u001b[36m0.0138\u001b[0m        0.0134  8.6388\n",
      "     58        \u001b[36m0.0137\u001b[0m        0.0133  8.4043\n",
      "     59        \u001b[36m0.0137\u001b[0m        0.0137  8.2814\n",
      "     60        \u001b[36m0.0137\u001b[0m        \u001b[32m0.0131\u001b[0m  8.4176\n",
      "     61        \u001b[36m0.0136\u001b[0m        0.0132  8.3098\n",
      "     62        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0131\u001b[0m  8.4050\n",
      "     63        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0131\u001b[0m  8.2996\n",
      "     64        \u001b[36m0.0135\u001b[0m        0.0131  8.2168\n",
      "     65        0.0135        0.0131  8.2779\n",
      "     66        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0130\u001b[0m  8.3729\n",
      "     67        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0130\u001b[0m  8.1772\n",
      "     68        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0129\u001b[0m  8.2960\n",
      "     69        \u001b[36m0.0134\u001b[0m        0.0130  8.2458\n",
      "     70        \u001b[36m0.0133\u001b[0m        0.0132  8.2917\n",
      "     71        \u001b[36m0.0133\u001b[0m        0.0133  8.3170\n",
      "     72        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0129\u001b[0m  7.9559\n",
      "     73        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0128\u001b[0m  7.9533\n",
      "     74        \u001b[36m0.0132\u001b[0m        0.0129  7.8137\n",
      "     75        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0128\u001b[0m  7.8230\n",
      "     76        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0127\u001b[0m  8.2530\n",
      "     77        \u001b[36m0.0131\u001b[0m        0.0128  8.4022\n",
      "     78        \u001b[36m0.0131\u001b[0m        0.0133  8.3608\n",
      "     79        0.0131        \u001b[32m0.0127\u001b[0m  8.4026\n",
      "     80        \u001b[36m0.0130\u001b[0m        0.0131  8.1211\n",
      "     81        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0127\u001b[0m  8.2227\n",
      "     82        \u001b[36m0.0130\u001b[0m        0.0128  8.2274\n",
      "     83        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0126\u001b[0m  8.3294\n",
      "     84        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0126\u001b[0m  8.4460\n",
      "     85        0.0129        \u001b[32m0.0125\u001b[0m  8.3646\n",
      "     86        \u001b[36m0.0129\u001b[0m        0.0127  8.4075\n",
      "     87        \u001b[36m0.0128\u001b[0m        0.0125  8.3789\n",
      "     88        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0125\u001b[0m  8.0772\n",
      "     89        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0125\u001b[0m  8.3644\n",
      "     90        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0124\u001b[0m  8.6768\n",
      "     91        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0123\u001b[0m  8.3941\n",
      "     92        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0123\u001b[0m  8.3412\n",
      "     93        \u001b[36m0.0126\u001b[0m        0.0128  8.6369\n",
      "     94        \u001b[36m0.0125\u001b[0m        0.0123  8.4340\n",
      "     95        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0122\u001b[0m  8.3727\n",
      "     96        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0120\u001b[0m  8.4697\n",
      "     97        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0119\u001b[0m  8.3659\n",
      "     98        \u001b[36m0.0123\u001b[0m        0.0126  8.2434\n",
      "     99        \u001b[36m0.0123\u001b[0m        0.0121  8.2329\n",
      "    100        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0119\u001b[0m  7.3103\n",
      "    101        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0119\u001b[0m  8.0665\n",
      "    102        \u001b[36m0.0121\u001b[0m        \u001b[32m0.0118\u001b[0m  7.2012\n",
      "    103        \u001b[36m0.0121\u001b[0m        0.0118  8.0615\n",
      "    104        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0116\u001b[0m  7.2402\n",
      "    105        \u001b[36m0.0120\u001b[0m        0.0125  8.0375\n",
      "    106        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0116\u001b[0m  7.2430\n",
      "    107        \u001b[36m0.0119\u001b[0m        0.0118  8.0103\n",
      "    108        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0114\u001b[0m  7.2531\n",
      "    109        \u001b[36m0.0118\u001b[0m        0.0115  7.8807\n",
      "    110        \u001b[36m0.0117\u001b[0m        0.0115  7.3245\n",
      "    111        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0112\u001b[0m  7.8682\n",
      "    112        \u001b[36m0.0117\u001b[0m        0.0113  7.2241\n",
      "    113        \u001b[36m0.0116\u001b[0m        \u001b[32m0.0112\u001b[0m  7.8024\n",
      "    114        \u001b[36m0.0116\u001b[0m        0.0113  7.2890\n",
      "    115        \u001b[36m0.0115\u001b[0m        0.0115  7.7482\n",
      "    116        \u001b[36m0.0115\u001b[0m        0.0117  7.5648\n",
      "    117        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0109\u001b[0m  7.6834\n",
      "    118        \u001b[36m0.0114\u001b[0m        0.0116  7.4849\n",
      "    119        \u001b[36m0.0114\u001b[0m        \u001b[32m0.0109\u001b[0m  7.7019\n",
      "    120        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0108\u001b[0m  7.2383\n",
      "    121        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0108\u001b[0m  7.6091\n",
      "    122        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0108\u001b[0m  7.2707\n",
      "    123        \u001b[36m0.0112\u001b[0m        0.0123  7.4066\n",
      "    124        \u001b[36m0.0112\u001b[0m        0.0109  7.2867\n",
      "    125        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0106\u001b[0m  7.3156\n",
      "    126        \u001b[36m0.0111\u001b[0m        0.0107  7.2767\n",
      "    127        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0106\u001b[0m  7.5191\n",
      "    128        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0106\u001b[0m  8.4414\n",
      "    129        0.0110        0.0111  8.3899\n",
      "    130        \u001b[36m0.0109\u001b[0m        0.0106  8.4185\n",
      "    131        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0103\u001b[0m  8.2995\n",
      "    132        \u001b[36m0.0108\u001b[0m        0.0104  7.3007\n",
      "    133        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0103\u001b[0m  7.2578\n",
      "    134        \u001b[36m0.0107\u001b[0m        0.0105  6.8441\n",
      "    135        \u001b[36m0.0107\u001b[0m        0.0105  6.8603\n",
      "    136        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0101\u001b[0m  6.8561\n",
      "    137        \u001b[36m0.0106\u001b[0m        0.0104  6.8596\n",
      "    138        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0100\u001b[0m  8.1222\n",
      "    139        \u001b[36m0.0105\u001b[0m        0.0105  8.7167\n",
      "    140        \u001b[36m0.0105\u001b[0m        0.0105  8.7000\n",
      "    141        \u001b[36m0.0104\u001b[0m        0.0103  8.6360\n",
      "    142        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0100\u001b[0m  8.6913\n",
      "    143        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0100\u001b[0m  8.5358\n",
      "    144        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0098\u001b[0m  8.7232\n",
      "    145        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0096\u001b[0m  8.4127\n",
      "    146        \u001b[36m0.0101\u001b[0m        0.0098  8.3308\n",
      "    147        \u001b[36m0.0101\u001b[0m        0.0097  8.1296\n",
      "    148        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0095\u001b[0m  8.2422\n",
      "    149        \u001b[36m0.0099\u001b[0m        0.0099  8.3705\n",
      "    150        \u001b[36m0.0099\u001b[0m        0.0095  8.3977\n",
      "    151        \u001b[36m0.0099\u001b[0m        0.0098  8.1742\n",
      "    152        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0093\u001b[0m  8.2631\n",
      "    153        \u001b[36m0.0096\u001b[0m        0.0094  8.4337\n",
      "    154        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0091\u001b[0m  8.1532\n",
      "    155        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0090\u001b[0m  8.2698\n",
      "    156        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0089\u001b[0m  8.3834\n",
      "    157        \u001b[36m0.0093\u001b[0m        \u001b[32m0.0089\u001b[0m  8.1506\n",
      "    158        \u001b[36m0.0092\u001b[0m        0.0091  8.1686\n",
      "    159        \u001b[36m0.0091\u001b[0m        \u001b[32m0.0085\u001b[0m  8.2902\n",
      "    160        \u001b[36m0.0090\u001b[0m        \u001b[32m0.0084\u001b[0m  8.0389\n",
      "    161        \u001b[36m0.0089\u001b[0m        0.0088  8.1309\n",
      "    162        \u001b[36m0.0088\u001b[0m        0.0085  8.3200\n",
      "    163        \u001b[36m0.0087\u001b[0m        \u001b[32m0.0083\u001b[0m  8.3106\n",
      "    164        \u001b[36m0.0086\u001b[0m        0.0093  8.1691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    165        \u001b[36m0.0086\u001b[0m        \u001b[32m0.0081\u001b[0m  8.1382\n",
      "    166        \u001b[36m0.0085\u001b[0m        \u001b[32m0.0080\u001b[0m  8.4161\n",
      "    167        \u001b[36m0.0084\u001b[0m        \u001b[32m0.0079\u001b[0m  8.5115\n",
      "    168        \u001b[36m0.0084\u001b[0m        0.0080  8.5643\n",
      "    169        \u001b[36m0.0082\u001b[0m        \u001b[32m0.0077\u001b[0m  8.4022\n",
      "    170        \u001b[36m0.0082\u001b[0m        0.0078  8.4234\n",
      "    171        \u001b[36m0.0081\u001b[0m        0.0083  8.4933\n",
      "    172        0.0081        0.0079  8.3017\n",
      "    173        \u001b[36m0.0079\u001b[0m        \u001b[32m0.0076\u001b[0m  8.3193\n",
      "    174        \u001b[36m0.0079\u001b[0m        0.0080  7.4483\n",
      "    175        \u001b[36m0.0078\u001b[0m        0.0079  7.7402\n",
      "    176        \u001b[36m0.0078\u001b[0m        0.0077  7.7951\n",
      "    177        \u001b[36m0.0077\u001b[0m        \u001b[32m0.0073\u001b[0m  7.5214\n",
      "    178        \u001b[36m0.0076\u001b[0m        \u001b[32m0.0073\u001b[0m  6.8655\n",
      "    179        \u001b[36m0.0075\u001b[0m        0.0073  7.7650\n",
      "    180        0.0076        0.0074  7.4886\n",
      "    181        \u001b[36m0.0075\u001b[0m        \u001b[32m0.0070\u001b[0m  7.7356\n",
      "    182        \u001b[36m0.0074\u001b[0m        0.0071  7.5358\n",
      "    183        \u001b[36m0.0074\u001b[0m        0.0071  8.1005\n",
      "    184        \u001b[36m0.0073\u001b[0m        \u001b[32m0.0070\u001b[0m  7.8185\n",
      "    185        0.0073        0.0071  6.8765\n",
      "    186        0.0073        \u001b[32m0.0069\u001b[0m  7.9232\n",
      "    187        \u001b[36m0.0071\u001b[0m        0.0072  7.6459\n",
      "    188        \u001b[36m0.0071\u001b[0m        \u001b[32m0.0067\u001b[0m  7.7005\n",
      "    189        0.0071        0.0068  7.7202\n",
      "    190        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0066\u001b[0m  7.8288\n",
      "    191        \u001b[36m0.0070\u001b[0m        0.0072  7.4600\n",
      "    192        \u001b[36m0.0070\u001b[0m        \u001b[32m0.0066\u001b[0m  7.9232\n",
      "    193        \u001b[36m0.0069\u001b[0m        0.0069  8.0545\n",
      "    194        \u001b[36m0.0069\u001b[0m        0.0067  8.0212\n",
      "    195        \u001b[36m0.0067\u001b[0m        \u001b[32m0.0064\u001b[0m  7.7837\n",
      "    196        \u001b[36m0.0067\u001b[0m        0.0067  7.8637\n",
      "    197        \u001b[36m0.0067\u001b[0m        0.0068  7.8537\n",
      "    198        \u001b[36m0.0067\u001b[0m        0.0065  7.9524\n",
      "    199        \u001b[36m0.0066\u001b[0m        \u001b[32m0.0063\u001b[0m  8.0551\n",
      "    200        \u001b[36m0.0066\u001b[0m        0.0067  8.0663\n",
      "    201        \u001b[36m0.0065\u001b[0m        \u001b[32m0.0063\u001b[0m  8.1754\n",
      "    202        \u001b[36m0.0065\u001b[0m        0.0082  7.8925\n",
      "    203        0.0065        0.0068  7.9644\n",
      "    204        0.0065        \u001b[32m0.0062\u001b[0m  7.9524\n",
      "    205        \u001b[36m0.0063\u001b[0m        \u001b[32m0.0060\u001b[0m  7.8229\n",
      "    206        \u001b[36m0.0063\u001b[0m        \u001b[32m0.0059\u001b[0m  7.9088\n",
      "    207        \u001b[36m0.0062\u001b[0m        0.0060  7.8928\n",
      "    208        \u001b[36m0.0062\u001b[0m        0.0060  8.2348\n",
      "    209        \u001b[36m0.0061\u001b[0m        0.0062  7.6939\n",
      "    210        \u001b[36m0.0061\u001b[0m        \u001b[32m0.0058\u001b[0m  8.1918\n",
      "    211        0.0061        0.0064  8.1916\n",
      "    212        \u001b[36m0.0060\u001b[0m        0.0059  7.8097\n",
      "    213        \u001b[36m0.0060\u001b[0m        \u001b[32m0.0056\u001b[0m  8.1662\n",
      "    214        \u001b[36m0.0060\u001b[0m        0.0068  8.0046\n",
      "    215        \u001b[36m0.0059\u001b[0m        0.0075  8.0287\n",
      "    216        \u001b[36m0.0059\u001b[0m        0.0061  7.7809\n",
      "    217        \u001b[36m0.0058\u001b[0m        \u001b[32m0.0055\u001b[0m  7.8039\n",
      "    218        \u001b[36m0.0058\u001b[0m        \u001b[32m0.0055\u001b[0m  7.8466\n",
      "    219        \u001b[36m0.0058\u001b[0m        0.0065  8.3313\n",
      "    220        \u001b[36m0.0057\u001b[0m        0.0059  8.0670\n",
      "    221        0.0057        0.0059  7.9782\n",
      "    222        0.0057        0.0070  8.2611\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.6578616953285867\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0166\u001b[0m  8.1219\n",
      "      2        \u001b[36m0.0169\u001b[0m        0.0166  8.1075\n",
      "      3        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0164\u001b[0m  8.4079\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0166  8.5229\n",
      "      5        0.0168        0.0165  7.0723\n",
      "      6        \u001b[36m0.0167\u001b[0m        0.0167  7.8017\n",
      "      7        0.0168        0.0164  7.5047\n",
      "      8        0.0167        \u001b[32m0.0164\u001b[0m  7.9530\n",
      "      9        \u001b[36m0.0167\u001b[0m        0.0166  7.8379\n",
      "     10        \u001b[36m0.0167\u001b[0m        0.0168  8.3412\n",
      "     11        0.0167        0.0164  7.4139\n",
      "     12        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  8.1183\n",
      "     13        \u001b[36m0.0167\u001b[0m        0.0166  8.2388\n",
      "     14        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3329\n",
      "     15        0.0167        0.0165  8.4124\n",
      "     16        0.0167        \u001b[32m0.0164\u001b[0m  8.3355\n",
      "     17        0.0167        0.0164  8.2010\n",
      "     18        0.0167        0.0167  8.1989\n",
      "     19        0.0167        0.0167  8.2068\n",
      "     20        \u001b[36m0.0167\u001b[0m        0.0164  8.3677\n",
      "     21        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0164\u001b[0m  8.4155\n",
      "     22        \u001b[36m0.0167\u001b[0m        0.0166  8.2679\n",
      "     23        0.0167        0.0165  8.3627\n",
      "     24        \u001b[36m0.0167\u001b[0m        0.0165  8.2446\n",
      "     25        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0164\u001b[0m  8.3798\n",
      "     26        0.0166        0.0166  8.6164\n",
      "     27        0.0167        0.0164  8.4048\n",
      "     28        0.0166        \u001b[32m0.0164\u001b[0m  8.5297\n",
      "     29        0.0166        0.0164  8.4427\n",
      "     30        \u001b[36m0.0166\u001b[0m        0.0170  8.6085\n",
      "     31        0.0166        0.0164  8.3118\n",
      "     32        0.0167        \u001b[32m0.0164\u001b[0m  8.2851\n",
      "     33        0.0166        0.0164  8.1989\n",
      "     34        \u001b[36m0.0166\u001b[0m        0.0165  8.1267\n",
      "     35        0.0166        0.0164  8.4300\n",
      "     36        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0164\u001b[0m  8.4367\n",
      "     37        0.0166        \u001b[32m0.0163\u001b[0m  8.6283\n",
      "     38        \u001b[36m0.0166\u001b[0m        0.0164  8.4860\n",
      "     39        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0163\u001b[0m  8.6034\n",
      "     40        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0163\u001b[0m  8.4133\n",
      "     41        \u001b[36m0.0166\u001b[0m        0.0166  8.5805\n",
      "     42        \u001b[36m0.0166\u001b[0m        0.0164  8.4319\n",
      "     43        0.0166        0.0164  8.5447\n",
      "     44        0.0166        0.0163  8.6197\n",
      "     45        0.0166        \u001b[32m0.0163\u001b[0m  8.5289\n",
      "     46        0.0166        0.0163  8.3692\n",
      "     47        0.0166        0.0164  7.9658\n",
      "     48        0.0166        0.0164  8.0172\n",
      "     49        0.0166        \u001b[32m0.0163\u001b[0m  7.6675\n",
      "     50        0.0166        0.0164  8.3301\n",
      "     51        0.0166        0.0163  8.6834\n",
      "     52        0.0166        0.0170  8.4213\n",
      "     53        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0163\u001b[0m  8.2146\n",
      "     54        0.0166        0.0164  8.1650\n",
      "     55        \u001b[36m0.0166\u001b[0m        0.0164  8.2726\n",
      "     56        \u001b[36m0.0165\u001b[0m        0.0163  7.9770\n",
      "     57        0.0166        \u001b[32m0.0163\u001b[0m  8.2970\n",
      "     58        0.0165        \u001b[32m0.0163\u001b[0m  7.9710\n",
      "     59        \u001b[36m0.0165\u001b[0m        0.0164  8.1669\n",
      "     60        \u001b[36m0.0165\u001b[0m        0.0164  7.8702\n",
      "     61        0.0165        0.0163  7.8338\n",
      "     62        0.0165        0.0163  6.6263\n",
      "     63        0.0165        \u001b[32m0.0163\u001b[0m  7.6441\n",
      "     64        0.0165        0.0163  8.4320\n",
      "     65        \u001b[36m0.0165\u001b[0m        0.0163  8.4746\n",
      "     66        \u001b[36m0.0165\u001b[0m        0.0164  8.6462\n",
      "     67        \u001b[36m0.0165\u001b[0m        0.0163  8.4962\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0003324282021206493\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0164\u001b[0m  8.2739\n",
      "      2        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  8.5568\n",
      "      3        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0163\u001b[0m  8.2807\n",
      "      4        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0162\u001b[0m  7.2302\n",
      "      5        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0161\u001b[0m  7.7684\n",
      "      6        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0161\u001b[0m  6.8585\n",
      "      7        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0161\u001b[0m  7.4597\n",
      "      8        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0161\u001b[0m  7.2432\n",
      "      9        \u001b[36m0.0159\u001b[0m        0.0161  7.4475\n",
      "     10        \u001b[36m0.0159\u001b[0m        0.0161  6.8656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        \u001b[36m0.0159\u001b[0m        0.0162  6.9248\n",
      "     12        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0160\u001b[0m  7.2939\n",
      "     13        \u001b[36m0.0158\u001b[0m        0.0163  6.8535\n",
      "     14        \u001b[36m0.0158\u001b[0m        0.0160  7.9848\n",
      "     15        \u001b[36m0.0157\u001b[0m        0.0163  6.8698\n",
      "     16        0.0157        \u001b[32m0.0159\u001b[0m  7.8496\n",
      "     17        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0158\u001b[0m  7.0322\n",
      "     18        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0158\u001b[0m  7.4870\n",
      "     19        \u001b[36m0.0156\u001b[0m        0.0161  7.5196\n",
      "     20        \u001b[36m0.0156\u001b[0m        0.0158  8.2240\n",
      "     21        \u001b[36m0.0156\u001b[0m        0.0158  8.3523\n",
      "     22        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0156\u001b[0m  7.9786\n",
      "     23        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0156\u001b[0m  8.1003\n",
      "     24        \u001b[36m0.0155\u001b[0m        0.0157  8.0153\n",
      "     25        \u001b[36m0.0154\u001b[0m        0.0156  7.9822\n",
      "     26        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0155\u001b[0m  8.3300\n",
      "     27        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0153\u001b[0m  8.2897\n",
      "     28        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0153\u001b[0m  7.8870\n",
      "     29        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0153\u001b[0m  8.1298\n",
      "     30        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0152\u001b[0m  7.9588\n",
      "     31        \u001b[36m0.0152\u001b[0m        0.0153  8.1270\n",
      "     32        \u001b[36m0.0151\u001b[0m        0.0155  7.9833\n",
      "     33        0.0152        \u001b[32m0.0151\u001b[0m  8.3907\n",
      "     34        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0150\u001b[0m  8.4073\n",
      "     35        0.0151        \u001b[32m0.0150\u001b[0m  8.5238\n",
      "     36        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0150\u001b[0m  8.3415\n",
      "     37        \u001b[36m0.0150\u001b[0m        0.0151  8.5705\n",
      "     38        0.0151        \u001b[32m0.0149\u001b[0m  8.3644\n",
      "     39        \u001b[36m0.0150\u001b[0m        0.0150  8.5226\n",
      "     40        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0147\u001b[0m  8.7045\n",
      "     41        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0147\u001b[0m  8.3798\n",
      "     42        \u001b[36m0.0146\u001b[0m        \u001b[32m0.0145\u001b[0m  7.2733\n",
      "     43        \u001b[36m0.0145\u001b[0m        0.0146  6.9041\n",
      "     44        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0144\u001b[0m  8.1391\n",
      "     45        \u001b[36m0.0144\u001b[0m        0.0147  7.3441\n",
      "     46        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0143\u001b[0m  7.3892\n",
      "     47        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0141\u001b[0m  7.0275\n",
      "     48        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0141\u001b[0m  7.4560\n",
      "     49        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0140\u001b[0m  6.9576\n",
      "     50        \u001b[36m0.0139\u001b[0m        0.0145  6.2733\n",
      "     51        \u001b[36m0.0139\u001b[0m        0.0155  6.1306\n",
      "     52        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0136\u001b[0m  5.7909\n",
      "     53        \u001b[36m0.0137\u001b[0m        \u001b[32m0.0134\u001b[0m  5.7645\n",
      "     54        \u001b[36m0.0136\u001b[0m        0.0135  6.4972\n",
      "     55        \u001b[36m0.0135\u001b[0m        0.0143  6.1801\n",
      "     56        \u001b[36m0.0134\u001b[0m        0.0146  6.2314\n",
      "     57        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0131\u001b[0m  5.6256\n",
      "     58        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0131\u001b[0m  7.0534\n",
      "     59        \u001b[36m0.0133\u001b[0m        0.0135  6.5597\n",
      "     60        \u001b[36m0.0132\u001b[0m        0.0132  6.6674\n",
      "     61        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0129\u001b[0m  6.0620\n",
      "     62        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0129\u001b[0m  6.4169\n",
      "     63        \u001b[36m0.0130\u001b[0m        0.0129  6.1436\n",
      "     64        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0127\u001b[0m  8.0248\n",
      "     65        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0127\u001b[0m  6.0066\n",
      "     66        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0127\u001b[0m  6.2676\n",
      "     67        \u001b[36m0.0127\u001b[0m        0.0127  6.2680\n",
      "     68        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0125\u001b[0m  6.8640\n",
      "     69        \u001b[36m0.0126\u001b[0m        0.0129  6.6623\n",
      "     70        \u001b[36m0.0125\u001b[0m        0.0128  5.6478\n",
      "     71        0.0125        \u001b[32m0.0123\u001b[0m  6.3754\n",
      "     72        \u001b[36m0.0125\u001b[0m        0.0135  7.3038\n",
      "     73        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0123\u001b[0m  8.4104\n",
      "     74        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0122\u001b[0m  8.2138\n",
      "     75        0.0124        \u001b[32m0.0122\u001b[0m  8.3105\n",
      "     76        \u001b[36m0.0123\u001b[0m        0.0122  8.1851\n",
      "     77        \u001b[36m0.0123\u001b[0m        0.0126  8.1911\n",
      "     78        \u001b[36m0.0122\u001b[0m        0.0124  8.3858\n",
      "     79        \u001b[36m0.0122\u001b[0m        0.0124  8.4711\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.2714074237247789\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0177\u001b[0m        \u001b[32m0.0163\u001b[0m  7.8946\n",
      "      2        \u001b[36m0.0164\u001b[0m        0.0164  7.6146\n",
      "      3        0.0164        0.0163  7.2841\n",
      "      4        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  8.2038\n",
      "      5        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  7.3399\n",
      "      6        0.0164        0.0164  8.5188\n",
      "      7        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0163\u001b[0m  7.5227\n",
      "      8        0.0164        0.0163  8.1630\n",
      "      9        \u001b[36m0.0163\u001b[0m        0.0163  7.2416\n",
      "     10        0.0163        0.0163  8.0334\n",
      "     11        0.0163        \u001b[32m0.0163\u001b[0m  7.9545\n",
      "     12        0.0163        0.0164  8.2527\n",
      "     13        \u001b[36m0.0163\u001b[0m        0.0165  7.6563\n",
      "     14        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  8.0945\n",
      "     15        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0163\u001b[0m  7.9802\n",
      "     16        0.0163        0.0166  8.0601\n",
      "     17        \u001b[36m0.0163\u001b[0m        0.0163  7.2432\n",
      "     18        0.0163        0.0165  8.0634\n",
      "     19        0.0163        0.0165  7.5448\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.020839982041130467\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0181\u001b[0m        \u001b[32m0.0168\u001b[0m  7.9810\n",
      "      2        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0164\u001b[0m  8.0684\n",
      "      3        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0164\u001b[0m  7.9444\n",
      "      4        \u001b[36m0.0165\u001b[0m        \u001b[32m0.0162\u001b[0m  8.0841\n",
      "      5        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0162\u001b[0m  7.6269\n",
      "      6        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0160\u001b[0m  8.0023\n",
      "      7        \u001b[36m0.0162\u001b[0m        \u001b[32m0.0160\u001b[0m  8.2444\n",
      "      8        \u001b[36m0.0162\u001b[0m        0.0163  8.0795\n",
      "      9        \u001b[36m0.0161\u001b[0m        0.0160  7.2259\n",
      "     10        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0159\u001b[0m  7.8427\n",
      "     11        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0159\u001b[0m  6.8528\n",
      "     12        \u001b[36m0.0160\u001b[0m        0.0159  7.4704\n",
      "     13        \u001b[36m0.0160\u001b[0m        0.0162  7.6410\n",
      "     14        \u001b[36m0.0160\u001b[0m        0.0160  7.4610\n",
      "     15        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0158\u001b[0m  6.8345\n",
      "     16        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0158\u001b[0m  7.2923\n",
      "     17        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0158\u001b[0m  8.5055\n",
      "     18        \u001b[36m0.0159\u001b[0m        0.0162  8.3571\n",
      "     19        \u001b[36m0.0159\u001b[0m        0.0158  8.4145\n",
      "     20        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0157\u001b[0m  8.3512\n",
      "     21        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0157\u001b[0m  8.4899\n",
      "     22        0.0158        \u001b[32m0.0157\u001b[0m  8.2946\n",
      "     23        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0157\u001b[0m  8.4838\n",
      "     24        0.0158        \u001b[32m0.0156\u001b[0m  8.2675\n",
      "     25        \u001b[36m0.0157\u001b[0m        0.0160  8.3660\n",
      "     26        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0156\u001b[0m  8.2665\n",
      "     27        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0155\u001b[0m  8.3856\n",
      "     28        \u001b[36m0.0157\u001b[0m        0.0156  8.2730\n",
      "     29        \u001b[36m0.0156\u001b[0m        0.0156  8.2769\n",
      "     30        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0155\u001b[0m  8.2344\n",
      "     31        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0155\u001b[0m  8.3984\n",
      "     32        \u001b[36m0.0156\u001b[0m        0.0158  8.2825\n",
      "     33        \u001b[36m0.0155\u001b[0m        0.0155  8.4019\n",
      "     34        \u001b[36m0.0155\u001b[0m        0.0156  8.4057\n",
      "     35        \u001b[36m0.0155\u001b[0m        0.0156  8.3708\n",
      "     36        0.0155        \u001b[32m0.0154\u001b[0m  8.3027\n",
      "     37        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0153\u001b[0m  8.3584\n",
      "     38        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0153\u001b[0m  8.4333\n",
      "     39        \u001b[36m0.0154\u001b[0m        0.0165  8.3740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     40        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0150\u001b[0m  8.4185\n",
      "     41        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0150\u001b[0m  8.3005\n",
      "     42        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0148\u001b[0m  8.3983\n",
      "     43        \u001b[36m0.0151\u001b[0m        0.0154  8.2861\n",
      "     44        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0147\u001b[0m  7.7923\n",
      "     45        \u001b[36m0.0149\u001b[0m        0.0153  8.3769\n",
      "     46        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0145\u001b[0m  8.2977\n",
      "     47        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0142\u001b[0m  8.1675\n",
      "     48        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0141\u001b[0m  8.4013\n",
      "     49        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0140\u001b[0m  8.0782\n",
      "     50        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0140\u001b[0m  8.2048\n",
      "     51        \u001b[36m0.0145\u001b[0m        0.0140  7.8850\n",
      "     52        \u001b[36m0.0144\u001b[0m        0.0152  7.5267\n",
      "     53        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0139\u001b[0m  8.0430\n",
      "     54        \u001b[36m0.0143\u001b[0m        0.0141  7.7048\n",
      "     55        0.0143        0.0140  8.0658\n",
      "     56        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0137\u001b[0m  7.7606\n",
      "     57        \u001b[36m0.0142\u001b[0m        0.0144  8.0347\n",
      "     58        \u001b[36m0.0141\u001b[0m        0.0140  8.1535\n",
      "     59        \u001b[36m0.0141\u001b[0m        0.0143  7.1062\n",
      "     60        \u001b[36m0.0141\u001b[0m        0.0138  7.8663\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 0.1395982078073319\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0182\u001b[0m        \u001b[32m0.0166\u001b[0m  7.9367\n",
      "      2        \u001b[36m0.0168\u001b[0m        0.0168  7.9450\n",
      "      3        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0165\u001b[0m  8.1674\n",
      "      4        0.0168        0.0165  8.1780\n",
      "      5        0.0168        0.0167  8.4014\n",
      "      6        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0165\u001b[0m  8.4316\n",
      "      7        0.0168        0.0165  8.1688\n",
      "      8        0.0168        \u001b[32m0.0165\u001b[0m  8.4569\n",
      "      9        \u001b[36m0.0168\u001b[0m        0.0166  8.3794\n",
      "     10        \u001b[36m0.0168\u001b[0m        0.0165  8.4270\n",
      "     11        \u001b[36m0.0168\u001b[0m        0.0166  8.2685\n",
      "     12        0.0168        \u001b[32m0.0165\u001b[0m  8.4090\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0006012331086069356\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0177\u001b[0m        \u001b[32m0.0161\u001b[0m  8.2866\n",
      "      2        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0160\u001b[0m  8.0536\n",
      "      3        \u001b[36m0.0164\u001b[0m        0.0160  8.2554\n",
      "      4        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0156\u001b[0m  8.0049\n",
      "      5        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0155\u001b[0m  8.3742\n",
      "      6        \u001b[36m0.0158\u001b[0m        0.0157  8.0109\n",
      "      7        \u001b[36m0.0157\u001b[0m        \u001b[32m0.0151\u001b[0m  8.3465\n",
      "      8        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0150\u001b[0m  8.1568\n",
      "      9        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0150\u001b[0m  7.1789\n",
      "     10        \u001b[36m0.0155\u001b[0m        0.0150  7.2686\n",
      "     11        \u001b[36m0.0155\u001b[0m        0.0150  7.0229\n",
      "     12        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0150\u001b[0m  7.1387\n",
      "     13        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0149\u001b[0m  7.0558\n",
      "     14        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0149\u001b[0m  7.5447\n",
      "     15        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0149\u001b[0m  8.6733\n",
      "     16        \u001b[36m0.0154\u001b[0m        0.0149  8.3868\n",
      "     17        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0149\u001b[0m  6.9140\n",
      "     18        0.0154        0.0150  6.8397\n",
      "     19        \u001b[36m0.0154\u001b[0m        0.0150  6.8316\n",
      "     20        \u001b[36m0.0153\u001b[0m        0.0150  8.2405\n",
      "     21        0.0154        0.0150  6.8456\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.08090386452616183\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0159\u001b[0m  6.8291\n",
      "      2        \u001b[36m0.0160\u001b[0m        \u001b[32m0.0158\u001b[0m  6.8551\n",
      "      3        \u001b[36m0.0159\u001b[0m        0.0161  6.8475\n",
      "      4        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0157\u001b[0m  7.3997\n",
      "      5        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0157\u001b[0m  7.5429\n",
      "      6        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0157\u001b[0m  7.1045\n",
      "      7        \u001b[36m0.0158\u001b[0m        0.0157  7.6587\n",
      "      8        \u001b[36m0.0158\u001b[0m        0.0158  6.8342\n",
      "      9        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0156\u001b[0m  7.2523\n",
      "     10        \u001b[36m0.0157\u001b[0m        0.0158  6.8558\n",
      "     11        \u001b[36m0.0157\u001b[0m        0.0159  8.2005\n",
      "     12        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0155\u001b[0m  6.8412\n",
      "     13        \u001b[36m0.0156\u001b[0m        0.0160  7.9265\n",
      "     14        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0155\u001b[0m  6.8517\n",
      "     15        \u001b[36m0.0155\u001b[0m        0.0156  7.9915\n",
      "     16        \u001b[36m0.0155\u001b[0m        0.0155  8.6976\n",
      "     17        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0153\u001b[0m  8.6941\n",
      "     18        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0153\u001b[0m  8.5165\n",
      "     19        0.0154        \u001b[32m0.0152\u001b[0m  8.6941\n",
      "     20        \u001b[36m0.0154\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4226\n",
      "     21        0.0154        0.0153  8.6762\n",
      "     22        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0152\u001b[0m  8.4671\n",
      "     23        \u001b[36m0.0153\u001b[0m        0.0153  8.4459\n",
      "     24        \u001b[36m0.0153\u001b[0m        0.0153  8.6529\n",
      "     25        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0151\u001b[0m  8.6407\n",
      "     26        \u001b[36m0.0152\u001b[0m        \u001b[32m0.0149\u001b[0m  8.6777\n",
      "     27        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0149\u001b[0m  8.5648\n",
      "     28        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0146\u001b[0m  8.6740\n",
      "     29        \u001b[36m0.0149\u001b[0m        \u001b[32m0.0145\u001b[0m  8.4355\n",
      "     30        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0144\u001b[0m  8.6850\n",
      "     31        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0144\u001b[0m  8.4522\n",
      "     32        \u001b[36m0.0146\u001b[0m        \u001b[32m0.0142\u001b[0m  8.7045\n",
      "     33        \u001b[36m0.0145\u001b[0m        0.0142  8.4017\n",
      "     34        \u001b[36m0.0144\u001b[0m        \u001b[32m0.0140\u001b[0m  8.6708\n",
      "     35        \u001b[36m0.0142\u001b[0m        0.0141  8.5131\n",
      "     36        \u001b[36m0.0141\u001b[0m        0.0159  8.4657\n",
      "     37        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0136\u001b[0m  8.6641\n",
      "     38        \u001b[36m0.0139\u001b[0m        0.0138  8.6191\n",
      "     39        \u001b[36m0.0137\u001b[0m        0.0137  8.6856\n",
      "     40        \u001b[36m0.0137\u001b[0m        0.0146  8.5404\n",
      "     41        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0133\u001b[0m  8.6852\n",
      "     42        \u001b[36m0.0135\u001b[0m        0.0137  8.4927\n",
      "     43        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0132\u001b[0m  8.6732\n",
      "     44        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0131\u001b[0m  8.3512\n",
      "     45        \u001b[36m0.0133\u001b[0m        0.0134  8.2574\n",
      "     46        \u001b[36m0.0132\u001b[0m        \u001b[32m0.0130\u001b[0m  8.2972\n",
      "     47        \u001b[36m0.0132\u001b[0m        0.0131  8.4244\n",
      "     48        \u001b[36m0.0131\u001b[0m        0.0136  8.1987\n",
      "     49        \u001b[36m0.0131\u001b[0m        0.0134  7.3971\n",
      "     50        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0128\u001b[0m  7.9222\n",
      "     51        \u001b[36m0.0130\u001b[0m        0.0128  6.6105\n",
      "     52        \u001b[36m0.0129\u001b[0m        0.0130  7.5168\n",
      "     53        \u001b[36m0.0128\u001b[0m        0.0139  5.8803\n",
      "     54        \u001b[36m0.0128\u001b[0m        0.0133  6.6893\n",
      "     55        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0127\u001b[0m  6.4023\n",
      "     56        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0124\u001b[0m  6.1989\n",
      "     57        \u001b[36m0.0126\u001b[0m        0.0124  7.6342\n",
      "     58        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0123\u001b[0m  6.5498\n",
      "     59        \u001b[36m0.0125\u001b[0m        0.0125  7.4692\n",
      "     60        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0123\u001b[0m  7.3872\n",
      "     61        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0122\u001b[0m  7.2491\n",
      "     62        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0121\u001b[0m  6.7128\n",
      "     63        \u001b[36m0.0123\u001b[0m        0.0123  7.5380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     64        \u001b[36m0.0123\u001b[0m        0.0122  7.5532\n",
      "     65        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0120\u001b[0m  7.1806\n",
      "     66        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0120\u001b[0m  6.9521\n",
      "     67        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0119\u001b[0m  7.2010\n",
      "     68        \u001b[36m0.0121\u001b[0m        0.0119  7.8944\n",
      "     69        \u001b[36m0.0121\u001b[0m        0.0119  8.0888\n",
      "     70        0.0121        \u001b[32m0.0118\u001b[0m  6.7946\n",
      "     71        \u001b[36m0.0120\u001b[0m        0.0126  6.7432\n",
      "     72        0.0121        \u001b[32m0.0118\u001b[0m  7.7889\n",
      "     73        \u001b[36m0.0120\u001b[0m        0.0120  7.0900\n",
      "     74        \u001b[36m0.0120\u001b[0m        0.0121  7.4960\n",
      "     75        \u001b[36m0.0119\u001b[0m        0.0120  7.0760\n",
      "     76        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0117\u001b[0m  7.1073\n",
      "     77        0.0119        \u001b[32m0.0117\u001b[0m  7.0751\n",
      "     78        \u001b[36m0.0119\u001b[0m        0.0117  8.1329\n",
      "     79        \u001b[36m0.0119\u001b[0m        0.0119  8.3926\n",
      "     80        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0116\u001b[0m  8.4518\n",
      "     81        \u001b[36m0.0118\u001b[0m        \u001b[32m0.0116\u001b[0m  8.4502\n",
      "     82        \u001b[36m0.0118\u001b[0m        0.0121  8.2385\n",
      "     83        0.0118        0.0117  8.3004\n",
      "     84        \u001b[36m0.0118\u001b[0m        0.0118  8.3196\n",
      "     85        \u001b[36m0.0118\u001b[0m        0.0118  8.5884\n",
      "     86        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0116\u001b[0m  8.2238\n",
      "     87        \u001b[36m0.0117\u001b[0m        \u001b[32m0.0115\u001b[0m  8.5992\n",
      "     88        \u001b[36m0.0116\u001b[0m        0.0116  8.5499\n",
      "     89        \u001b[36m0.0116\u001b[0m        0.0117  8.4207\n",
      "     90        \u001b[36m0.0116\u001b[0m        0.0115  8.2345\n",
      "     91        0.0116        \u001b[32m0.0115\u001b[0m  8.4770\n",
      "     92        \u001b[36m0.0116\u001b[0m        0.0117  7.9924\n",
      "     93        \u001b[36m0.0116\u001b[0m        0.0116  8.1792\n",
      "     94        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0113\u001b[0m  8.3660\n",
      "     95        \u001b[36m0.0115\u001b[0m        \u001b[32m0.0112\u001b[0m  8.4607\n",
      "     96        \u001b[36m0.0114\u001b[0m        0.0115  8.4595\n",
      "     97        \u001b[36m0.0114\u001b[0m        0.0116  8.4959\n",
      "     98        \u001b[36m0.0114\u001b[0m        0.0112  8.3817\n",
      "     99        \u001b[36m0.0113\u001b[0m        0.0115  8.4062\n",
      "    100        \u001b[36m0.0113\u001b[0m        \u001b[32m0.0110\u001b[0m  8.4600\n",
      "    101        \u001b[36m0.0113\u001b[0m        0.0111  8.3754\n",
      "    102        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0109\u001b[0m  8.4437\n",
      "    103        \u001b[36m0.0112\u001b[0m        0.0111  8.4962\n",
      "    104        \u001b[36m0.0111\u001b[0m        0.0115  8.4507\n",
      "    105        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0108\u001b[0m  8.3543\n",
      "    106        \u001b[36m0.0110\u001b[0m        0.0111  8.3919\n",
      "    107        \u001b[36m0.0110\u001b[0m        0.0108  8.4646\n",
      "    108        \u001b[36m0.0109\u001b[0m        0.0113  8.4222\n",
      "    109        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0108\u001b[0m  8.4170\n",
      "    110        0.0109        \u001b[32m0.0106\u001b[0m  8.4832\n",
      "    111        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0105\u001b[0m  8.3372\n",
      "    112        \u001b[36m0.0108\u001b[0m        0.0109  8.5064\n",
      "    113        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0104\u001b[0m  8.4026\n",
      "    114        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0104\u001b[0m  8.4364\n",
      "    115        \u001b[36m0.0106\u001b[0m        0.0108  8.4542\n",
      "    116        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0103\u001b[0m  8.4693\n",
      "    117        \u001b[36m0.0105\u001b[0m        0.0103  8.4415\n",
      "    118        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0103\u001b[0m  8.4531\n",
      "    119        \u001b[36m0.0104\u001b[0m        0.0104  8.3835\n",
      "    120        \u001b[36m0.0103\u001b[0m        0.0107  8.3708\n",
      "    121        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0101\u001b[0m  8.4132\n",
      "    122        0.0103        \u001b[32m0.0100\u001b[0m  8.3759\n",
      "    123        \u001b[36m0.0102\u001b[0m        0.0116  8.4356\n",
      "    124        0.0102        0.0101  8.4241\n",
      "    125        \u001b[36m0.0101\u001b[0m        0.0103  8.4132\n",
      "    126        \u001b[36m0.0100\u001b[0m        0.0104  8.4510\n",
      "    127        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0097\u001b[0m  8.4431\n",
      "    128        \u001b[36m0.0099\u001b[0m        0.0102  8.4081\n",
      "    129        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0096\u001b[0m  8.4105\n",
      "    130        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0095\u001b[0m  8.4186\n",
      "    131        0.0099        0.0101  8.3905\n",
      "    132        \u001b[36m0.0098\u001b[0m        0.0095  8.4624\n",
      "    133        \u001b[36m0.0097\u001b[0m        0.0096  8.4018\n",
      "    134        \u001b[36m0.0096\u001b[0m        0.0098  8.4290\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.41959308755837343\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0165\u001b[0m  8.3675\n",
      "      2        \u001b[36m0.0169\u001b[0m        0.0166  8.4144\n",
      "      3        \u001b[36m0.0169\u001b[0m        0.0165  8.4424\n",
      "      4        0.0169        \u001b[32m0.0164\u001b[0m  8.3881\n",
      "      5        0.0169        0.0167  8.5217\n",
      "      6        0.0169        0.0164  8.6832\n",
      "      7        0.0169        0.0165  8.4758\n",
      "      8        \u001b[36m0.0169\u001b[0m        0.0169  8.5088\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.06239356547326991\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0256\u001b[0m        \u001b[32m0.0250\u001b[0m  8.3757\n",
      "      2        \u001b[36m0.0251\u001b[0m        \u001b[32m0.0249\u001b[0m  8.3051\n",
      "      3        \u001b[36m0.0250\u001b[0m        \u001b[32m0.0249\u001b[0m  8.3787\n",
      "      4        \u001b[36m0.0250\u001b[0m        0.0250  8.4129\n",
      "      5        \u001b[36m0.0250\u001b[0m        \u001b[32m0.0248\u001b[0m  7.5726\n",
      "      6        \u001b[36m0.0249\u001b[0m        \u001b[32m0.0248\u001b[0m  7.9544\n",
      "      7        \u001b[36m0.0249\u001b[0m        0.0249  7.4257\n",
      "      8        \u001b[36m0.0249\u001b[0m        0.0254  8.1814\n",
      "      9        \u001b[36m0.0248\u001b[0m        \u001b[32m0.0247\u001b[0m  7.8442\n",
      "     10        0.0249        0.0247  8.3988\n",
      "     11        \u001b[36m0.0248\u001b[0m        0.0248  8.4998\n",
      "     12        \u001b[36m0.0248\u001b[0m        0.0248  8.5086\n",
      "     13        \u001b[36m0.0248\u001b[0m        0.0249  8.4633\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.008752440704312803\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0276\u001b[0m        \u001b[32m0.0250\u001b[0m  8.4753\n",
      "      2        \u001b[36m0.0254\u001b[0m        \u001b[32m0.0247\u001b[0m  8.4032\n",
      "      3        \u001b[36m0.0253\u001b[0m        \u001b[32m0.0246\u001b[0m  8.4480\n",
      "      4        \u001b[36m0.0253\u001b[0m        0.0249  8.5354\n",
      "      5        \u001b[36m0.0253\u001b[0m        0.0256  8.4974\n",
      "      6        \u001b[36m0.0252\u001b[0m        0.0246  8.4793\n",
      "      7        0.0252        \u001b[32m0.0245\u001b[0m  8.4812\n",
      "      8        \u001b[36m0.0252\u001b[0m        0.0246  8.4708\n",
      "      9        0.0252        \u001b[32m0.0245\u001b[0m  8.4098\n",
      "     10        0.0252        0.0246  8.3860\n",
      "     11        \u001b[36m0.0252\u001b[0m        0.0246  8.3661\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.003302079063449481\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0259\u001b[0m        \u001b[32m0.0240\u001b[0m  8.4591\n",
      "      2        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0240\u001b[0m  7.8167\n",
      "      3        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0240\u001b[0m  7.7761\n",
      "      4        \u001b[36m0.0244\u001b[0m        0.0241  7.7038\n",
      "      5        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0239\u001b[0m  7.6429\n",
      "      6        \u001b[36m0.0243\u001b[0m        0.0240  7.5411\n",
      "      7        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0239\u001b[0m  7.9255\n",
      "      8        \u001b[36m0.0242\u001b[0m        0.0242  7.4131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m0.0242\u001b[0m        0.0247  8.2471\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.0316279803564713\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0259\u001b[0m        \u001b[32m0.0247\u001b[0m  7.5800\n",
      "      2        \u001b[36m0.0247\u001b[0m        \u001b[32m0.0247\u001b[0m  7.4064\n",
      "      3        \u001b[36m0.0247\u001b[0m        \u001b[32m0.0245\u001b[0m  8.2657\n",
      "      4        \u001b[36m0.0246\u001b[0m        0.0249  7.3258\n",
      "      5        \u001b[36m0.0246\u001b[0m        0.0246  8.1779\n",
      "      6        \u001b[36m0.0245\u001b[0m        0.0245  7.7184\n",
      "      7        \u001b[36m0.0245\u001b[0m        \u001b[32m0.0244\u001b[0m  8.1383\n",
      "      8        \u001b[36m0.0245\u001b[0m        0.0250  7.7067\n",
      "      9        \u001b[36m0.0245\u001b[0m        0.0244  8.1773\n",
      "     10        \u001b[36m0.0245\u001b[0m        \u001b[32m0.0243\u001b[0m  7.6306\n",
      "     11        0.0245        0.0248  8.0948\n",
      "     12        \u001b[36m0.0245\u001b[0m        0.0244  7.5790\n",
      "     13        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0243\u001b[0m  8.0273\n",
      "     14        0.0244        0.0243  8.4610\n",
      "     15        \u001b[36m0.0244\u001b[0m        0.0245  8.4180\n",
      "     16        0.0244        0.0245  8.1386\n",
      "     17        \u001b[36m0.0244\u001b[0m        0.0243  8.2220\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.016669690535094817\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0264\u001b[0m        \u001b[32m0.0253\u001b[0m  8.0811\n",
      "      2        \u001b[36m0.0253\u001b[0m        0.0270  7.7531\n",
      "      3        0.0254        \u001b[32m0.0252\u001b[0m  8.3105\n",
      "      4        0.0254        \u001b[32m0.0252\u001b[0m  7.6841\n",
      "      5        0.0253        0.0252  8.0579\n",
      "      6        \u001b[36m0.0253\u001b[0m        0.0252  8.3352\n",
      "      7        \u001b[36m0.0253\u001b[0m        0.0252  7.9886\n",
      "      8        \u001b[36m0.0253\u001b[0m        0.0252  7.7493\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.00022358143767675642\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0258\u001b[0m        \u001b[32m0.0251\u001b[0m  7.7724\n",
      "      2        \u001b[36m0.0252\u001b[0m        0.0251  7.7810\n",
      "      3        \u001b[36m0.0252\u001b[0m        0.0252  5.8915\n",
      "      4        \u001b[36m0.0251\u001b[0m        0.0255  6.7345\n",
      "      5        \u001b[36m0.0251\u001b[0m        0.0255  6.5890\n",
      "      6        0.0251        \u001b[32m0.0250\u001b[0m  6.6911\n",
      "      7        0.0251        0.0251  6.3485\n",
      "      8        \u001b[36m0.0251\u001b[0m        0.0258  5.9602\n",
      "      9        \u001b[36m0.0251\u001b[0m        0.0252  6.4485\n",
      "     10        0.0251        0.0251  6.6229\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.02236840115110006\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0258\u001b[0m        \u001b[32m0.0257\u001b[0m  7.2535\n",
      "      2        \u001b[36m0.0250\u001b[0m        \u001b[32m0.0255\u001b[0m  8.0101\n",
      "      3        \u001b[36m0.0250\u001b[0m        0.0257  8.2928\n",
      "      4        0.0250        0.0260  8.0301\n",
      "      5        \u001b[36m0.0250\u001b[0m        0.0257  8.0245\n",
      "      6        \u001b[36m0.0249\u001b[0m        0.0258  8.0446\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.011624892767658546\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0276\u001b[0m        \u001b[32m0.0251\u001b[0m  8.0348\n",
      "      2        \u001b[36m0.0251\u001b[0m        \u001b[32m0.0250\u001b[0m  7.8579\n",
      "      3        \u001b[36m0.0250\u001b[0m        0.0251  8.0960\n",
      "      4        \u001b[36m0.0250\u001b[0m        0.0255  8.4519\n",
      "      5        0.0251        \u001b[32m0.0250\u001b[0m  8.3776\n",
      "      6        \u001b[36m0.0250\u001b[0m        0.0250  8.5563\n",
      "      7        0.0251        0.0251  8.6762\n",
      "      8        0.0250        \u001b[32m0.0249\u001b[0m  8.6651\n",
      "      9        \u001b[36m0.0250\u001b[0m        0.0250  8.5817\n",
      "     10        0.0250        0.0250  8.2831\n",
      "     11        0.0250        0.0250  8.3116\n",
      "     12        0.0250        0.0253  8.2486\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.00167202892909879\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0256\u001b[0m        \u001b[32m0.0256\u001b[0m  8.3075\n",
      "      2        \u001b[36m0.0251\u001b[0m        \u001b[32m0.0250\u001b[0m  8.2244\n",
      "      3        0.0252        0.0252  8.2490\n",
      "      4        0.0251        0.0250  8.2536\n",
      "      5        \u001b[36m0.0251\u001b[0m        0.0252  8.3566\n",
      "      6        \u001b[36m0.0251\u001b[0m        0.0253  8.3680\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.005249322362020026\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0264\u001b[0m        \u001b[32m0.0247\u001b[0m  8.3449\n",
      "      2        \u001b[36m0.0250\u001b[0m        0.0248  8.3816\n",
      "      3        \u001b[36m0.0249\u001b[0m        \u001b[32m0.0246\u001b[0m  8.3574\n",
      "      4        \u001b[36m0.0249\u001b[0m        \u001b[32m0.0246\u001b[0m  8.3743\n",
      "      5        \u001b[36m0.0249\u001b[0m        0.0246  8.3641\n",
      "      6        \u001b[36m0.0248\u001b[0m        \u001b[32m0.0246\u001b[0m  8.3663\n",
      "      7        0.0248        0.0246  8.3854\n",
      "      8        \u001b[36m0.0247\u001b[0m        0.0246  8.3537\n",
      "      9        \u001b[36m0.0247\u001b[0m        0.0247  8.3606\n",
      "     10        \u001b[36m0.0247\u001b[0m        0.0246  8.3956\n",
      "     11        \u001b[36m0.0246\u001b[0m        \u001b[32m0.0244\u001b[0m  8.4069\n",
      "     12        \u001b[36m0.0246\u001b[0m        \u001b[32m0.0243\u001b[0m  8.3903\n",
      "     13        \u001b[36m0.0245\u001b[0m        \u001b[32m0.0242\u001b[0m  8.3568\n",
      "     14        \u001b[36m0.0244\u001b[0m        0.0245  8.3833\n",
      "     15        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0242\u001b[0m  8.3759\n",
      "     16        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0241\u001b[0m  8.3878\n",
      "     17        \u001b[36m0.0244\u001b[0m        0.0241  8.3548\n",
      "     18        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0241\u001b[0m  8.3554\n",
      "     19        0.0243        0.0242  8.3321\n",
      "     20        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0240\u001b[0m  8.3917\n",
      "     21        0.0243        0.0241  8.3673\n",
      "     22        \u001b[36m0.0243\u001b[0m        0.0240  8.3218\n",
      "     23        \u001b[36m0.0242\u001b[0m        0.0240  8.3395\n",
      "     24        0.0242        0.0240  8.3606\n",
      "     25        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0240\u001b[0m  8.3890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     26        0.0242        0.0240  8.3814\n",
      "     27        0.0242        0.0240  8.3893\n",
      "     28        0.0242        0.0240  8.3730\n",
      "     29        0.0242        0.0241  8.3382\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.035293354651567355\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0300\u001b[0m  8.3717\n",
      "      2        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0289\u001b[0m  8.3650\n",
      "      3        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0289\u001b[0m  8.3949\n",
      "      4        \u001b[36m0.0295\u001b[0m        0.0294  8.3917\n",
      "      5        0.0295        0.0290  8.3706\n",
      "      6        0.0295        \u001b[32m0.0289\u001b[0m  8.3665\n",
      "      7        \u001b[36m0.0294\u001b[0m        \u001b[32m0.0288\u001b[0m  8.4143\n",
      "      8        0.0294        0.0289  8.4624\n",
      "      9        \u001b[36m0.0294\u001b[0m        0.0295  8.3778\n",
      "     10        0.0294        0.0289  8.4472\n",
      "     11        0.0294        0.0292  8.4081\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.000991220898074907\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0306\u001b[0m        \u001b[32m0.0299\u001b[0m  8.3920\n",
      "      2        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0299\u001b[0m  8.4941\n",
      "      3        \u001b[36m0.0298\u001b[0m        0.0300  8.4670\n",
      "      4        \u001b[36m0.0298\u001b[0m        0.0300  8.3595\n",
      "      5        \u001b[36m0.0297\u001b[0m        0.0305  8.4592\n",
      "      6        0.0297        \u001b[32m0.0299\u001b[0m  8.5275\n",
      "      7        \u001b[36m0.0297\u001b[0m        0.0303  8.3588\n",
      "      8        \u001b[36m0.0297\u001b[0m        0.0299  7.5722\n",
      "      9        \u001b[36m0.0297\u001b[0m        0.0299  7.2369\n",
      "     10        0.0297        \u001b[32m0.0299\u001b[0m  7.5364\n",
      "     11        \u001b[36m0.0296\u001b[0m        0.0299  7.8354\n",
      "     12        0.0297        0.0307  8.1543\n",
      "     13        0.0297        0.0301  8.2756\n",
      "     14        \u001b[36m0.0296\u001b[0m        0.0300  8.2155\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.002248210527951544\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0296\u001b[0m  8.2901\n",
      "      2        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0296\u001b[0m  8.2550\n",
      "      3        \u001b[36m0.0297\u001b[0m        0.0296  8.4174\n",
      "      4        \u001b[36m0.0296\u001b[0m        0.0298  8.3502\n",
      "      5        \u001b[36m0.0296\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3211\n",
      "      6        \u001b[36m0.0296\u001b[0m        0.0297  8.3601\n",
      "      7        0.0296        0.0298  8.3892\n",
      "      8        \u001b[36m0.0296\u001b[0m        0.0298  8.3816\n",
      "      9        0.0296        \u001b[32m0.0295\u001b[0m  7.4113\n",
      "     10        \u001b[36m0.0296\u001b[0m        0.0295  7.9664\n",
      "     11        \u001b[36m0.0295\u001b[0m        0.0297  7.3343\n",
      "     12        0.0295        0.0296  8.1667\n",
      "     13        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0294\u001b[0m  7.5543\n",
      "     14        \u001b[36m0.0295\u001b[0m        0.0297  8.1730\n",
      "     15        \u001b[36m0.0295\u001b[0m        0.0296  8.5265\n",
      "     16        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0294\u001b[0m  8.4000\n",
      "     17        \u001b[36m0.0295\u001b[0m        0.0294  8.3474\n",
      "     18        0.0295        \u001b[32m0.0294\u001b[0m  8.3869\n",
      "     19        \u001b[36m0.0294\u001b[0m        0.0294  8.3465\n",
      "     20        \u001b[36m0.0294\u001b[0m        \u001b[32m0.0293\u001b[0m  8.3782\n",
      "     21        0.0295        0.0293  8.3962\n",
      "     22        0.0294        0.0294  8.4411\n",
      "     23        \u001b[36m0.0294\u001b[0m        0.0294  8.2317\n",
      "     24        0.0295        0.0295  8.2430\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.005989711592632951\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0310\u001b[0m        \u001b[32m0.0307\u001b[0m  8.2805\n",
      "      2        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0303\u001b[0m  8.3518\n",
      "      3        \u001b[36m0.0296\u001b[0m        0.0305  8.3828\n",
      "      4        \u001b[36m0.0295\u001b[0m        0.0304  8.3659\n",
      "      5        \u001b[36m0.0295\u001b[0m        0.0313  8.3548\n",
      "      6        \u001b[36m0.0295\u001b[0m        0.0308  8.3651\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.012209065360362148\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0311\u001b[0m        \u001b[32m0.0301\u001b[0m  8.3284\n",
      "      2        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0290\u001b[0m  8.3907\n",
      "      3        0.0298        \u001b[32m0.0290\u001b[0m  8.3739\n",
      "      4        0.0299        0.0290  8.1566\n",
      "      5        \u001b[36m0.0298\u001b[0m        0.0290  8.1793\n",
      "      6        0.0299        0.0291  7.3720\n",
      "      7        0.0298        0.0290  7.6981\n",
      "      8        0.0298        \u001b[32m0.0290\u001b[0m  7.6342\n",
      "      9        \u001b[36m0.0298\u001b[0m        0.0291  8.1952\n",
      "     10        0.0298        0.0290  7.6275\n",
      "     11        0.0298        0.0291  8.1692\n",
      "     12        \u001b[36m0.0298\u001b[0m        0.0290  7.5539\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.00014176375927932305\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0297\u001b[0m  8.0667\n",
      "      2        \u001b[36m0.0296\u001b[0m        \u001b[32m0.0296\u001b[0m  7.6896\n",
      "      3        \u001b[36m0.0296\u001b[0m        0.0302  8.0946\n",
      "      4        \u001b[36m0.0296\u001b[0m        \u001b[32m0.0295\u001b[0m  8.1009\n",
      "      5        0.0296        \u001b[32m0.0295\u001b[0m  8.0639\n",
      "      6        \u001b[36m0.0295\u001b[0m        0.0296  8.3981\n",
      "      7        0.0295        0.0296  7.9664\n",
      "      8        0.0295        0.0295  7.8867\n",
      "      9        0.0295        0.0296  7.9369\n",
      "     10        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0295\u001b[0m  8.1738\n",
      "     11        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0295\u001b[0m  8.2413\n",
      "     12        \u001b[36m0.0295\u001b[0m        \u001b[32m0.0294\u001b[0m  7.9917\n",
      "     13        0.0295        0.0295  7.9933\n",
      "     14        \u001b[36m0.0295\u001b[0m        0.0295  8.2563\n",
      "     15        0.0295        0.0297  8.3949\n",
      "     16        0.0295        0.0295  8.4766\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 0.0068092903183629705\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0307\u001b[0m        \u001b[32m0.0295\u001b[0m  8.5046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0300\u001b[0m        0.0295  8.4020\n",
      "      3        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0294\u001b[0m  8.4931\n",
      "      4        \u001b[36m0.0299\u001b[0m        0.0295  8.2300\n",
      "      5        0.0299        0.0294  8.2252\n",
      "      6        \u001b[36m0.0298\u001b[0m        0.0294  8.2540\n",
      "      7        0.0298        0.0296  8.1179\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.007829924379996633\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0308\u001b[0m        \u001b[32m0.0310\u001b[0m  8.1651\n",
      "      2        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0300\u001b[0m  8.1616\n",
      "      3        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0299\u001b[0m  8.0239\n",
      "      4        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0298\u001b[0m  8.0375\n",
      "      5        \u001b[36m0.0298\u001b[0m        0.0302  8.2479\n",
      "      6        \u001b[36m0.0297\u001b[0m        0.0299  8.1599\n",
      "      7        0.0297        0.0299  8.2780\n",
      "      8        \u001b[36m0.0297\u001b[0m        0.0299  8.4492\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.0010999435685850045\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0307\u001b[0m        \u001b[32m0.0298\u001b[0m  8.1666\n",
      "      2        \u001b[36m0.0298\u001b[0m        0.0302  8.5018\n",
      "      3        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0297\u001b[0m  8.4112\n",
      "      4        \u001b[36m0.0297\u001b[0m        0.0300  8.4488\n",
      "      5        \u001b[36m0.0297\u001b[0m        0.0299  8.3949\n",
      "      6        \u001b[36m0.0297\u001b[0m        0.0305  8.4310\n",
      "      7        0.0297        0.0305  8.4037\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.006347040676352833\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0294\u001b[0m  8.3589\n",
      "      2        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0294\u001b[0m  8.3728\n",
      "      3        \u001b[36m0.0298\u001b[0m        0.0299  8.3704\n",
      "      4        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0294\u001b[0m  8.3755\n",
      "      5        0.0298        0.0299  8.3781\n",
      "      6        \u001b[36m0.0297\u001b[0m        0.0294  8.3760\n",
      "      7        \u001b[36m0.0297\u001b[0m        0.0295  8.4705\n",
      "      8        0.0297        0.0294  8.4156\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.010570166920487623\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0170\u001b[0m  7.7748\n",
      "      2        \u001b[36m0.0171\u001b[0m        0.0171  7.8671\n",
      "      3        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0169\u001b[0m  7.9353\n",
      "      4        \u001b[36m0.0171\u001b[0m        0.0169  7.6428\n",
      "      5        0.0171        \u001b[32m0.0169\u001b[0m  7.5778\n",
      "      6        0.0171        0.0170  7.9754\n",
      "      7        \u001b[36m0.0171\u001b[0m        0.0170  7.9389\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0005530148346597485\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0168\u001b[0m  7.9345\n",
      "      2        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0167\u001b[0m  7.6361\n",
      "      3        \u001b[36m0.0170\u001b[0m        0.0168  7.5652\n",
      "      4        \u001b[36m0.0170\u001b[0m        0.0167  8.1485\n",
      "      5        \u001b[36m0.0170\u001b[0m        0.0168  8.3411\n",
      "      6        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3625\n",
      "      7        0.0169        0.0167  8.3249\n",
      "      8        0.0170        0.0167  8.3250\n",
      "      9        \u001b[36m0.0169\u001b[0m        0.0167  8.2460\n",
      "     10        \u001b[36m0.0169\u001b[0m        0.0170  8.1718\n",
      "     11        0.0169        \u001b[32m0.0166\u001b[0m  8.1581\n",
      "     12        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0166\u001b[0m  8.1767\n",
      "     13        \u001b[36m0.0169\u001b[0m        0.0168  8.1885\n",
      "     14        \u001b[36m0.0169\u001b[0m        0.0168  8.1926\n",
      "     15        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0166\u001b[0m  8.1888\n",
      "     16        0.0169        0.0169  8.1117\n",
      "     17        0.0169        \u001b[32m0.0166\u001b[0m  8.3268\n",
      "     18        \u001b[36m0.0169\u001b[0m        0.0167  8.3252\n",
      "     19        0.0169        0.0167  8.3535\n",
      "     20        \u001b[36m0.0169\u001b[0m        0.0168  8.3496\n",
      "     21        0.0169        \u001b[32m0.0166\u001b[0m  8.3283\n",
      "     22        \u001b[36m0.0169\u001b[0m        0.0167  8.3310\n",
      "     23        0.0169        0.0167  8.3101\n",
      "     24        0.0169        \u001b[32m0.0166\u001b[0m  8.3304\n",
      "     25        0.0169        0.0166  8.3423\n",
      "     26        0.0169        0.0167  8.3425\n",
      "     27        \u001b[36m0.0169\u001b[0m        0.0166  8.3617\n",
      "     28        \u001b[36m0.0168\u001b[0m        0.0167  8.3334\n",
      "     29        0.0168        \u001b[32m0.0166\u001b[0m  8.3199\n",
      "     30        0.0168        \u001b[32m0.0166\u001b[0m  8.3544\n",
      "     31        0.0169        0.0166  8.3489\n",
      "     32        0.0168        0.0166  8.3262\n",
      "     33        \u001b[36m0.0168\u001b[0m        0.0167  8.3405\n",
      "     34        0.0168        \u001b[32m0.0166\u001b[0m  8.3161\n",
      "     35        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3387\n",
      "     36        0.0168        0.0169  8.3390\n",
      "     37        0.0168        0.0167  8.3417\n",
      "     38        \u001b[36m0.0168\u001b[0m        0.0166  8.3503\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.010045531973354782\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0170\u001b[0m  8.3396\n",
      "      2        \u001b[36m0.0170\u001b[0m        0.0170  8.3631\n",
      "      3        \u001b[36m0.0170\u001b[0m        0.0170  8.3351\n",
      "      4        0.0170        \u001b[32m0.0169\u001b[0m  8.3101\n",
      "      5        \u001b[36m0.0170\u001b[0m        0.0169  8.3402\n",
      "      6        0.0170        \u001b[32m0.0169\u001b[0m  8.3322\n",
      "      7        0.0170        0.0171  8.3524\n",
      "      8        \u001b[36m0.0170\u001b[0m        0.0170  8.3461\n",
      "      9        0.0170        \u001b[32m0.0169\u001b[0m  8.3559\n",
      "     10        \u001b[36m0.0170\u001b[0m        0.0170  8.3555\n",
      "     11        \u001b[36m0.0170\u001b[0m        0.0169  8.3538\n",
      "     12        0.0170        \u001b[32m0.0169\u001b[0m  8.3423\n",
      "     13        0.0170        0.0169  8.3668\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 0.003230622687146245\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0170\u001b[0m  8.3320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0168\u001b[0m        0.0172  8.3358\n",
      "      3        \u001b[36m0.0168\u001b[0m        0.0171  8.3332\n",
      "      4        0.0168        0.0170  8.3523\n",
      "      5        \u001b[36m0.0168\u001b[0m        0.0170  8.2973\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.008175688039891882\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0182\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3496\n",
      "      2        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3531\n",
      "      3        0.0170        \u001b[32m0.0169\u001b[0m  8.3585\n",
      "      4        \u001b[36m0.0170\u001b[0m        0.0170  8.3321\n",
      "      5        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3357\n",
      "      6        \u001b[36m0.0170\u001b[0m        0.0170  8.3486\n",
      "      7        \u001b[36m0.0170\u001b[0m        0.0170  8.3469\n",
      "      8        0.0170        0.0170  8.3532\n",
      "      9        \u001b[36m0.0169\u001b[0m        0.0169  8.3345\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of 0.0017977880492756615\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3331\n",
      "      2        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3502\n",
      "      3        \u001b[36m0.0170\u001b[0m        0.0169  8.3034\n",
      "      4        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3365\n",
      "      5        0.0170        0.0168  8.3108\n",
      "      6        \u001b[36m0.0170\u001b[0m        0.0174  8.3307\n",
      "      7        0.0170        \u001b[32m0.0167\u001b[0m  8.3283\n",
      "      8        \u001b[36m0.0170\u001b[0m        0.0169  8.3272\n",
      "      9        \u001b[36m0.0170\u001b[0m        0.0169  8.3196\n",
      "     10        0.0170        \u001b[32m0.0167\u001b[0m  8.3483\n",
      "     11        \u001b[36m0.0169\u001b[0m        0.0170  8.3151\n",
      "     12        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3350\n",
      "     13        \u001b[36m0.0169\u001b[0m        0.0168  8.3155\n",
      "     14        0.0169        \u001b[32m0.0167\u001b[0m  8.3204\n",
      "     15        \u001b[36m0.0169\u001b[0m        0.0167  8.3305\n",
      "     16        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3308\n",
      "     17        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3463\n",
      "     18        0.0169        0.0167  8.3353\n",
      "     19        \u001b[36m0.0169\u001b[0m        0.0169  8.3195\n",
      "     20        0.0169        0.0167  8.3339\n",
      "     21        0.0169        \u001b[32m0.0167\u001b[0m  8.3447\n",
      "     22        0.0169        0.0168  8.3213\n",
      "     23        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3553\n",
      "     24        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3473\n",
      "     25        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3593\n",
      "     26        0.0169        0.0167  8.3392\n",
      "     27        \u001b[36m0.0168\u001b[0m        0.0167  8.3223\n",
      "     28        0.0169        \u001b[32m0.0167\u001b[0m  8.3348\n",
      "     29        0.0169        0.0167  8.3358\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0007229460758264583\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0171\u001b[0m  8.3445\n",
      "      2        \u001b[36m0.0168\u001b[0m        0.0171  8.3237\n",
      "      3        \u001b[36m0.0167\u001b[0m        0.0177  8.3318\n",
      "      4        0.0168        \u001b[32m0.0170\u001b[0m  8.3291\n",
      "      5        0.0167        0.0171  8.3359\n",
      "      6        0.0167        0.0172  8.3061\n",
      "      7        \u001b[36m0.0167\u001b[0m        0.0171  8.3241\n",
      "      8        \u001b[36m0.0167\u001b[0m        0.0170  8.3271\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.004432122749233658\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0178\u001b[0m        \u001b[32m0.0182\u001b[0m  8.3389\n",
      "      2        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0171\u001b[0m  8.3137\n",
      "      3        \u001b[36m0.0171\u001b[0m        0.0172  8.3318\n",
      "      4        0.0171        \u001b[32m0.0171\u001b[0m  8.3297\n",
      "      5        \u001b[36m0.0171\u001b[0m        0.0171  8.3236\n",
      "      6        0.0171        0.0171  8.3381\n",
      "      7        \u001b[36m0.0171\u001b[0m        0.0171  8.3171\n",
      "      8        \u001b[36m0.0171\u001b[0m        0.0171  8.3325\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.024391313986247276\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0185\u001b[0m        \u001b[32m0.0172\u001b[0m  8.3034\n",
      "      2        \u001b[36m0.0172\u001b[0m        0.0174  8.3856\n",
      "      3        \u001b[36m0.0171\u001b[0m        0.0173  8.3231\n",
      "      4        \u001b[36m0.0171\u001b[0m        0.0173  8.3052\n",
      "      5        \u001b[36m0.0171\u001b[0m        0.0173  8.3458\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -1.8357426006199162e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0175\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3174\n",
      "      2        \u001b[36m0.0171\u001b[0m        0.0168  8.3337\n",
      "      3        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3432\n",
      "      4        \u001b[36m0.0170\u001b[0m        0.0168  8.3186\n",
      "      5        \u001b[36m0.0170\u001b[0m        0.0169  8.3401\n",
      "      6        0.0170        0.0168  8.3362\n",
      "      7        \u001b[36m0.0170\u001b[0m        0.0169  8.3227\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.0015328771056626334\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0172\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3381\n",
      "      2        \u001b[36m0.0169\u001b[0m        0.0171  8.3429\n",
      "      3        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3790\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0168  8.3681\n",
      "      5        \u001b[36m0.0168\u001b[0m        0.0170  8.3352\n",
      "      6        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3290\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0169  8.3708\n",
      "      8        \u001b[36m0.0168\u001b[0m        0.0167  8.3391\n",
      "      9        0.0168        0.0171  8.3546\n",
      "     10        \u001b[36m0.0167\u001b[0m        0.0169  8.3474\n",
      "     11        0.0167        \u001b[32m0.0167\u001b[0m  8.3232\n",
      "     12        \u001b[36m0.0167\u001b[0m        0.0170  8.3670\n",
      "     13        \u001b[36m0.0167\u001b[0m        0.0167  8.3402\n",
      "     14        \u001b[36m0.0167\u001b[0m        0.0167  8.3582\n",
      "     15        0.0167        0.0168  8.3435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0011346581917026644\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0173\u001b[0m  8.3672\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3327\n",
      "      3        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3394\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0167  8.3391\n",
      "      5        \u001b[36m0.0168\u001b[0m        0.0168  8.3533\n",
      "      6        \u001b[36m0.0168\u001b[0m        0.0169  8.3174\n",
      "      7        0.0168        \u001b[32m0.0167\u001b[0m  8.3399\n",
      "      8        \u001b[36m0.0168\u001b[0m        0.0167  8.3301\n",
      "      9        \u001b[36m0.0168\u001b[0m        0.0168  8.3716\n",
      "     10        \u001b[36m0.0168\u001b[0m        0.0167  8.3487\n",
      "     11        \u001b[36m0.0168\u001b[0m        0.0167  8.3725\n",
      "     12        0.0168        \u001b[32m0.0167\u001b[0m  8.3652\n",
      "     13        \u001b[36m0.0168\u001b[0m        0.0167  8.3461\n",
      "     14        \u001b[36m0.0168\u001b[0m        0.0169  8.3633\n",
      "     15        0.0168        0.0167  8.3360\n",
      "     16        0.0168        \u001b[32m0.0167\u001b[0m  8.3418\n",
      "     17        0.0168        \u001b[32m0.0167\u001b[0m  8.3432\n",
      "     18        \u001b[36m0.0168\u001b[0m        0.0167  8.3520\n",
      "     19        0.0168        0.0167  8.3724\n",
      "     20        \u001b[36m0.0168\u001b[0m        0.0168  8.3548\n",
      "     21        0.0168        0.0167  8.3501\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of 0.00036849875472577676\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3732\n",
      "      2        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3521\n",
      "      3        \u001b[36m0.0167\u001b[0m        0.0169  8.3571\n",
      "      4        \u001b[36m0.0167\u001b[0m        0.0170  8.3400\n",
      "      5        0.0167        0.0169  8.3227\n",
      "      6        \u001b[36m0.0167\u001b[0m        0.0168  8.3339\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0058809544016160675\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0186\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3607\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0165\u001b[0m  8.3529\n",
      "      3        \u001b[36m0.0169\u001b[0m        0.0166  8.3527\n",
      "      4        \u001b[36m0.0169\u001b[0m        0.0166  8.3621\n",
      "      5        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0165\u001b[0m  8.3594\n",
      "      6        \u001b[36m0.0169\u001b[0m        0.0166  8.3279\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.0026407059713198233\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3481\n",
      "      2        \u001b[36m0.0169\u001b[0m        0.0167  8.3657\n",
      "      3        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3557\n",
      "      4        \u001b[36m0.0169\u001b[0m        0.0166  8.3717\n",
      "      5        0.0169        0.0166  8.3252\n",
      "      6        0.0169        0.0166  8.3348\n",
      "      7        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3680\n",
      "      8        0.0169        0.0166  8.3340\n",
      "      9        \u001b[36m0.0168\u001b[0m        0.0166  8.3374\n",
      "     10        \u001b[36m0.0168\u001b[0m        0.0167  8.3402\n",
      "     11        0.0168        \u001b[32m0.0166\u001b[0m  8.3379\n",
      "     12        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3366\n",
      "     13        0.0168        0.0166  8.3391\n",
      "     14        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3418\n",
      "     15        0.0168        0.0166  8.3218\n",
      "     16        0.0168        \u001b[32m0.0165\u001b[0m  8.3493\n",
      "     17        \u001b[36m0.0168\u001b[0m        0.0166  8.3197\n",
      "     18        0.0168        0.0166  8.3193\n",
      "     19        0.0168        0.0166  8.3308\n",
      "     20        0.0168        \u001b[32m0.0165\u001b[0m  8.3614\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.00015092108522041237\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0184\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3516\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3062\n",
      "      3        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0168\u001b[0m  8.3160\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0171  8.3424\n",
      "      5        0.0168        \u001b[32m0.0168\u001b[0m  8.3280\n",
      "      6        \u001b[36m0.0168\u001b[0m        0.0169  8.3154\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0168  8.3166\n",
      "      8        \u001b[36m0.0168\u001b[0m        0.0173  8.3377\n",
      "      9        \u001b[36m0.0168\u001b[0m        0.0169  8.3170\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.008704486145761692\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0186\u001b[0m        \u001b[32m0.0172\u001b[0m  8.3405\n",
      "      2        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0170\u001b[0m  8.3054\n",
      "      3        \u001b[36m0.0168\u001b[0m        0.0170  8.3467\n",
      "      4        0.0168        \u001b[32m0.0170\u001b[0m  8.3825\n",
      "      5        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0170\u001b[0m  8.3787\n",
      "      6        \u001b[36m0.0168\u001b[0m        0.0170  8.3253\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0172  8.3367\n",
      "      8        0.0168        \u001b[32m0.0170\u001b[0m  8.3469\n",
      "      9        \u001b[36m0.0168\u001b[0m        0.0172  8.3664\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of 0.0028236682672548863\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0174\u001b[0m        \u001b[32m0.0167\u001b[0m  8.3493\n",
      "      2        \u001b[36m0.0169\u001b[0m        \u001b[32m0.0165\u001b[0m  8.3604\n",
      "      3        \u001b[36m0.0169\u001b[0m        0.0170  8.3598\n",
      "      4        0.0169        0.0165  8.3667\n",
      "      5        0.0169        0.0166  8.3674\n",
      "      6        \u001b[36m0.0169\u001b[0m        0.0166  8.3570\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.000363351668488221\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3457\n",
      "      2        \u001b[36m0.0168\u001b[0m        0.0167  8.3437\n",
      "      3        0.0168        \u001b[32m0.0166\u001b[0m  8.3463\n",
      "      4        \u001b[36m0.0168\u001b[0m        0.0171  8.3736\n",
      "      5        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3663\n",
      "      6        0.0168        0.0166  8.3507\n",
      "      7        \u001b[36m0.0168\u001b[0m        0.0167  8.3678\n",
      "      8        0.0168        0.0166  8.3456\n",
      "      9        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0166\u001b[0m  8.3721\n",
      "     10        \u001b[36m0.0168\u001b[0m        0.0167  8.3454\n",
      "     11        0.0168        0.0166  8.3921\n",
      "     12        \u001b[36m0.0167\u001b[0m        0.0168  8.3749\n",
      "     13        \u001b[36m0.0167\u001b[0m        0.0166  8.3305\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.008499857391413013\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3575\n",
      "      2        \u001b[36m0.0168\u001b[0m        0.0170  8.3768\n",
      "      3        \u001b[36m0.0167\u001b[0m        \u001b[32m0.0169\u001b[0m  8.3617\n",
      "      4        \u001b[36m0.0167\u001b[0m        0.0172  8.3730\n",
      "      5        0.0167        0.0170  8.3791\n",
      "      6        0.0167        0.0170  8.3281\n",
      "      7        \u001b[36m0.0167\u001b[0m        0.0169  8.3629\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.00153114503209828\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0237\u001b[0m  8.3524\n",
      "      2        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3305\n",
      "      3        0.0237        \u001b[32m0.0234\u001b[0m  8.3771\n",
      "      4        0.0237        0.0237  8.3494\n",
      "      5        \u001b[36m0.0236\u001b[0m        0.0235  8.3591\n",
      "      6        0.0237        0.0235  8.3734\n",
      "      7        0.0236        0.0235  8.3664\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of 0.0018026324969691032\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0248\u001b[0m        \u001b[32m0.0240\u001b[0m  8.3904\n",
      "      2        \u001b[36m0.0237\u001b[0m        0.0255  8.3626\n",
      "      3        \u001b[36m0.0237\u001b[0m        0.0242  8.3492\n",
      "      4        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0238\u001b[0m  8.3735\n",
      "      5        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0238\u001b[0m  8.3768\n",
      "      6        0.0237        0.0238  8.3437\n",
      "      7        0.0237        0.0238  8.3619\n",
      "      8        \u001b[36m0.0237\u001b[0m        0.0245  8.3576\n",
      "      9        \u001b[36m0.0236\u001b[0m        0.0240  8.3605\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.004402868249174352\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0240\u001b[0m  7.9006\n",
      "      2        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0238\u001b[0m  7.7972\n",
      "      3        0.0238        \u001b[32m0.0237\u001b[0m  8.3381\n",
      "      4        \u001b[36m0.0238\u001b[0m        0.0245  8.3426\n",
      "      5        0.0238        \u001b[32m0.0236\u001b[0m  8.3810\n",
      "      6        0.0238        0.0238  8.3326\n",
      "      7        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3674\n",
      "      8        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3741\n",
      "      9        0.0237        \u001b[32m0.0236\u001b[0m  8.3625\n",
      "     10        0.0237        0.0239  8.3367\n",
      "     11        \u001b[36m0.0237\u001b[0m        0.0237  8.3505\n",
      "     12        0.0237        0.0236  8.3434\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0001530200551111971\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0241\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3715\n",
      "      2        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3407\n",
      "      3        \u001b[36m0.0238\u001b[0m        0.0236  8.3247\n",
      "      4        \u001b[36m0.0237\u001b[0m        0.0238  8.3757\n",
      "      5        0.0237        0.0235  8.3599\n",
      "      6        0.0238        0.0234  8.3415\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of 0.0005758843239832556\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0251\u001b[0m        \u001b[32m0.0240\u001b[0m  8.1054\n",
      "      2        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0240\u001b[0m  8.4320\n",
      "      3        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0240\u001b[0m  8.3155\n",
      "      4        \u001b[36m0.0237\u001b[0m        0.0241  8.3717\n",
      "      5        \u001b[36m0.0237\u001b[0m        0.0241  8.3333\n",
      "      6        0.0237        \u001b[32m0.0240\u001b[0m  8.3408\n",
      "      7        0.0237        0.0240  8.3355\n",
      "      8        \u001b[36m0.0237\u001b[0m        0.0240  8.3170\n",
      "      9        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0240\u001b[0m  8.4046\n",
      "     10        0.0236        0.0242  8.3400\n",
      "     11        0.0237        0.0241  8.3276\n",
      "     12        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0240\u001b[0m  8.3435\n",
      "     13        0.0236        0.0243  8.6398\n",
      "     14        0.0236        0.0240  8.3954\n",
      "     15        \u001b[36m0.0236\u001b[0m        0.0240  8.3804\n",
      "     16        0.0236        \u001b[32m0.0240\u001b[0m  8.2376\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.00016978917034893293\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3017\n",
      "      2        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0235\u001b[0m  8.2287\n",
      "      3        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3283\n",
      "      4        0.0237        0.0236  8.3172\n",
      "      5        0.0237        \u001b[32m0.0234\u001b[0m  8.3113\n",
      "      6        \u001b[36m0.0237\u001b[0m        0.0234  8.3125\n",
      "      7        \u001b[36m0.0237\u001b[0m        0.0234  8.3397\n",
      "      8        0.0237        \u001b[32m0.0234\u001b[0m  8.3737\n",
      "      9        \u001b[36m0.0236\u001b[0m        0.0236  8.3870\n",
      "     10        0.0236        0.0234  8.3112\n",
      "     11        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3629\n",
      "     12        0.0236        \u001b[32m0.0233\u001b[0m  8.3904\n",
      "     13        \u001b[36m0.0236\u001b[0m        0.0238  8.3959\n",
      "     14        \u001b[36m0.0236\u001b[0m        0.0234  8.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15        0.0236        0.0234  8.3469\n",
      "     16        \u001b[36m0.0236\u001b[0m        0.0234  8.3594\n",
      "     17        0.0236        \u001b[32m0.0233\u001b[0m  8.3472\n",
      "     18        0.0236        0.0233  8.3740\n",
      "     19        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3717\n",
      "     20        0.0236        \u001b[32m0.0233\u001b[0m  8.3648\n",
      "     21        0.0236        0.0233  8.4498\n",
      "     22        \u001b[36m0.0236\u001b[0m        0.0233  8.4706\n",
      "     23        \u001b[36m0.0236\u001b[0m        0.0233  8.3592\n",
      "     24        0.0236        0.0234  8.4211\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.007544186622047366\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0240\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3573\n",
      "      2        \u001b[36m0.0238\u001b[0m        0.0236  8.3826\n",
      "      3        0.0238        0.0236  8.3319\n",
      "      4        \u001b[36m0.0238\u001b[0m        0.0238  8.3941\n",
      "      5        0.0238        0.0239  8.2999\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.005317768861715866\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0254\u001b[0m        \u001b[32m0.0235\u001b[0m  8.4119\n",
      "      2        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0234\u001b[0m  8.4807\n",
      "      3        \u001b[36m0.0238\u001b[0m        0.0240  8.3624\n",
      "      4        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0234\u001b[0m  8.2394\n",
      "      5        \u001b[36m0.0237\u001b[0m        0.0236  8.3523\n",
      "      6        \u001b[36m0.0237\u001b[0m        0.0234  8.3619\n",
      "      7        0.0237        0.0238  8.1840\n",
      "      8        0.0237        0.0244  8.3517\n",
      "      9        0.0237        \u001b[32m0.0234\u001b[0m  8.3618\n",
      "     10        0.0237        \u001b[32m0.0233\u001b[0m  8.3411\n",
      "     11        0.0237        0.0236  8.3498\n",
      "     12        0.0237        0.0234  8.3323\n",
      "     13        0.0237        0.0234  8.3292\n",
      "     14        \u001b[36m0.0237\u001b[0m        0.0233  8.3363\n",
      "     15        0.0237        \u001b[32m0.0233\u001b[0m  8.3377\n",
      "     16        \u001b[36m0.0237\u001b[0m        0.0235  8.3638\n",
      "     17        0.0237        0.0234  8.3253\n",
      "     18        \u001b[36m0.0237\u001b[0m        0.0235  8.3457\n",
      "     19        \u001b[36m0.0237\u001b[0m        0.0234  8.3281\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of 0.0012652500200670547\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0238\u001b[0m  8.3257\n",
      "      2        \u001b[36m0.0238\u001b[0m        0.0239  8.3458\n",
      "      3        0.0238        0.0239  8.3619\n",
      "      4        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0238\u001b[0m  8.3547\n",
      "      5        \u001b[36m0.0237\u001b[0m        0.0240  8.3473\n",
      "      6        \u001b[36m0.0237\u001b[0m        0.0245  8.3504\n",
      "      7        0.0237        0.0239  8.3453\n",
      "      8        0.0237        0.0239  8.3676\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.0008167166513399771\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3753\n",
      "      2        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3525\n",
      "      3        \u001b[36m0.0236\u001b[0m        0.0238  8.3451\n",
      "      4        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3455\n",
      "      5        \u001b[36m0.0236\u001b[0m        0.0237  8.3407\n",
      "      6        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3598\n",
      "      7        \u001b[36m0.0236\u001b[0m        0.0236  8.3521\n",
      "      8        \u001b[36m0.0235\u001b[0m        0.0239  8.3453\n",
      "      9        0.0236        \u001b[32m0.0235\u001b[0m  8.0805\n",
      "     10        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0235\u001b[0m  8.0884\n",
      "     11        \u001b[36m0.0235\u001b[0m        0.0235  8.0943\n",
      "     12        0.0235        0.0235  8.1123\n",
      "     13        \u001b[36m0.0235\u001b[0m        0.0235  8.3390\n",
      "     14        0.0235        0.0235  8.3544\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.003263842727672994\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0321\u001b[0m        \u001b[32m0.0308\u001b[0m  8.3482\n",
      "      2        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0307\u001b[0m  8.3430\n",
      "      3        \u001b[36m0.0313\u001b[0m        0.0307  8.3533\n",
      "      4        \u001b[36m0.0312\u001b[0m        0.0312  8.4241\n",
      "      5        \u001b[36m0.0312\u001b[0m        0.0307  8.4173\n",
      "      6        \u001b[36m0.0312\u001b[0m        0.0319  8.4189\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.01124119202620566\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0316\u001b[0m        \u001b[32m0.0315\u001b[0m  8.3534\n",
      "      2        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0315\u001b[0m  8.3329\n",
      "      3        \u001b[36m0.0314\u001b[0m        0.0319  8.3209\n",
      "      4        \u001b[36m0.0313\u001b[0m        0.0315  8.3721\n",
      "      5        0.0314        0.0315  8.3648\n",
      "      6        \u001b[36m0.0313\u001b[0m        0.0315  8.3613\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.009879944953396613\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0321\u001b[0m        \u001b[32m0.0313\u001b[0m  8.3454\n",
      "      2        \u001b[36m0.0312\u001b[0m        \u001b[32m0.0312\u001b[0m  8.2709\n",
      "      3        \u001b[36m0.0311\u001b[0m        0.0313  8.2931\n",
      "      4        \u001b[36m0.0311\u001b[0m        \u001b[32m0.0312\u001b[0m  8.1993\n",
      "      5        \u001b[36m0.0311\u001b[0m        \u001b[32m0.0311\u001b[0m  8.1930\n",
      "      6        \u001b[36m0.0311\u001b[0m        0.0312  8.2584\n",
      "      7        0.0311        0.0312  8.2706\n",
      "      8        \u001b[36m0.0310\u001b[0m        0.0315  8.3125\n",
      "      9        0.0310        \u001b[32m0.0311\u001b[0m  8.3998\n",
      "     10        \u001b[36m0.0310\u001b[0m        0.0312  8.3920\n",
      "     11        0.0310        0.0314  8.2297\n",
      "     12        0.0310        0.0312  8.4235\n",
      "     13        \u001b[36m0.0310\u001b[0m        \u001b[32m0.0311\u001b[0m  8.4805\n",
      "     14        0.0310        0.0311  8.3676\n",
      "     15        0.0310        0.0313  8.3147\n",
      "     16        0.0310        0.0311  8.3312\n",
      "     17        0.0310        0.0312  8.3252\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0011072411239823055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0321\u001b[0m        \u001b[32m0.0317\u001b[0m  8.3397\n",
      "      2        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0315\u001b[0m  8.3617\n",
      "      3        \u001b[36m0.0313\u001b[0m        0.0315  8.3581\n",
      "      4        \u001b[36m0.0313\u001b[0m        0.0316  8.3289\n",
      "      5        \u001b[36m0.0312\u001b[0m        0.0316  8.3456\n",
      "      6        \u001b[36m0.0312\u001b[0m        \u001b[32m0.0315\u001b[0m  8.3649\n",
      "      7        0.0313        0.0320  8.3376\n",
      "      8        \u001b[36m0.0312\u001b[0m        0.0320  8.3301\n",
      "      9        0.0312        0.0315  8.3557\n",
      "     10        0.0312        0.0317  8.3189\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.009385997162341342\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0328\u001b[0m        \u001b[32m0.0313\u001b[0m  8.3798\n",
      "      2        \u001b[36m0.0315\u001b[0m        0.0320  8.3698\n",
      "      3        \u001b[36m0.0314\u001b[0m        0.0317  8.3390\n",
      "      4        \u001b[36m0.0314\u001b[0m        0.0320  8.3398\n",
      "      5        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0312\u001b[0m  8.3795\n",
      "      6        \u001b[36m0.0314\u001b[0m        0.0312  8.3645\n",
      "      7        \u001b[36m0.0313\u001b[0m        0.0318  8.3503\n",
      "      8        0.0314        0.0312  8.3389\n",
      "      9        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0312\u001b[0m  8.3402\n",
      "     10        0.0313        \u001b[32m0.0312\u001b[0m  8.3367\n",
      "     11        0.0313        0.0314  8.3622\n",
      "     12        \u001b[36m0.0313\u001b[0m        0.0312  8.3659\n",
      "     13        0.0313        0.0312  8.3679\n",
      "     14        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0311\u001b[0m  8.3729\n",
      "     15        0.0313        0.0313  8.3257\n",
      "     16        0.0313        0.0314  8.3579\n",
      "     17        \u001b[36m0.0312\u001b[0m        0.0312  8.4053\n",
      "     18        0.0313        0.0313  8.3590\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.009671230287766175\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0322\u001b[0m        \u001b[32m0.0313\u001b[0m  8.3559\n",
      "      2        \u001b[36m0.0313\u001b[0m        0.0315  8.3602\n",
      "      3        \u001b[36m0.0313\u001b[0m        0.0314  8.3607\n",
      "      4        \u001b[36m0.0312\u001b[0m        0.0319  8.3476\n",
      "      5        \u001b[36m0.0312\u001b[0m        0.0315  8.3481\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.006834660142533977\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0320\u001b[0m        \u001b[32m0.0320\u001b[0m  8.3673\n",
      "      2        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0310\u001b[0m  8.3327\n",
      "      3        \u001b[36m0.0314\u001b[0m        0.0310  8.3406\n",
      "      4        \u001b[36m0.0314\u001b[0m        0.0310  8.3167\n",
      "      5        \u001b[36m0.0313\u001b[0m        0.0311  8.3303\n",
      "      6        0.0313        0.0310  8.3513\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.007214401369723289\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0320\u001b[0m        \u001b[32m0.0321\u001b[0m  8.3439\n",
      "      2        \u001b[36m0.0311\u001b[0m        \u001b[32m0.0315\u001b[0m  8.3702\n",
      "      3        \u001b[36m0.0310\u001b[0m        0.0316  8.3586\n",
      "      4        \u001b[36m0.0310\u001b[0m        0.0316  8.3566\n",
      "      5        0.0310        \u001b[32m0.0314\u001b[0m  8.3629\n",
      "      6        \u001b[36m0.0310\u001b[0m        0.0317  8.3487\n",
      "      7        0.0310        0.0314  8.3182\n",
      "      8        \u001b[36m0.0310\u001b[0m        0.0316  8.3506\n",
      "      9        \u001b[36m0.0310\u001b[0m        0.0314  8.3112\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.006526293740005462\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0316\u001b[0m        \u001b[32m0.0313\u001b[0m  8.3661\n",
      "      2        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0310\u001b[0m  8.3406\n",
      "      3        \u001b[36m0.0314\u001b[0m        0.0311  8.3350\n",
      "      4        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0310\u001b[0m  8.3491\n",
      "      5        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0309\u001b[0m  8.3337\n",
      "      6        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0309\u001b[0m  8.3700\n",
      "      7        \u001b[36m0.0313\u001b[0m        0.0312  8.3256\n",
      "      8        \u001b[36m0.0312\u001b[0m        0.0313  8.3403\n",
      "      9        0.0312        0.0311  8.3187\n",
      "     10        0.0312        0.0312  8.3177\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of 0.0017241532466542031\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0325\u001b[0m        \u001b[32m0.0325\u001b[0m  8.3589\n",
      "      2        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0311\u001b[0m  8.3360\n",
      "      3        \u001b[36m0.0314\u001b[0m        0.0312  8.3177\n",
      "      4        0.0314        0.0311  8.3161\n",
      "      5        0.0314        \u001b[32m0.0311\u001b[0m  8.3379\n",
      "      6        0.0314        0.0312  8.3578\n",
      "      7        \u001b[36m0.0314\u001b[0m        0.0314  8.3311\n",
      "      8        0.0314        0.0312  8.3591\n",
      "      9        \u001b[36m0.0314\u001b[0m        \u001b[32m0.0311\u001b[0m  8.3503\n",
      "     10        0.0314        \u001b[32m0.0311\u001b[0m  8.4440\n",
      "     11        0.0314        0.0313  8.4303\n",
      "     12        \u001b[36m0.0313\u001b[0m        0.0311  8.4498\n",
      "     13        0.0314        \u001b[32m0.0311\u001b[0m  8.3425\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.003827162774854509\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0300\u001b[0m  8.4012\n",
      "      2        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0297\u001b[0m  8.3373\n",
      "      3        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0296\u001b[0m  8.4603\n",
      "      4        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3278\n",
      "      5        \u001b[36m0.0298\u001b[0m        0.0296  8.3523\n",
      "      6        \u001b[36m0.0298\u001b[0m        0.0296  8.3382\n",
      "      7        0.0298        0.0296  8.3225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        0.0298        0.0297  8.3579\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.020083403149312007\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3747\n",
      "      2        \u001b[36m0.0298\u001b[0m        0.0296  8.3534\n",
      "      3        \u001b[36m0.0298\u001b[0m        0.0295  8.3725\n",
      "      4        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3306\n",
      "      5        0.0297        \u001b[32m0.0295\u001b[0m  8.3403\n",
      "      6        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3473\n",
      "      7        \u001b[36m0.0297\u001b[0m        0.0296  8.3448\n",
      "      8        0.0297        0.0296  8.3436\n",
      "      9        \u001b[36m0.0297\u001b[0m        0.0295  8.3354\n",
      "     10        0.0297        \u001b[32m0.0295\u001b[0m  8.3281\n",
      "     11        \u001b[36m0.0297\u001b[0m        0.0295  8.3267\n",
      "     12        0.0297        0.0295  8.3611\n",
      "     13        \u001b[36m0.0297\u001b[0m        0.0295  8.3599\n",
      "     14        \u001b[36m0.0297\u001b[0m        0.0296  8.3338\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.000931833658757153\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0303\u001b[0m        \u001b[32m0.0299\u001b[0m  8.3448\n",
      "      2        \u001b[36m0.0300\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3506\n",
      "      3        \u001b[36m0.0299\u001b[0m        0.0298  8.3642\n",
      "      4        \u001b[36m0.0299\u001b[0m        0.0301  8.3296\n",
      "      5        0.0299        0.0298  8.3454\n",
      "      6        \u001b[36m0.0298\u001b[0m        0.0298  8.2990\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.009502525953248186\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0308\u001b[0m        \u001b[32m0.0296\u001b[0m  8.3653\n",
      "      2        \u001b[36m0.0299\u001b[0m        0.0296  8.3358\n",
      "      3        \u001b[36m0.0299\u001b[0m        0.0296  8.3360\n",
      "      4        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3453\n",
      "      5        \u001b[36m0.0298\u001b[0m        0.0297  8.3697\n",
      "      6        0.0298        0.0295  8.3431\n",
      "      7        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3658\n",
      "      8        \u001b[36m0.0298\u001b[0m        0.0297  8.3448\n",
      "      9        0.0298        0.0296  8.3496\n",
      "     10        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3346\n",
      "     11        0.0298        0.0295  8.3478\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -2.2998938687956993e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0306\u001b[0m        \u001b[32m0.0302\u001b[0m  8.3502\n",
      "      2        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0302\u001b[0m  8.3752\n",
      "      3        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0298\u001b[0m  8.3672\n",
      "      4        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0297\u001b[0m  8.3435\n",
      "      5        \u001b[36m0.0297\u001b[0m        0.0298  8.3510\n",
      "      6        0.0297        0.0302  8.3652\n",
      "      7        \u001b[36m0.0297\u001b[0m        0.0298  8.3736\n",
      "      8        0.0297        0.0299  8.3768\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.0049670344450269965\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0313\u001b[0m        \u001b[32m0.0299\u001b[0m  8.3476\n",
      "      2        \u001b[36m0.0297\u001b[0m        0.0299  8.3501\n",
      "      3        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0298\u001b[0m  8.3063\n",
      "      4        0.0297        0.0299  8.3181\n",
      "      5        \u001b[36m0.0296\u001b[0m        0.0298  8.3229\n",
      "      6        \u001b[36m0.0296\u001b[0m        0.0300  8.3233\n",
      "      7        0.0296        0.0298  8.2628\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of 3.4196828698762616e-06\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0308\u001b[0m        \u001b[32m0.0306\u001b[0m  8.2036\n",
      "      2        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0303\u001b[0m  8.1737\n",
      "      3        0.0297        0.0304  8.1792\n",
      "      4        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0303\u001b[0m  8.3208\n",
      "      5        0.0297        0.0303  8.3109\n",
      "      6        \u001b[36m0.0296\u001b[0m        \u001b[32m0.0303\u001b[0m  8.3264\n",
      "      7        0.0297        0.0303  8.3241\n",
      "      8        \u001b[36m0.0296\u001b[0m        0.0303  8.3365\n",
      "      9        0.0297        0.0304  8.3429\n",
      "     10        0.0296        0.0304  8.3336\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.00299603850309893\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0312\u001b[0m        \u001b[32m0.0308\u001b[0m  8.3102\n",
      "      2        \u001b[36m0.0300\u001b[0m        \u001b[32m0.0296\u001b[0m  8.3073\n",
      "      3        0.0300        0.0298  8.3617\n",
      "      4        \u001b[36m0.0298\u001b[0m        0.0297  8.3134\n",
      "      5        0.0298        \u001b[32m0.0296\u001b[0m  8.3356\n",
      "      6        0.0299        0.0296  8.3022\n",
      "      7        \u001b[36m0.0298\u001b[0m        0.0296  8.3452\n",
      "      8        \u001b[36m0.0298\u001b[0m        0.0297  8.3434\n",
      "      9        \u001b[36m0.0298\u001b[0m        0.0296  8.3388\n",
      "     10        \u001b[36m0.0298\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3506\n",
      "     11        0.0298        0.0297  8.3521\n",
      "     12        \u001b[36m0.0298\u001b[0m        0.0296  8.3280\n",
      "     13        \u001b[36m0.0297\u001b[0m        0.0300  8.3334\n",
      "     14        0.0298        0.0297  8.3557\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.013388353998555091\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0306\u001b[0m        \u001b[32m0.0293\u001b[0m  8.3455\n",
      "      2        \u001b[36m0.0299\u001b[0m        0.0294  8.3481\n",
      "      3        \u001b[36m0.0299\u001b[0m        0.0297  8.3449\n",
      "      4        \u001b[36m0.0298\u001b[0m        0.0296  8.3380\n",
      "      5        \u001b[36m0.0298\u001b[0m        0.0294  8.3078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.0006884513739746634\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0316\u001b[0m        \u001b[32m0.0297\u001b[0m  8.3675\n",
      "      2        \u001b[36m0.0299\u001b[0m        0.0302  8.3608\n",
      "      3        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3451\n",
      "      4        \u001b[36m0.0298\u001b[0m        0.0296  8.3272\n",
      "      5        0.0298        \u001b[32m0.0295\u001b[0m  8.3636\n",
      "      6        \u001b[36m0.0298\u001b[0m        0.0302  8.3484\n",
      "      7        \u001b[36m0.0298\u001b[0m        0.0297  8.3607\n",
      "      8        \u001b[36m0.0297\u001b[0m        0.0296  8.3540\n",
      "      9        \u001b[36m0.0297\u001b[0m        0.0301  8.3558\n",
      "     10        \u001b[36m0.0297\u001b[0m        \u001b[32m0.0295\u001b[0m  8.3598\n",
      "     11        \u001b[36m0.0297\u001b[0m        0.0297  8.3292\n",
      "     12        0.0297        0.0297  8.3785\n",
      "     13        \u001b[36m0.0297\u001b[0m        0.0298  8.3607\n",
      "     14        0.0297        \u001b[32m0.0295\u001b[0m  8.3313\n",
      "     15        0.0297        0.0297  8.3519\n",
      "     16        0.0297        0.0297  8.3558\n",
      "     17        0.0297        0.0295  8.3626\n",
      "     18        0.0297        0.0295  8.3621\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.003310434357879144\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0246\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3408\n",
      "      2        \u001b[36m0.0238\u001b[0m        0.0235  8.3832\n",
      "      3        0.0238        0.0242  8.3763\n",
      "      4        0.0238        0.0235  8.3668\n",
      "      5        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3710\n",
      "      6        \u001b[36m0.0238\u001b[0m        0.0233  8.3402\n",
      "      7        0.0238        \u001b[32m0.0232\u001b[0m  8.3278\n",
      "      8        \u001b[36m0.0238\u001b[0m        0.0239  8.3357\n",
      "      9        0.0238        0.0236  8.0579\n",
      "     10        0.0238        0.0232  8.0747\n",
      "     11        \u001b[36m0.0237\u001b[0m        0.0233  8.1202\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.01955513588633706\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0240\u001b[0m  7.9960\n",
      "      2        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0239\u001b[0m  8.1720\n",
      "      3        \u001b[36m0.0235\u001b[0m        0.0239  8.3706\n",
      "      4        \u001b[36m0.0235\u001b[0m        0.0240  8.3542\n",
      "      5        \u001b[36m0.0235\u001b[0m        0.0239  8.3364\n",
      "      6        0.0235        0.0239  8.3352\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.003696001039607877\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3504\n",
      "      2        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3606\n",
      "      3        \u001b[36m0.0236\u001b[0m        0.0233  8.3536\n",
      "      4        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3237\n",
      "      5        0.0236        0.0236  8.3595\n",
      "      6        \u001b[36m0.0236\u001b[0m        0.0233  8.3372\n",
      "      7        0.0236        0.0236  8.3556\n",
      "      8        \u001b[36m0.0236\u001b[0m        0.0233  8.2742\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0001417106405359192\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0232\u001b[0m  8.2258\n",
      "      2        \u001b[36m0.0238\u001b[0m        0.0233  8.2231\n",
      "      3        \u001b[36m0.0237\u001b[0m        0.0232  8.2855\n",
      "      4        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0232\u001b[0m  8.2230\n",
      "      5        0.0237        0.0232  8.2517\n",
      "      6        \u001b[36m0.0237\u001b[0m        0.0232  8.2622\n",
      "      7        0.0237        0.0232  8.2604\n",
      "      8        \u001b[36m0.0237\u001b[0m        0.0232  8.2437\n",
      "      9        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0231\u001b[0m  8.2619\n",
      "     10        \u001b[36m0.0236\u001b[0m        0.0232  8.3455\n",
      "     11        0.0236        0.0237  8.3342\n",
      "     12        \u001b[36m0.0236\u001b[0m        0.0233  8.3578\n",
      "     13        0.0237        0.0232  8.3656\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.004180826845852081\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0241\u001b[0m        \u001b[32m0.0237\u001b[0m  8.3308\n",
      "      2        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3471\n",
      "      3        \u001b[36m0.0237\u001b[0m        0.0237  8.3254\n",
      "      4        \u001b[36m0.0237\u001b[0m        0.0239  8.3509\n",
      "      5        0.0237        0.0236  8.3760\n",
      "      6        \u001b[36m0.0236\u001b[0m        0.0236  8.3403\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.024078639848720274\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0246\u001b[0m        \u001b[32m0.0244\u001b[0m  8.3127\n",
      "      2        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3758\n",
      "      3        0.0236        0.0235  8.3487\n",
      "      4        0.0236        \u001b[32m0.0235\u001b[0m  8.3597\n",
      "      5        \u001b[36m0.0236\u001b[0m        0.0236  8.3615\n",
      "      6        0.0236        0.0237  8.3847\n",
      "      7        0.0236        0.0236  8.3573\n",
      "      8        \u001b[36m0.0236\u001b[0m        0.0237  8.3402\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.016499339051609274\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3567\n",
      "      2        \u001b[36m0.0236\u001b[0m        0.0238  8.3495\n",
      "      3        0.0236        0.0237  8.3454\n",
      "      4        0.0236        \u001b[32m0.0236\u001b[0m  8.3563\n",
      "      5        \u001b[36m0.0236\u001b[0m        0.0240  8.3148\n",
      "      6        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3488\n",
      "      7        0.0236        \u001b[32m0.0236\u001b[0m  8.3534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        0.0236        0.0236  8.3473\n",
      "      9        0.0235        0.0238  8.3399\n",
      "     10        0.0235        0.0239  8.3621\n",
      "     11        0.0235        0.0236  8.3632\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.00014209892767014232\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0239\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3690\n",
      "      2        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3312\n",
      "      3        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3339\n",
      "      4        0.0236        0.0233  8.3481\n",
      "      5        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3677\n",
      "      6        \u001b[36m0.0236\u001b[0m        0.0233  8.3526\n",
      "      7        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3655\n",
      "      8        0.0236        \u001b[32m0.0233\u001b[0m  8.3505\n",
      "      9        0.0236        0.0233  8.3579\n",
      "     10        \u001b[36m0.0236\u001b[0m        0.0233  8.3643\n",
      "     11        \u001b[36m0.0236\u001b[0m        0.0233  8.3673\n",
      "     12        0.0236        0.0233  8.3444\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.004828654464515747\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0249\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3506\n",
      "      2        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0236\u001b[0m  8.3890\n",
      "      3        0.0235        \u001b[32m0.0236\u001b[0m  8.3683\n",
      "      4        \u001b[36m0.0235\u001b[0m        0.0236  8.3439\n",
      "      5        0.0235        0.0236  8.3805\n",
      "      6        0.0235        0.0236  8.3641\n",
      "      7        0.0235        0.0237  8.3093\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.01781258417736642\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0248\u001b[0m        \u001b[32m0.0237\u001b[0m  8.3558\n",
      "      2        \u001b[36m0.0235\u001b[0m        0.0240  8.3310\n",
      "      3        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3502\n",
      "      4        \u001b[36m0.0235\u001b[0m        0.0236  8.3418\n",
      "      5        0.0235        0.0237  8.3751\n",
      "      6        \u001b[36m0.0235\u001b[0m        0.0236  8.3441\n",
      "      7        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3422\n",
      "      8        \u001b[36m0.0235\u001b[0m        0.0237  8.3507\n",
      "      9        0.0235        0.0236  8.3418\n",
      "     10        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3596\n",
      "     11        \u001b[36m0.0234\u001b[0m        0.0237  8.3340\n",
      "     12        \u001b[36m0.0234\u001b[0m        0.0237  8.3600\n",
      "     13        \u001b[36m0.0234\u001b[0m        0.0235  8.3478\n",
      "     14        0.0234        0.0237  8.3595\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of 0.00035154360424727304\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0311\u001b[0m        \u001b[32m0.0299\u001b[0m  8.3522\n",
      "      2        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0297\u001b[0m  8.3421\n",
      "      3        0.0302        0.0297  8.3454\n",
      "      4        \u001b[36m0.0301\u001b[0m        0.0297  8.3442\n",
      "      5        \u001b[36m0.0301\u001b[0m        0.0297  8.3648\n",
      "      6        0.0301        0.0304  8.3665\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0009758446947623067\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0308\u001b[0m        \u001b[32m0.0319\u001b[0m  8.3619\n",
      "      2        \u001b[36m0.0303\u001b[0m        \u001b[32m0.0304\u001b[0m  8.3431\n",
      "      3        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0302\u001b[0m  8.3602\n",
      "      4        0.0302        0.0302  8.3762\n",
      "      5        \u001b[36m0.0302\u001b[0m        0.0302  8.4443\n",
      "      6        0.0302        0.0304  8.4351\n",
      "      7        \u001b[36m0.0301\u001b[0m        0.0302  8.4507\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.00437502120032951\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0338\u001b[0m        \u001b[32m0.0310\u001b[0m  8.4106\n",
      "      2        \u001b[36m0.0304\u001b[0m        \u001b[32m0.0297\u001b[0m  8.3753\n",
      "      3        \u001b[36m0.0304\u001b[0m        0.0303  8.4090\n",
      "      4        \u001b[36m0.0304\u001b[0m        \u001b[32m0.0297\u001b[0m  8.3243\n",
      "      5        \u001b[36m0.0304\u001b[0m        0.0298  8.3981\n",
      "      6        \u001b[36m0.0303\u001b[0m        0.0299  8.4169\n",
      "      7        0.0303        0.0300  8.4052\n",
      "      8        0.0303        0.0300  8.3747\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.009651695683688954\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0320\u001b[0m        \u001b[32m0.0304\u001b[0m  8.3445\n",
      "      2        \u001b[36m0.0301\u001b[0m        0.0304  8.4054\n",
      "      3        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0302\u001b[0m  8.3725\n",
      "      4        0.0301        0.0303  8.3729\n",
      "      5        \u001b[36m0.0300\u001b[0m        0.0303  8.3948\n",
      "      6        0.0300        0.0316  8.7664\n",
      "      7        \u001b[36m0.0300\u001b[0m        0.0306  8.5564\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.00012231002545215475\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0306\u001b[0m        \u001b[32m0.0302\u001b[0m  8.4231\n",
      "      2        \u001b[36m0.0301\u001b[0m        0.0306  8.3881\n",
      "      3        0.0301        \u001b[32m0.0302\u001b[0m  8.3584\n",
      "      4        0.0302        0.0303  8.3502\n",
      "      5        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0301\u001b[0m  8.3258\n",
      "      6        0.0301        0.0304  8.3146\n",
      "      7        \u001b[36m0.0301\u001b[0m        0.0302  8.3488\n",
      "      8        0.0301        0.0307  8.3124\n",
      "      9        0.0301        0.0303  8.3237\n",
      "     10        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0301\u001b[0m  8.3060\n",
      "     11        0.0301        0.0302  8.3221\n",
      "     12        \u001b[36m0.0300\u001b[0m        0.0302  8.3103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13        0.0301        0.0302  8.3561\n",
      "     14        0.0301        0.0302  8.3250\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.012898177604090622\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0305\u001b[0m        \u001b[32m0.0302\u001b[0m  8.3381\n",
      "      2        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0300\u001b[0m  8.3454\n",
      "      3        0.0302        \u001b[32m0.0299\u001b[0m  8.3630\n",
      "      4        \u001b[36m0.0302\u001b[0m        \u001b[32m0.0299\u001b[0m  8.3371\n",
      "      5        \u001b[36m0.0302\u001b[0m        0.0300  8.3450\n",
      "      6        \u001b[36m0.0301\u001b[0m        0.0305  8.3586\n",
      "      7        0.0302        0.0299  8.3715\n",
      "      8        0.0302        0.0301  8.3319\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0011261043872203835\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0305\u001b[0m        \u001b[32m0.0305\u001b[0m  8.3249\n",
      "      2        \u001b[36m0.0301\u001b[0m        \u001b[32m0.0304\u001b[0m  8.3198\n",
      "      3        \u001b[36m0.0300\u001b[0m        0.0305  8.3437\n",
      "      4        0.0301        0.0307  8.3280\n",
      "      5        \u001b[36m0.0300\u001b[0m        \u001b[32m0.0303\u001b[0m  8.1797\n",
      "      6        0.0300        \u001b[32m0.0303\u001b[0m  8.3612\n",
      "      7        0.0301        \u001b[32m0.0303\u001b[0m  8.3331\n",
      "      8        0.0301        \u001b[32m0.0303\u001b[0m  8.3733\n",
      "      9        0.0300        0.0303  7.9459\n",
      "     10        \u001b[36m0.0300\u001b[0m        0.0306  8.3188\n",
      "     11        \u001b[36m0.0300\u001b[0m        0.0303  8.3317\n",
      "     12        \u001b[36m0.0300\u001b[0m        0.0303  8.3402\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0001678835807428225\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0320\u001b[0m        \u001b[32m0.0299\u001b[0m  8.3265\n",
      "      2        \u001b[36m0.0304\u001b[0m        0.0300  8.3367\n",
      "      3        \u001b[36m0.0304\u001b[0m        0.0300  8.4169\n",
      "      4        \u001b[36m0.0303\u001b[0m        0.0301  8.3263\n",
      "      5        \u001b[36m0.0303\u001b[0m        0.0300  8.3592\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.01347676698953193\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0310\u001b[0m        \u001b[32m0.0300\u001b[0m  8.3220\n",
      "      2        \u001b[36m0.0301\u001b[0m        0.0300  8.3381\n",
      "      3        \u001b[36m0.0301\u001b[0m        0.0300  8.3714\n",
      "      4        0.0301        0.0300  8.3637\n",
      "      5        \u001b[36m0.0300\u001b[0m        0.0301  8.3512\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.00023323319576640245\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0312\u001b[0m        \u001b[32m0.0302\u001b[0m  8.3523\n",
      "      2        \u001b[36m0.0304\u001b[0m        0.0325  8.3806\n",
      "      3        \u001b[36m0.0303\u001b[0m        \u001b[32m0.0301\u001b[0m  8.3599\n",
      "      4        \u001b[36m0.0302\u001b[0m        0.0302  8.3433\n",
      "      5        \u001b[36m0.0302\u001b[0m        0.0301  8.3502\n",
      "      6        \u001b[36m0.0302\u001b[0m        0.0307  8.3656\n",
      "      7        \u001b[36m0.0301\u001b[0m        0.0307  8.3385\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.0002961538484687054\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0240\u001b[0m        \u001b[32m0.0228\u001b[0m  8.3502\n",
      "      2        \u001b[36m0.0236\u001b[0m        0.0232  8.3658\n",
      "      3        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0228\u001b[0m  8.3681\n",
      "      4        \u001b[36m0.0235\u001b[0m        0.0230  8.3710\n",
      "      5        0.0235        \u001b[32m0.0228\u001b[0m  8.3334\n",
      "      6        \u001b[36m0.0234\u001b[0m        0.0231  8.2986\n",
      "      7        \u001b[36m0.0234\u001b[0m        0.0229  8.3716\n",
      "      8        0.0234        0.0231  8.3646\n",
      "      9        \u001b[36m0.0234\u001b[0m        0.0234  8.3471\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0019950857894763807\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0247\u001b[0m        \u001b[32m0.0252\u001b[0m  8.3351\n",
      "      2        \u001b[36m0.0236\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3252\n",
      "      3        \u001b[36m0.0235\u001b[0m        0.0235  8.3514\n",
      "      4        \u001b[36m0.0235\u001b[0m        0.0235  8.3416\n",
      "      5        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3537\n",
      "      6        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3727\n",
      "      7        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3450\n",
      "      8        0.0234        0.0234  8.3314\n",
      "      9        \u001b[36m0.0233\u001b[0m        0.0237  8.3462\n",
      "     10        0.0233        0.0234  8.3496\n",
      "     11        0.0233        0.0234  8.3390\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.002632900492101964\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3434\n",
      "      2        \u001b[36m0.0236\u001b[0m        0.0246  8.3660\n",
      "      3        \u001b[36m0.0236\u001b[0m        0.0235  8.3298\n",
      "      4        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3629\n",
      "      5        0.0235        0.0240  8.3513\n",
      "      6        \u001b[36m0.0235\u001b[0m        0.0236  8.3320\n",
      "      7        \u001b[36m0.0235\u001b[0m        0.0234  8.3320\n",
      "      8        \u001b[36m0.0234\u001b[0m        0.0235  8.3464\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.00018299113313613447\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0241\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3495\n",
      "      2        \u001b[36m0.0234\u001b[0m        0.0235  8.3777\n",
      "      3        0.0234        \u001b[32m0.0233\u001b[0m  8.3602\n",
      "      4        \u001b[36m0.0234\u001b[0m        0.0234  8.3660\n",
      "      5        0.0234        0.0234  8.3337\n",
      "      6        0.0234        0.0233  8.3599\n",
      "      7        0.0234        0.0234  8.3602\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -7.166065729613535e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0235\u001b[0m  8.3411\n",
      "      2        \u001b[36m0.0233\u001b[0m        \u001b[32m0.0232\u001b[0m  8.3482\n",
      "      3        \u001b[36m0.0233\u001b[0m        \u001b[32m0.0230\u001b[0m  8.3671\n",
      "      4        0.0233        \u001b[32m0.0230\u001b[0m  8.3766\n",
      "      5        \u001b[36m0.0233\u001b[0m        0.0232  8.3466\n",
      "      6        \u001b[36m0.0233\u001b[0m        0.0231  8.3518\n",
      "      7        \u001b[36m0.0233\u001b[0m        0.0231  8.3607\n",
      "      8        \u001b[36m0.0233\u001b[0m        0.0232  8.3607\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.006070649391714644\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0239\u001b[0m        \u001b[32m0.0234\u001b[0m  8.3540\n",
      "      2        \u001b[36m0.0234\u001b[0m        0.0236  8.3334\n",
      "      3        \u001b[36m0.0233\u001b[0m        0.0234  8.3071\n",
      "      4        0.0233        \u001b[32m0.0234\u001b[0m  8.3572\n",
      "      5        0.0234        0.0235  8.3543\n",
      "      6        \u001b[36m0.0233\u001b[0m        0.0234  8.3332\n",
      "      7        0.0233        0.0234  8.3588\n",
      "      8        \u001b[36m0.0233\u001b[0m        0.0234  8.3272\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.013048683489432555\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0245\u001b[0m        \u001b[32m0.0232\u001b[0m  8.3268\n",
      "      2        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0231\u001b[0m  8.3416\n",
      "      3        \u001b[36m0.0235\u001b[0m        0.0232  8.3476\n",
      "      4        \u001b[36m0.0235\u001b[0m        0.0232  8.3216\n",
      "      5        0.0235        0.0236  8.3609\n",
      "      6        0.0235        0.0232  8.3444\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.008089435709372372\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0252\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3514\n",
      "      2        \u001b[36m0.0235\u001b[0m        0.0235  8.3482\n",
      "      3        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3495\n",
      "      4        0.0234        \u001b[32m0.0233\u001b[0m  8.3568\n",
      "      5        0.0234        0.0234  8.3504\n",
      "      6        \u001b[36m0.0234\u001b[0m        0.0236  8.3634\n",
      "      7        0.0234        0.0235  8.3500\n",
      "      8        0.0234        0.0234  8.3372\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.006939379507794996\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0242\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3538\n",
      "      2        \u001b[36m0.0234\u001b[0m        0.0234  8.3271\n",
      "      3        \u001b[36m0.0233\u001b[0m        0.0234  8.3333\n",
      "      4        \u001b[36m0.0233\u001b[0m        \u001b[32m0.0233\u001b[0m  8.3363\n",
      "      5        0.0233        0.0233  8.3383\n",
      "      6        0.0234        \u001b[32m0.0233\u001b[0m  8.3494\n",
      "      7        0.0233        0.0234  8.3502\n",
      "      8        \u001b[36m0.0233\u001b[0m        0.0234  8.3431\n",
      "      9        0.0233        0.0234  8.3606\n",
      "     10        0.0233        0.0239  8.3541\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.005617470538940905\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0255\u001b[0m        \u001b[32m0.0239\u001b[0m  8.3495\n",
      "      2        \u001b[36m0.0235\u001b[0m        0.0241  8.3818\n",
      "      3        0.0235        \u001b[32m0.0234\u001b[0m  8.3519\n",
      "      4        \u001b[36m0.0234\u001b[0m        0.0239  8.3419\n",
      "      5        0.0234        0.0240  8.3580\n",
      "      6        0.0234        \u001b[32m0.0234\u001b[0m  8.3525\n",
      "      7        \u001b[36m0.0234\u001b[0m        0.0234  8.3231\n",
      "      8        0.0234        0.0239  8.3715\n",
      "      9        \u001b[36m0.0234\u001b[0m        0.0234  8.3765\n",
      "     10        \u001b[36m0.0234\u001b[0m        0.0237  8.3635\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.00876593187603758\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0385\u001b[0m        \u001b[32m0.0377\u001b[0m  8.3289\n",
      "      2        \u001b[36m0.0370\u001b[0m        \u001b[32m0.0370\u001b[0m  8.3404\n",
      "      3        \u001b[36m0.0370\u001b[0m        0.0378  8.3724\n",
      "      4        \u001b[36m0.0370\u001b[0m        \u001b[32m0.0370\u001b[0m  8.3563\n",
      "      5        \u001b[36m0.0369\u001b[0m        0.0371  8.3420\n",
      "      6        \u001b[36m0.0369\u001b[0m        0.0371  8.3675\n",
      "      7        \u001b[36m0.0369\u001b[0m        0.0372  8.3873\n",
      "      8        \u001b[36m0.0368\u001b[0m        0.0374  8.3697\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.005542676160197546\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0378\u001b[0m        \u001b[32m0.0367\u001b[0m  8.3495\n",
      "      2        \u001b[36m0.0371\u001b[0m        \u001b[32m0.0366\u001b[0m  8.3591\n",
      "      3        \u001b[36m0.0370\u001b[0m        0.0366  8.3252\n",
      "      4        0.0371        0.0367  8.3345\n",
      "      5        0.0371        0.0367  8.3486\n",
      "      6        \u001b[36m0.0370\u001b[0m        0.0366  8.3541\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.001143148729658705\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0379\u001b[0m        \u001b[32m0.0375\u001b[0m  8.3354\n",
      "      2        \u001b[36m0.0369\u001b[0m        0.0378  8.3335\n",
      "      3        0.0369        0.0375  8.3687\n",
      "      4        0.0369        0.0378  8.3493\n",
      "      5        0.0369        0.0376  8.3567\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.00038037083185749054\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0375\u001b[0m        \u001b[32m0.0364\u001b[0m  8.2686\n",
      "      2        \u001b[36m0.0373\u001b[0m        \u001b[32m0.0364\u001b[0m  8.3394\n",
      "      3        \u001b[36m0.0371\u001b[0m        0.0366  8.3573\n",
      "      4        \u001b[36m0.0371\u001b[0m        0.0372  8.3496\n",
      "      5        \u001b[36m0.0371\u001b[0m        0.0365  8.3728\n",
      "      6        \u001b[36m0.0370\u001b[0m        0.0364  8.3705\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.005833265181545366\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0377\u001b[0m        \u001b[32m0.0374\u001b[0m  8.3755\n",
      "      2        \u001b[36m0.0373\u001b[0m        \u001b[32m0.0373\u001b[0m  8.3666\n",
      "      3        \u001b[36m0.0372\u001b[0m        \u001b[32m0.0370\u001b[0m  8.3751\n",
      "      4        \u001b[36m0.0371\u001b[0m        \u001b[32m0.0370\u001b[0m  8.3598\n",
      "      5        \u001b[36m0.0371\u001b[0m        0.0381  8.3763\n",
      "      6        \u001b[36m0.0371\u001b[0m        0.0376  8.3641\n",
      "      7        0.0371        0.0377  8.3516\n",
      "      8        0.0371        0.0371  8.3568\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.0002454793262889332\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0380\u001b[0m        \u001b[32m0.0375\u001b[0m  8.3534\n",
      "      2        \u001b[36m0.0370\u001b[0m        \u001b[32m0.0370\u001b[0m  8.3602\n",
      "      3        \u001b[36m0.0370\u001b[0m        0.0371  8.3752\n",
      "      4        \u001b[36m0.0370\u001b[0m        0.0370  8.3780\n",
      "      5        0.0370        0.0371  8.3464\n",
      "      6        \u001b[36m0.0370\u001b[0m        0.0371  8.3609\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.009018377417842949\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0379\u001b[0m        \u001b[32m0.0362\u001b[0m  8.3447\n",
      "      2        \u001b[36m0.0372\u001b[0m        0.0377  8.3384\n",
      "      3        0.0372        \u001b[32m0.0362\u001b[0m  8.3530\n",
      "      4        \u001b[36m0.0372\u001b[0m        0.0362  8.3783\n",
      "      5        \u001b[36m0.0371\u001b[0m        0.0363  8.3777\n",
      "      6        \u001b[36m0.0371\u001b[0m        0.0362  8.3869\n",
      "      7        0.0371        0.0368  8.3636\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0005484259618544485\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0380\u001b[0m        \u001b[32m0.0367\u001b[0m  8.3306\n",
      "      2        \u001b[36m0.0371\u001b[0m        \u001b[32m0.0367\u001b[0m  8.2850\n",
      "      3        \u001b[36m0.0370\u001b[0m        0.0368  7.6963\n",
      "      4        0.0370        0.0369  8.3246\n",
      "      5        \u001b[36m0.0370\u001b[0m        \u001b[32m0.0367\u001b[0m  8.5877\n",
      "      6        0.0370        0.0369  8.4392\n",
      "      7        0.0370        0.0368  8.4530\n",
      "      8        0.0370        0.0367  8.3597\n",
      "      9        \u001b[36m0.0369\u001b[0m        0.0367  8.3593\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.0018231264855737006\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0377\u001b[0m        \u001b[32m0.0374\u001b[0m  8.3317\n",
      "      2        \u001b[36m0.0370\u001b[0m        \u001b[32m0.0371\u001b[0m  8.3631\n",
      "      3        0.0370        \u001b[32m0.0370\u001b[0m  8.3069\n",
      "      4        \u001b[36m0.0369\u001b[0m        0.0373  8.3771\n",
      "      5        \u001b[36m0.0369\u001b[0m        0.0372  8.3296\n",
      "      6        \u001b[36m0.0369\u001b[0m        0.0377  8.3403\n",
      "      7        0.0369        0.0371  8.3618\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.005725971154995957\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0380\u001b[0m        \u001b[32m0.0368\u001b[0m  8.3498\n",
      "      2        \u001b[36m0.0371\u001b[0m        \u001b[32m0.0368\u001b[0m  8.3448\n",
      "      3        0.0372        \u001b[32m0.0367\u001b[0m  8.3182\n",
      "      4        \u001b[36m0.0371\u001b[0m        \u001b[32m0.0367\u001b[0m  8.3335\n",
      "      5        \u001b[36m0.0371\u001b[0m        \u001b[32m0.0367\u001b[0m  8.3741\n",
      "      6        \u001b[36m0.0371\u001b[0m        0.0368  8.3385\n",
      "      7        \u001b[36m0.0371\u001b[0m        0.0368  8.3334\n",
      "      8        \u001b[36m0.0371\u001b[0m        0.0367  8.3166\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.0015566952082053387\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0844\u001b[0m  8.3048\n",
      "      2        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3235\n",
      "      3        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3153\n",
      "      4        \u001b[36m0.0836\u001b[0m        0.0835  8.3312\n",
      "      5        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3267\n",
      "      6        0.0835        0.0832  8.3382\n",
      "      7        0.0834        0.0838  8.3402\n",
      "      8        0.0834        0.0831  8.3393\n",
      "      9        \u001b[36m0.0834\u001b[0m        0.0843  8.3354\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.01547269877601165\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0850\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3305\n",
      "      3        \u001b[36m0.0835\u001b[0m        0.0836  8.3192\n",
      "      4        \u001b[36m0.0834\u001b[0m        0.0849  8.3045\n",
      "      5        0.0834        0.0835  8.3187\n",
      "      6        0.0834        0.0839  8.3350\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.005760495072626215\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0847\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3347\n",
      "      2        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3313\n",
      "      3        \u001b[36m0.0835\u001b[0m        0.0848  8.3192\n",
      "      4        \u001b[36m0.0833\u001b[0m        0.0851  8.3250\n",
      "      5        \u001b[36m0.0833\u001b[0m        0.0840  8.3114\n",
      "      6        0.0834        0.0843  8.3639\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.004384479327333324\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0844\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3394\n",
      "      2        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3666\n",
      "      3        \u001b[36m0.0836\u001b[0m        0.0836  8.3389\n",
      "      4        \u001b[36m0.0835\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3929\n",
      "      5        \u001b[36m0.0835\u001b[0m        0.0837  8.3391\n",
      "      6        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3252\n",
      "      7        \u001b[36m0.0834\u001b[0m        0.0836  8.3251\n",
      "      8        0.0834        0.0836  8.3579\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.0012906344737608766\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0846\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3350\n",
      "      2        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0828\u001b[0m  8.3383\n",
      "      3        \u001b[36m0.0835\u001b[0m        \u001b[32m0.0827\u001b[0m  8.3247\n",
      "      4        0.0835        0.0828  8.3358\n",
      "      5        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0827\u001b[0m  8.3158\n",
      "      6        \u001b[36m0.0834\u001b[0m        0.0827  8.2919\n",
      "      7        \u001b[36m0.0834\u001b[0m        0.0828  8.3135\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.00028050564107506837\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0844\u001b[0m  8.3517\n",
      "      2        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0838\u001b[0m  8.3114\n",
      "      3        \u001b[36m0.0833\u001b[0m        0.0840  8.3383\n",
      "      4        0.0833        \u001b[32m0.0838\u001b[0m  8.3243\n",
      "      5        \u001b[36m0.0833\u001b[0m        0.0838  8.3149\n",
      "      6        0.0833        0.0839  8.3265\n",
      "      7        0.0833        0.0839  8.3076\n",
      "      8        \u001b[36m0.0832\u001b[0m        \u001b[32m0.0838\u001b[0m  8.3150\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.015018717262168968\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0847\u001b[0m        \u001b[32m0.0823\u001b[0m  8.3484\n",
      "      2        \u001b[36m0.0839\u001b[0m        0.0835  8.3606\n",
      "      3        \u001b[36m0.0839\u001b[0m        0.0826  8.3423\n",
      "      4        \u001b[36m0.0838\u001b[0m        0.0827  8.3529\n",
      "      5        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0823\u001b[0m  8.3325\n",
      "      6        0.0838        0.0825  8.3407\n",
      "      7        \u001b[36m0.0837\u001b[0m        0.0824  8.3545\n",
      "      8        \u001b[36m0.0837\u001b[0m        0.0831  8.3202\n",
      "      9        \u001b[36m0.0837\u001b[0m        0.0823  8.3549\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0001657954613736301\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0844\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3472\n",
      "      2        \u001b[36m0.0833\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3389\n",
      "      3        \u001b[36m0.0833\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3235\n",
      "      4        \u001b[36m0.0833\u001b[0m        0.0842  8.3363\n",
      "      5        \u001b[36m0.0832\u001b[0m        0.0841  8.3408\n",
      "      6        \u001b[36m0.0831\u001b[0m        0.0846  8.3143\n",
      "      7        0.0832        \u001b[32m0.0836\u001b[0m  8.3274\n",
      "      8        \u001b[36m0.0831\u001b[0m        0.0837  8.3266\n",
      "      9        \u001b[36m0.0831\u001b[0m        0.0844  8.3301\n",
      "     10        0.0831        0.0836  8.3225\n",
      "     11        0.0831        0.0836  8.3181\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.0031935834939396113\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0846\u001b[0m  8.3544\n",
      "      2        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0846\u001b[0m  8.3376\n",
      "      3        \u001b[36m0.0836\u001b[0m        0.0847  8.3511\n",
      "      4        0.0836        \u001b[32m0.0845\u001b[0m  8.3464\n",
      "      5        \u001b[36m0.0834\u001b[0m        0.0846  8.3249\n",
      "      6        0.0835        0.0845  8.3390\n",
      "      7        0.0834        0.0846  8.3311\n",
      "      8        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0845\u001b[0m  8.3223\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.010007085089582146\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0849\u001b[0m  8.3797\n",
      "      2        \u001b[36m0.0835\u001b[0m        \u001b[32m0.0836\u001b[0m  8.7047\n",
      "      3        \u001b[36m0.0835\u001b[0m        0.0852  8.5792\n",
      "      4        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3648\n",
      "      5        \u001b[36m0.0833\u001b[0m        0.0833  8.3534\n",
      "      6        \u001b[36m0.0833\u001b[0m        0.0835  8.4418\n",
      "      7        0.0833        0.0838  8.3367\n",
      "      8        0.0833        0.0837  8.4271\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.001979914232759228\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0845\u001b[0m        \u001b[32m0.0841\u001b[0m  8.4404\n",
      "      2        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0838\u001b[0m  8.4985\n",
      "      3        \u001b[36m0.0836\u001b[0m        0.0842  8.4931\n",
      "      4        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3522\n",
      "      5        \u001b[36m0.0835\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3270\n",
      "      6        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3942\n",
      "      7        0.0835        0.0837  8.4149\n",
      "      8        0.0835        0.0836  8.3744\n",
      "      9        \u001b[36m0.0834\u001b[0m        0.0839  8.4956\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.004699195529902056\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0862\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3448\n",
      "      2        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3815\n",
      "      3        \u001b[36m0.0838\u001b[0m        0.0839  8.3076\n",
      "      4        \u001b[36m0.0837\u001b[0m        0.0838  8.3675\n",
      "      5        \u001b[36m0.0836\u001b[0m        0.0840  8.3507\n",
      "      6        \u001b[36m0.0836\u001b[0m        0.0836  8.3087\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -9.958452206260127e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0860\u001b[0m        \u001b[32m0.0856\u001b[0m  8.3313\n",
      "      2        \u001b[36m0.0840\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3224\n",
      "      3        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3218\n",
      "      4        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0839\u001b[0m  8.2807\n",
      "      5        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0839\u001b[0m  8.2547\n",
      "      6        \u001b[36m0.0836\u001b[0m        0.0845  8.1644\n",
      "      7        \u001b[36m0.0835\u001b[0m        0.0851  8.1584\n",
      "      8        0.0835        0.0839  8.2191\n",
      "      9        0.0836        0.0842  8.2350\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0004665835518056216\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0859\u001b[0m        \u001b[32m0.0865\u001b[0m  8.2113\n",
      "      2        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0846\u001b[0m  8.0769\n",
      "      3        \u001b[36m0.0838\u001b[0m        0.0856  8.0762\n",
      "      4        \u001b[36m0.0837\u001b[0m        0.0874  8.1843\n",
      "      5        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0840\u001b[0m  8.1886\n",
      "      6        \u001b[36m0.0836\u001b[0m        0.0844  8.3417\n",
      "      7        0.0836        0.0841  8.3424\n",
      "      8        \u001b[36m0.0835\u001b[0m        0.0841  8.3281\n",
      "      9        0.0835        0.0841  8.3466\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.00025151386155308764\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0859\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3308\n",
      "      2        \u001b[36m0.0844\u001b[0m        0.0837  8.3094\n",
      "      3        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0830\u001b[0m  8.3385\n",
      "      4        \u001b[36m0.0842\u001b[0m        0.0834  8.3225\n",
      "      5        \u001b[36m0.0841\u001b[0m        0.0832  8.3188\n",
      "      6        \u001b[36m0.0841\u001b[0m        0.0841  8.3369\n",
      "      7        0.0841        0.0830  8.3353\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.003318912405947838\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0846\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3257\n",
      "      2        \u001b[36m0.0840\u001b[0m        0.0841  8.3745\n",
      "      3        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3389\n",
      "      4        \u001b[36m0.0837\u001b[0m        0.0840  8.3648\n",
      "      5        \u001b[36m0.0836\u001b[0m        0.0837  8.3626\n",
      "      6        \u001b[36m0.0835\u001b[0m        0.0838  8.3210\n",
      "      7        0.0836        0.0835  8.3351\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -7.48571207558335e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0850\u001b[0m        \u001b[32m0.0843\u001b[0m  8.3443\n",
      "      2        \u001b[36m0.0840\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3186\n",
      "      3        \u001b[36m0.0840\u001b[0m        0.0845  8.3015\n",
      "      4        \u001b[36m0.0839\u001b[0m        0.0855  8.3243\n",
      "      5        \u001b[36m0.0839\u001b[0m        0.0849  8.3322\n",
      "      6        0.0839        \u001b[32m0.0841\u001b[0m  8.3231\n",
      "      7        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3028\n",
      "      8        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3422\n",
      "      9        \u001b[36m0.0838\u001b[0m        0.0846  8.3508\n",
      "     10        \u001b[36m0.0838\u001b[0m        0.0842  8.3591\n",
      "     11        0.0838        0.0845  8.3447\n",
      "     12        0.0838        0.0837  8.3586\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.0002637806524181663\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0857\u001b[0m        \u001b[32m0.0832\u001b[0m  8.4929\n",
      "      2        \u001b[36m0.0840\u001b[0m        0.0838  8.4101\n",
      "      3        \u001b[36m0.0838\u001b[0m        0.0833  8.3665\n",
      "      4        \u001b[36m0.0838\u001b[0m        0.0835  8.3708\n",
      "      5        \u001b[36m0.0837\u001b[0m        0.0832  8.3387\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -1.7495913335885405e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0855\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3536\n",
      "      2        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3614\n",
      "      3        \u001b[36m0.0840\u001b[0m        \u001b[32m0.0828\u001b[0m  8.3536\n",
      "      4        0.0841        \u001b[32m0.0826\u001b[0m  8.3434\n",
      "      5        \u001b[36m0.0840\u001b[0m        0.0826  8.3422\n",
      "      6        \u001b[36m0.0838\u001b[0m        0.0834  8.3163\n",
      "      7        0.0838        0.0830  8.3707\n",
      "      8        \u001b[36m0.0837\u001b[0m        0.0826  8.3519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.0009977436702108822\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0856\u001b[0m        \u001b[32m0.0822\u001b[0m  8.3469\n",
      "      2        \u001b[36m0.0846\u001b[0m        0.0832  8.3711\n",
      "      3        \u001b[36m0.0845\u001b[0m        0.0824  8.3314\n",
      "      4        \u001b[36m0.0844\u001b[0m        0.0830  8.3400\n",
      "      5        \u001b[36m0.0843\u001b[0m        0.0825  8.3335\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.0035363176969218646\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0833\u001b[0m  8.3543\n",
      "      2        \u001b[36m0.0837\u001b[0m        0.0845  8.3267\n",
      "      3        \u001b[36m0.0836\u001b[0m        0.0835  8.3601\n",
      "      4        \u001b[36m0.0835\u001b[0m        0.0840  8.3285\n",
      "      5        \u001b[36m0.0834\u001b[0m        0.0834  8.3607\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0034465793260882727\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0848\u001b[0m        \u001b[32m0.0847\u001b[0m  8.3269\n",
      "      2        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0827\u001b[0m  8.3707\n",
      "      3        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0826\u001b[0m  8.3345\n",
      "      4        \u001b[36m0.0836\u001b[0m        0.0826  8.3414\n",
      "      5        \u001b[36m0.0835\u001b[0m        0.0837  8.3468\n",
      "      6        \u001b[36m0.0835\u001b[0m        0.0827  8.3673\n",
      "      7        0.0835        0.0835  8.3762\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.0012616875806894523\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0842\u001b[0m        \u001b[32m0.0824\u001b[0m  8.3549\n",
      "      2        \u001b[36m0.0838\u001b[0m        0.0831  8.3609\n",
      "      3        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0821\u001b[0m  8.3529\n",
      "      4        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0821\u001b[0m  8.3422\n",
      "      5        \u001b[36m0.0835\u001b[0m        0.0823  8.3865\n",
      "      6        0.0835        0.0821  8.3256\n",
      "      7        \u001b[36m0.0834\u001b[0m        0.0822  8.3696\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0008284929723498813\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0844\u001b[0m        \u001b[32m0.0824\u001b[0m  8.3758\n",
      "      2        \u001b[36m0.0833\u001b[0m        0.0825  8.3770\n",
      "      3        \u001b[36m0.0833\u001b[0m        0.0832  8.3593\n",
      "      4        \u001b[36m0.0833\u001b[0m        0.0825  8.3750\n",
      "      5        \u001b[36m0.0832\u001b[0m        0.0829  8.3728\n",
      "      6        0.0833        \u001b[32m0.0824\u001b[0m  8.3801\n",
      "      7        0.0832        0.0825  8.3507\n",
      "      8        0.0832        0.0824  8.3969\n",
      "      9        \u001b[36m0.0832\u001b[0m        0.0826  8.3427\n",
      "     10        0.0832        0.0829  8.3530\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.020249000394646943\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3629\n",
      "      2        \u001b[36m0.0832\u001b[0m        \u001b[32m0.0833\u001b[0m  8.3352\n",
      "      3        0.0833        \u001b[32m0.0831\u001b[0m  8.3535\n",
      "      4        \u001b[36m0.0832\u001b[0m        \u001b[32m0.0829\u001b[0m  8.3537\n",
      "      5        \u001b[36m0.0831\u001b[0m        0.0831  8.3537\n",
      "      6        0.0831        0.0830  8.3191\n",
      "      7        \u001b[36m0.0831\u001b[0m        0.0830  8.3424\n",
      "      8        0.0831        0.0830  8.4721\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.002563615393985863\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0842\u001b[0m        \u001b[32m0.0819\u001b[0m  8.3615\n",
      "      2        \u001b[36m0.0836\u001b[0m        0.0822  8.3656\n",
      "      3        \u001b[36m0.0836\u001b[0m        0.0822  8.3492\n",
      "      4        \u001b[36m0.0836\u001b[0m        0.0827  8.3461\n",
      "      5        \u001b[36m0.0835\u001b[0m        0.0823  8.3137\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0010178976819983365\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3472\n",
      "      2        \u001b[36m0.0831\u001b[0m        0.0837  8.3574\n",
      "      3        \u001b[36m0.0831\u001b[0m        0.0837  8.3401\n",
      "      4        \u001b[36m0.0831\u001b[0m        0.0837  8.3363\n",
      "      5        \u001b[36m0.0831\u001b[0m        0.0840  8.3352\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -0.00030131895907747897\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0822\u001b[0m  8.3451\n",
      "      2        \u001b[36m0.0836\u001b[0m        0.0828  8.3511\n",
      "      3        \u001b[36m0.0835\u001b[0m        0.0829  8.3396\n",
      "      4        \u001b[36m0.0835\u001b[0m        \u001b[32m0.0822\u001b[0m  8.3580\n",
      "      5        \u001b[36m0.0834\u001b[0m        0.0824  8.3370\n",
      "      6        0.0834        0.0822  8.3176\n",
      "      7        0.0834        0.0823  8.2443\n",
      "      8        \u001b[36m0.0834\u001b[0m        0.0823  8.3424\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.001847194652303541\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0835\u001b[0m        \u001b[32m0.0830\u001b[0m  8.3462\n",
      "      2        \u001b[36m0.0833\u001b[0m        0.0830  8.3535\n",
      "      3        \u001b[36m0.0832\u001b[0m        \u001b[32m0.0830\u001b[0m  8.3620\n",
      "      4        \u001b[36m0.0832\u001b[0m        0.0835  8.3247\n",
      "      5        \u001b[36m0.0832\u001b[0m        0.0845  8.3964\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.009642830574228656\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3552\n",
      "      2        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3494\n",
      "      3        \u001b[36m0.0835\u001b[0m        0.0844  8.3371\n",
      "      4        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3433\n",
      "      5        \u001b[36m0.0834\u001b[0m        0.0836  8.3470\n",
      "      6        \u001b[36m0.0833\u001b[0m        0.0840  8.3534\n",
      "      7        0.0834        0.0835  8.3529\n",
      "      8        0.0834        0.0835  8.3599\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.012686479914089555\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0846\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3447\n",
      "      2        \u001b[36m0.0837\u001b[0m        0.0844  8.3140\n",
      "      3        0.0838        \u001b[32m0.0833\u001b[0m  8.3086\n",
      "      4        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3113\n",
      "      5        \u001b[36m0.0835\u001b[0m        0.0832  8.3263\n",
      "      6        0.0836        0.0834  8.3214\n",
      "      7        \u001b[36m0.0835\u001b[0m        0.0833  8.3585\n",
      "      8        \u001b[36m0.0835\u001b[0m        0.0836  8.3470\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0001186032480799959\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0855\u001b[0m        \u001b[32m0.0845\u001b[0m  8.3495\n",
      "      2        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0839\u001b[0m  8.3568\n",
      "      3        \u001b[36m0.0835\u001b[0m        0.0841  8.3462\n",
      "      4        \u001b[36m0.0833\u001b[0m        0.0840  8.3231\n",
      "      5        0.0833        \u001b[32m0.0838\u001b[0m  8.3712\n",
      "      6        0.0833        \u001b[32m0.0838\u001b[0m  8.3126\n",
      "      7        0.0833        0.0838  8.3435\n",
      "      8        \u001b[36m0.0832\u001b[0m        0.0840  8.3387\n",
      "      9        \u001b[36m0.0832\u001b[0m        0.0839  8.3682\n",
      "     10        \u001b[36m0.0832\u001b[0m        \u001b[32m0.0838\u001b[0m  8.3442\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.0018965658442979816\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0840\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3875\n",
      "      2        \u001b[36m0.0836\u001b[0m        0.0845  8.3648\n",
      "      3        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0838\u001b[0m  8.2593\n",
      "      4        0.0834        0.0839  8.3514\n",
      "      5        \u001b[36m0.0833\u001b[0m        0.0838  8.2097\n",
      "      6        0.0833        0.0839  8.4477\n",
      "      7        0.0833        0.0839  8.3668\n",
      "      8        \u001b[36m0.0833\u001b[0m        \u001b[32m0.0838\u001b[0m  8.3658\n",
      "      9        \u001b[36m0.0832\u001b[0m        0.0838  8.2893\n",
      "     10        0.0833        0.0838  8.3301\n",
      "     11        0.0833        0.0839  8.3615\n",
      "     12        \u001b[36m0.0832\u001b[0m        0.0838  8.2335\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of -0.0031267397988916645\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0847\u001b[0m        \u001b[32m0.0839\u001b[0m  8.2789\n",
      "      2        \u001b[36m0.0835\u001b[0m        0.0850  8.2421\n",
      "      3        0.0835        0.0839  8.2353\n",
      "      4        0.0835        0.0844  8.2467\n",
      "      5        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0837\u001b[0m  8.5330\n",
      "      6        \u001b[36m0.0834\u001b[0m        0.0845  8.3563\n",
      "      7        0.0834        0.0840  8.3608\n",
      "      8        0.0834        \u001b[32m0.0837\u001b[0m  8.3136\n",
      "      9        0.0834        0.0840  8.3234\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.0030650447684734683\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0848\u001b[0m        \u001b[32m0.0824\u001b[0m  8.3175\n",
      "      2        \u001b[36m0.0838\u001b[0m        0.0824  8.3278\n",
      "      3        \u001b[36m0.0837\u001b[0m        0.0826  8.3464\n",
      "      4        \u001b[36m0.0836\u001b[0m        0.0825  8.3229\n",
      "      5        \u001b[36m0.0836\u001b[0m        0.0828  8.3382\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.0002491113912643783\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0852\u001b[0m        \u001b[32m0.0843\u001b[0m  8.3527\n",
      "      2        \u001b[36m0.0837\u001b[0m        0.0855  8.3474\n",
      "      3        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3639\n",
      "      4        \u001b[36m0.0835\u001b[0m        0.0836  8.3279\n",
      "      5        \u001b[36m0.0835\u001b[0m        0.0855  8.4162\n",
      "      6        \u001b[36m0.0834\u001b[0m        0.0857  8.3396\n",
      "      7        \u001b[36m0.0834\u001b[0m        0.0853  8.3375\n",
      "      8        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0833\u001b[0m  8.3463\n",
      "      9        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0833\u001b[0m  8.4176\n",
      "     10        0.0834        0.0833  8.3254\n",
      "     11        0.0834        0.0836  8.3497\n",
      "     12        0.0834        0.0835  8.4078\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0012101499417618289\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0848\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3659\n",
      "      2        \u001b[36m0.0841\u001b[0m        0.0846  8.3190\n",
      "      3        0.0841        0.0833  8.3522\n",
      "      4        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0832\u001b[0m  8.2744\n",
      "      5        0.0839        0.0832  8.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.0839\u001b[0m        0.0835  8.3683\n",
      "      7        \u001b[36m0.0838\u001b[0m        0.0832  8.3297\n",
      "      8        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3236\n",
      "      9        \u001b[36m0.0838\u001b[0m        0.0833  8.3582\n",
      "     10        \u001b[36m0.0837\u001b[0m        0.0834  8.3518\n",
      "     11        0.0838        0.0836  8.3482\n",
      "     12        0.0837        0.0835  8.4618\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -8.885048939699658e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0850\u001b[0m  8.2992\n",
      "      2        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3382\n",
      "      3        \u001b[36m0.0834\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3249\n",
      "      4        \u001b[36m0.0834\u001b[0m        0.0834  8.3507\n",
      "      5        \u001b[36m0.0833\u001b[0m        0.0836  8.3297\n",
      "      6        0.0833        0.0831  8.3226\n",
      "      7        0.0833        0.0832  8.3104\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.007565406174544709\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0849\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3163\n",
      "      2        \u001b[36m0.0840\u001b[0m        0.0875  8.3327\n",
      "      3        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3189\n",
      "      4        \u001b[36m0.0838\u001b[0m        0.0838  8.3367\n",
      "      5        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3228\n",
      "      6        0.0837        0.0842  8.3337\n",
      "      7        \u001b[36m0.0836\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3335\n",
      "      8        0.0837        \u001b[32m0.0835\u001b[0m  8.3541\n",
      "      9        \u001b[36m0.0836\u001b[0m        0.0840  8.3616\n",
      "     10        0.0836        0.0841  8.3349\n",
      "     11        \u001b[36m0.0836\u001b[0m        0.0841  8.3496\n",
      "     12        \u001b[36m0.0836\u001b[0m        0.0846  8.3719\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -6.966809581121503e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3229\n",
      "      2        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3470\n",
      "      3        \u001b[36m0.0837\u001b[0m        0.0844  8.3351\n",
      "      4        \u001b[36m0.0837\u001b[0m        0.0836  8.3444\n",
      "      5        \u001b[36m0.0836\u001b[0m        0.0837  8.3472\n",
      "      6        0.0836        0.0847  8.3652\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.0005180943623295242\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0844\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3493\n",
      "      2        \u001b[36m0.0840\u001b[0m        0.0838  8.3686\n",
      "      3        \u001b[36m0.0839\u001b[0m        0.0839  8.3348\n",
      "      4        0.0840        \u001b[32m0.0834\u001b[0m  8.3232\n",
      "      5        0.0840        0.0836  8.3546\n",
      "      6        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0833\u001b[0m  8.3823\n",
      "      7        0.0839        0.0834  8.3674\n",
      "      8        \u001b[36m0.0839\u001b[0m        0.0835  8.3722\n",
      "      9        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0833\u001b[0m  8.3355\n",
      "     10        0.0839        0.0838  8.3616\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 10, RNN returned an R-squared of -0.0012121530248865309\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0848\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3432\n",
      "      2        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3553\n",
      "      3        \u001b[36m0.0840\u001b[0m        0.0835  8.3558\n",
      "      4        \u001b[36m0.0839\u001b[0m        0.0836  8.3681\n",
      "      5        0.0839        0.0839  8.3524\n",
      "      6        \u001b[36m0.0838\u001b[0m        0.0835  8.3294\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 20, RNN returned an R-squared of -0.0002675274983676701\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0847\u001b[0m        \u001b[32m0.0843\u001b[0m  8.3744\n",
      "      2        \u001b[36m0.0840\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3229\n",
      "      3        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3641\n",
      "      4        0.0840        0.0845  8.3685\n",
      "      5        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3515\n",
      "      6        0.0839        0.0842  8.3598\n",
      "      7        \u001b[36m0.0838\u001b[0m        0.0852  8.3312\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 30, RNN returned an R-squared of 1.7687568754642946e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0867\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3086\n",
      "      2        \u001b[36m0.0842\u001b[0m        \u001b[32m0.0837\u001b[0m  8.3550\n",
      "      3        \u001b[36m0.0839\u001b[0m        0.0841  8.3519\n",
      "      4        0.0839        0.0842  8.3688\n",
      "      5        \u001b[36m0.0838\u001b[0m        0.0839  8.3593\n",
      "      6        0.0839        0.0844  8.3451\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 40, RNN returned an R-squared of -0.0006543598439818776\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0851\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3785\n",
      "      2        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3720\n",
      "      3        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0841\u001b[0m  8.3895\n",
      "      4        \u001b[36m0.0838\u001b[0m        0.0842  8.3518\n",
      "      5        0.0839        0.0857  8.3756\n",
      "      6        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3783\n",
      "      7        \u001b[36m0.0837\u001b[0m        0.0846  8.3868\n",
      "      8        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0840\u001b[0m  8.3869\n",
      "      9        \u001b[36m0.0837\u001b[0m        0.0848  8.2940\n",
      "     10        \u001b[36m0.0836\u001b[0m        0.0840  8.3657\n",
      "     11        0.0836        0.0840  8.3479\n",
      "     12        0.0837        0.0840  8.3541\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 50, RNN returned an R-squared of -0.0030961909209268335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0850\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3601\n",
      "      2        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3766\n",
      "      3        \u001b[36m0.0841\u001b[0m        0.0835  8.3492\n",
      "      4        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0834\u001b[0m  8.4159\n",
      "      5        \u001b[36m0.0840\u001b[0m        0.0834  8.3720\n",
      "      6        0.0840        0.0840  8.3336\n",
      "      7        \u001b[36m0.0839\u001b[0m        0.0834  8.3236\n",
      "      8        \u001b[36m0.0839\u001b[0m        0.0835  8.3308\n",
      "      9        \u001b[36m0.0839\u001b[0m        \u001b[32m0.0834\u001b[0m  8.3514\n",
      "     10        0.0839        0.0834  8.3411\n",
      "     11        0.0840        0.0834  8.3716\n",
      "     12        \u001b[36m0.0839\u001b[0m        0.0841  8.3678\n",
      "     13        \u001b[36m0.0839\u001b[0m        0.0840  8.2919\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 60, RNN returned an R-squared of -0.0009209594167316482\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0851\u001b[0m        \u001b[32m0.0857\u001b[0m  8.3327\n",
      "      2        \u001b[36m0.0845\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3295\n",
      "      3        \u001b[36m0.0845\u001b[0m        \u001b[32m0.0835\u001b[0m  8.3331\n",
      "      4        \u001b[36m0.0843\u001b[0m        \u001b[32m0.0832\u001b[0m  8.3627\n",
      "      5        \u001b[36m0.0843\u001b[0m        0.0837  8.3817\n",
      "      6        \u001b[36m0.0842\u001b[0m        \u001b[32m0.0831\u001b[0m  8.3390\n",
      "      7        0.0843        0.0833  8.3374\n",
      "      8        0.0842        \u001b[32m0.0831\u001b[0m  8.3544\n",
      "      9        \u001b[36m0.0842\u001b[0m        0.0831  8.3469\n",
      "     10        0.0842        0.0834  8.3234\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 70, RNN returned an R-squared of -5.8213300218712405e-05\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0855\u001b[0m        \u001b[32m0.0836\u001b[0m  8.3637\n",
      "      2        \u001b[36m0.0842\u001b[0m        0.0856  8.3607\n",
      "      3        \u001b[36m0.0841\u001b[0m        0.0836  8.3644\n",
      "      4        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0836\u001b[0m  8.4178\n",
      "      5        \u001b[36m0.0840\u001b[0m        0.0843  8.3272\n",
      "      6        \u001b[36m0.0839\u001b[0m        0.0838  8.3138\n",
      "      7        0.0840        0.0836  8.3130\n",
      "      8        \u001b[36m0.0839\u001b[0m        0.0838  8.3139\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 80, RNN returned an R-squared of -0.0005168287316856812\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0852\u001b[0m        \u001b[32m0.0850\u001b[0m  8.3708\n",
      "      2        \u001b[36m0.0841\u001b[0m        \u001b[32m0.0847\u001b[0m  8.3439\n",
      "      3        \u001b[36m0.0838\u001b[0m        \u001b[32m0.0847\u001b[0m  8.3143\n",
      "      4        \u001b[36m0.0838\u001b[0m        0.0861  8.3279\n",
      "      5        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0845\u001b[0m  8.3183\n",
      "      6        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0845\u001b[0m  8.3231\n",
      "      7        \u001b[36m0.0836\u001b[0m        0.0847  8.3285\n",
      "      8        0.0837        0.0845  8.3312\n",
      "      9        \u001b[36m0.0836\u001b[0m        0.0845  8.3021\n",
      "     10        \u001b[36m0.0836\u001b[0m        0.0845  8.3197\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 90, RNN returned an R-squared of -0.00942131881863495\n",
      "Training model <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
      "  module=RNN(\n",
      "    (embed): Embedding(20, 5)\n",
      "    (internallayer): Linear(in_features=37, out_features=32, bias=True)\n",
      "    (outputlayer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  ),\n",
      ") on 80000 data points\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.0844\u001b[0m        \u001b[32m0.0847\u001b[0m  8.3176\n",
      "      2        \u001b[36m0.0840\u001b[0m        \u001b[32m0.0846\u001b[0m  8.3473\n",
      "      3        \u001b[36m0.0839\u001b[0m        0.0850  8.3423\n",
      "      4        \u001b[36m0.0838\u001b[0m        0.0851  8.3091\n",
      "      5        \u001b[36m0.0837\u001b[0m        \u001b[32m0.0845\u001b[0m  8.3351\n",
      "      6        \u001b[36m0.0837\u001b[0m        0.0846  8.3259\n",
      "      7        \u001b[36m0.0837\u001b[0m        0.0846  8.3225\n",
      "      8        \u001b[36m0.0836\u001b[0m        0.0846  8.2876\n",
      "      9        \u001b[36m0.0836\u001b[0m        0.0847  8.3272\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "For sequence length 100, RNN returned an R-squared of -0.001095315362305227\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.8148632046176768\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.8835553063022349\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.8672157404318428\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.867425866990789\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.7656252900174747\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.7368670495261016\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.8649606699041114\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.8865380467516215\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.8955703973956226\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.801773910912103\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.8047101644048297\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.9872678362980052\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.745885286158396\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.6650531222627748\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.6583546763690277\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.9623891884089933\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.9733772917201859\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.8538976099590422\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 90, MLP returned an R-squared of 0.771800873531496\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.6787447420618291\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.7912947305473844\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.7950769318512891\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.7965635286025092\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.9571784435779167\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.7454577060949593\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.8394989043910683\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.7540535497479401\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.648931161537438\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.8764587497421239\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.7694294699162116\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.658153083418249\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.9680096621235325\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.8767355553671766\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.7978691859068532\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.6304058261875144\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.9933066686226296\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.8622801658789683\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.9538743197589596\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.8710465036573942\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.6898602872477086\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.9361814365064203\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.8871777488342394\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.8962074172360023\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.7795231507848849\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.8534809480627539\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.8646593444693244\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.9470207150401008\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.8747388960943036\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.8228889359775499\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.9376330819614516\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.572963270792961\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.046985312256426104\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.5778432223101726\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.5532991812988841\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.5723291332703047\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.49445527572824466\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.5360495041562383\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.5208425820749689\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.502440082514861\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.4335545169777181\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.505351734403\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.586397989044905\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.5054931380618455\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.5368769249609724\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.6411057864456463\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.5152481997757717\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.6540503267970628\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.6419494087798305\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.5994275952519672\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.5186377512328532\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.5601519681297887\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 20, MLP returned an R-squared of 0.6076224530752081\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.5015829925293296\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.49348918561413757\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.5098904134644742\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.5764514206084046\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.00767753878142452\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.5383640394511976\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.5504106276941706\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.6055638817965034\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.6022092250504476\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.6065340310432681\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.6948015892306462\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.673334642109046\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.5802532655962112\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.5378263605965362\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.6453262909350864\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.6712403005430461\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.596709538142177\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.6886943477062728\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.7603337101923596\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.677350517036261\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.688743167991191\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.6707302707951148\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.7115970497441254\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.7668162960123158\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.6739200118405391\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.6413440894245204\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.6451850953031852\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.6942462797909663\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.0026053367807938876\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.3599184472535416\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.3908306500330023\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.030645435401244536\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.03389996324358324\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.021882548129630086\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.03361881039005443\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.02403372054979036\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.016510040268329806\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.013963771278192594\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.024063982632864245\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.020678980544621806\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.03128215212531016\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.06842647839816585\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.022945539960475214\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.02613855135833465\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.02495941594712392\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.03249205148065348\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.019644112974729566\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.011821481632392272\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.008342483979962179\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.0210877419030413\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.009285475381940134\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.036582075442925355\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 50, MLP returned an R-squared of 0.0023277434390300833\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.01813441709405672\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.020069749953708693\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.009141450779915905\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.0009217059090869117\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.019426442420331425\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.024950627815017112\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.005594746785106297\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.007281959133349947\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.010785440275868363\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.005756238060849128\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.004021855580075284\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.007786638604554641\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.011920923461835153\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of 0.020829814059054375\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.015083276687803737\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of 0.04884357502449577\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.03090303275248818\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.03578229014589962\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 0.016929341714945978\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of 0.3220743191937223\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of 0.044135455873869245\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of 0.06285155346516635\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of 0.08096296373874667\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.041284660604154366\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of 0.27840235805672153\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.008815543557432415\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.003135364992324252\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.014984465163105476\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of 5.015517899176203e-05\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.0026731410549136747\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.008176868096809953\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.00751463831068544\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.008336090666980711\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.006855005601793529\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.002079314135610133\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.001444528101716136\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of 0.0006950265982513804\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of 0.00030599446309320655\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.055452106226913545\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.002305495919916689\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.0036629149419855622\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.0038325917656563124\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -4.123029155778113e-05\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.00035714921985374737\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.009071439713320473\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.00186443647667911\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.002404305617831559\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.003566087168133114\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.014488998993139823\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.013393217794110734\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.052165567168743765\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 70, MLP returned an R-squared of -0.0008353314603881667\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.00905489977969931\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.008486024327105435\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.016741011406603112\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.0308459504815346\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.0008058307322302838\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.0012709824812910941\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.010034536178045617\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.001987835945733263\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.004630473095498688\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.001463185842346837\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.0010709877573746862\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.0007555731155117762\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.006017374918054319\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.009879863537662237\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.0027457374200492435\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.0019509685666210252\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.0015099817198678256\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.01457740653452344\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.0007231589790150927\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.002182799757649345\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.002288663052386619\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.0011802997997256703\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.0025928539770703196\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.00443642488465712\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.00044241138601686103\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.016569047474953802\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.0037271556887190638\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.002115918609516987\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.001296302377213232\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.003483737332411918\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.001095592723115546\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.0023467372865502156\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.00017849608548203832\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.021580629704301746\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.0011696631051407724\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.004733425474315656\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.0023209615536277095\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.0033609232291720303\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.0009382146307066463\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.0010725864191771795\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.0016323913100417808\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.0007011438253394786\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -6.589315767002724e-05\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.0003535459944785657\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -3.311019870366749e-05\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.003297210297116493\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -8.304961987026793e-05\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -5.424677757170926e-05\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.005883835863896181\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.002178586168289698\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.00012386095375926942\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 90, MLP returned an R-squared of -0.006096958898985205\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.0006794600681472307\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.0005189813794772569\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.003017488946041702\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.0013634957503101397\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.003446880689970122\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.0011345365210269076\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.013643367157479158\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.0011114075172808935\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.019411195854049268\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.0014821607027601935\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.027158343896283244\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 10, MLP returned an R-squared of -0.002739869073312473\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 20, MLP returned an R-squared of -0.0007890553261147293\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 30, MLP returned an R-squared of -0.0005642671982521996\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 40, MLP returned an R-squared of -0.002246451260497606\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 50, MLP returned an R-squared of -0.00017605008953958112\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 60, MLP returned an R-squared of -0.0009903629540504166\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 70, MLP returned an R-squared of -0.0012443649803905998\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 80, MLP returned an R-squared of -0.00014220723818536563\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 90, MLP returned an R-squared of -0.005971235842000322\n",
      "Training model MLPRegressor(hidden_layer_sizes=(50, 100, 50)) on 80000 data points\n",
      "For sequence length 100, MLP returned an R-squared of -0.0019307032542161817\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.25581802020062006\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.2558668652057542\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.24787720664128177\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.24743515130220106\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.25951677503398185\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.2586347812989638\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.2616654913463009\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.25398368988558284\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.2666048833507302\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.2642496538984357\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.2519044578229148\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.2511564446747101\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.26265290824056176\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.2533339239728517\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.2512171081582013\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.2521654750338551\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.25086323511774566\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.24910171789349245\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.2577657885502429\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.2464503915342383\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.3279112013783826\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.33367175765204893\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.3266532188616642\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.3272762974250496\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.32733108110178155\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.33946048494916736\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.33049954909627843\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.33160560348159374\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.3397996053050716\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.33507111316914784\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 10, SVR returned an R-squared of 0.30276722338596296\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.2974675375139402\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.3065010684361914\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.31393594787325807\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.30271521065444795\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.3093501030528151\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.29704348852125484\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.30628720951151345\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.3162923661200199\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.3045616778606941\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.38936336706924235\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.3753174289976363\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.39524056570704846\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.4022223355179727\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.41290513660925543\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.39495301653282144\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.4046562806605082\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.39840889621026054\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.41217490728015915\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.38672642665503454\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.08061950594671785\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.0793501277136488\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.0767785716741407\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.07974032901631445\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.07645695940290631\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.07242561026018224\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.08046085523020108\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.0784757516258513\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.07922609786589396\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.07353501416519581\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.10852969327371442\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.11373804440073942\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.1148687558191287\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.10459011630508352\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.10727377174683939\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.10782993485350212\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.09831085009962692\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.106868831144312\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.10954134310501062\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.10367362091211219\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.07053720776039696\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.08103710804296127\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.0786490835188477\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.07658978011078177\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.073821922285072\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.08065114472643842\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.08346792011830018\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.07631590727241355\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.07692771869232218\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.08177040260055335\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.08401170335978858\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.08131784170593781\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 30, SVR returned an R-squared of 0.08275681245784217\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.08677904505219358\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.08625537209361345\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.08610641768024085\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.08292868346613513\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.07425820888391099\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.09185509573216499\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.07992147335771094\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.11363029482896059\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.10182994553766145\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.09791752077757232\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.10522374011308355\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.10726483225897487\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.11068880010740245\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.10987533009451556\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.11718147131403556\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.10762484699941721\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.10585991703709363\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.01644203084202045\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.021432417205415888\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.019416575641545974\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.019203445594929813\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.01832593168905461\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.0189218497103123\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.0200628197801781\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.020448013941933296\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.01795017921196318\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.02126532692585159\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.026974055657925544\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.025510451483135577\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.024861990121115007\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.025772714854312873\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.029090189444865966\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.03006556540256422\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.025243166268324013\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.02977066516428173\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.028957380728912407\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.02653178142455581\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.026189000041539012\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.023685404984465364\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.026466059748341708\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.02596375834203657\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.024592864121299574\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.02188353780006469\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.02411220720296281\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.024807968716666795\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.023713767358285343\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.02904620496013821\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.019844055598051136\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.017443915757443418\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.019842452895151852\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.020601292235890245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.018260152227546844\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.02316727370407967\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.021229677017989834\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.01945246889330232\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.016899083377653223\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.019448051062586624\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.024776525275230443\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.0194102608992317\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.01887236332601394\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.021535373743951647\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.019641031354301175\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.022590835523247144\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.02313407717600957\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.019290551776626308\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.02151110290791347\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.021257151764624727\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.002576314085284137\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.0022190864127400456\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.002054186177506856\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.0032483162653500486\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.0020111077281602663\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.003360267650051596\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.0013218998154904682\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.002414522062838209\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.002876377455422241\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.0022810222894885657\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.004005872153379664\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.003933585138295115\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.0030795206269342845\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.0020623776789998605\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.005524546472153391\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.004497138739188\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.0035661024654428664\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.004612636947028181\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.004012621706218966\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.003311809325028614\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.0014265196504686939\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.0030497598547454263\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.001222046217571715\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.002579848994914591\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.003197877999512766\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.003322865785846396\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.0028761779014806477\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.002406296656266349\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.0015854909651522586\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.0028082162691543955\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.0009267848768289566\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.002638129723767646\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.002566490273343769\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.001040700119947835\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.0018963395898150548\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 60, SVR returned an R-squared of 0.0026794805661364274\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.0024899961903963597\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.0006216039340152246\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.00019615604089828764\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.0024658829542857363\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of 0.002890937373854041\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of 0.0025172082766017345\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of 0.0038697217762158065\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of 0.0033873495335745574\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of 0.0015647729188396076\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of 0.0027428701610323625\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of 0.003688640950201938\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of 0.0036561405697674854\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of 0.0021776898142021883\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of 0.002525882404436852\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of -0.0008247975784825279\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of -0.00220129552239845\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of -0.0020151857057237876\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of -0.0023476184117876553\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of -0.0018406766336644953\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of -0.0007755088526248954\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of -0.0024963996133093858\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of -0.0019277323341337915\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of -0.0013156385082537803\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of -0.00096589555276827\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of -0.0007929231216554644\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of -0.0019271246413190557\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of -0.0013891570235704531\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of -0.001202126313389451\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of -0.0012187256507754096\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of -0.002311699374278886\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of -0.002825948911768128\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of -0.0013011937609841429\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of -0.0032300301764394135\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of -0.0010946678951071487\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of -0.0011674254602527245\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of -0.0015112638947110124\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of -0.0012642646184335415\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of -0.0015474066934191288\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of -0.002356818167387198\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of -0.0015618575295694992\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of -0.001997065566014866\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of -0.0009468514287571672\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of -0.0006971943514768597\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of -0.0011346014522102266\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of -0.0013251837864196592\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of -0.0010172640334609184\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of -0.00022509461508923856\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of -0.0018090270006483333\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of -0.0004482181657439188\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of -0.0022583116675640724\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence length 70, SVR returned an R-squared of -0.0013932649000996111\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of -0.0010210588306642787\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of -0.0019269690452436983\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of -0.0008844300627042223\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 10, SVR returned an R-squared of -0.0006511769857768801\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 20, SVR returned an R-squared of -0.0014520872904004722\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 30, SVR returned an R-squared of -0.0011275032730184709\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 40, SVR returned an R-squared of -0.0012629421978891386\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 50, SVR returned an R-squared of -0.001450828781886404\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 60, SVR returned an R-squared of -0.000725914464742683\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 70, SVR returned an R-squared of -0.0014962902767734487\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 80, SVR returned an R-squared of -0.001595494307664591\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 90, SVR returned an R-squared of -0.001947424600998282\n",
      "Training model BaggingRegressor(base_estimator=SVR(), n_jobs=32) on 80000 data points\n",
      "For sequence length 100, SVR returned an R-squared of -0.0012365083799297238\n"
     ]
    }
   ],
   "source": [
    "# Example code to examine sequence lengths\n",
    "\n",
    "full_length_dependence_results = length.length_testing(modeldict,saved_nk_landscapes,seq_lens=[10,20,30,40,50,60,70,80,90,100],file_name=\"Full_Sequence_Length_NK_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RF': {'NK-0': array([[0.94927608, 0.94848097, 0.94752039, 0.94993732, 0.94810276,\n",
       "          0.94877597, 0.94848869, 0.94855927, 0.94901732, 0.94761264],\n",
       "         [0.93618074, 0.9365457 , 0.93476952, 0.9347386 , 0.93590797,\n",
       "          0.93513671, 0.93386742, 0.93315358, 0.9338608 , 0.93500718],\n",
       "         [0.98400622, 0.98319218, 0.9832205 , 0.98246289, 0.98269002,\n",
       "          0.98300143, 0.98339633, 0.98315943, 0.98434355, 0.98355993],\n",
       "         [0.98797241, 0.98665022, 0.98771975, 0.98762167, 0.98733581,\n",
       "          0.9874802 , 0.98737471, 0.98701341, 0.98744593, 0.98705252],\n",
       "         [0.96830568, 0.96896021, 0.96802896, 0.96764866, 0.9680619 ,\n",
       "          0.96840337, 0.96870459, 0.96848281, 0.96890582, 0.96780758]]),\n",
       "  'NK-1': array([[0.82269943, 0.82726028, 0.83488753, 0.84017076, 0.83583008,\n",
       "          0.83111953, 0.83479957, 0.82955751, 0.83026758, 0.83148728],\n",
       "         [0.81750986, 0.82111902, 0.81611153, 0.82148696, 0.82169595,\n",
       "          0.81933071, 0.82027347, 0.81917547, 0.81656774, 0.81729292],\n",
       "         [0.89865877, 0.89558036, 0.9022353 , 0.89686122, 0.89761179,\n",
       "          0.89840196, 0.89578002, 0.89646592, 0.90466425, 0.90281231],\n",
       "         [0.89490309, 0.8980766 , 0.89562889, 0.89480718, 0.89624135,\n",
       "          0.89776293, 0.89717202, 0.8980597 , 0.89548987, 0.89580766],\n",
       "         [0.91314786, 0.91199063, 0.91304632, 0.9117548 , 0.91340748,\n",
       "          0.91009195, 0.90905597, 0.91164428, 0.90968299, 0.91195023]]),\n",
       "  'NK-2': array([[0.8784981 , 0.88324113, 0.88408171, 0.88152033, 0.88046979,\n",
       "          0.88038658, 0.88118026, 0.88097537, 0.87909214, 0.88184364],\n",
       "         [0.660098  , 0.66757199, 0.66804402, 0.66960959, 0.66883136,\n",
       "          0.66906828, 0.66489152, 0.66732279, 0.66811236, 0.66839006],\n",
       "         [0.68901068, 0.69113967, 0.68706487, 0.68613813, 0.69232222,\n",
       "          0.68549167, 0.68550642, 0.67990864, 0.68819147, 0.68829872],\n",
       "         [0.67495328, 0.674264  , 0.66866618, 0.66458624, 0.67292774,\n",
       "          0.67032481, 0.67011925, 0.6669855 , 0.67506852, 0.66203241],\n",
       "         [0.8807367 , 0.88358079, 0.88243545, 0.88262359, 0.88312999,\n",
       "          0.88208166, 0.88484206, 0.88297753, 0.88423281, 0.88645477]]),\n",
       "  'NK-3': array([[0.44301538, 0.4415358 , 0.43460824, 0.43734722, 0.43997685,\n",
       "          0.44775795, 0.44634286, 0.45196111, 0.43419468, 0.44250045],\n",
       "         [0.47424988, 0.48209338, 0.46370813, 0.47191612, 0.47143846,\n",
       "          0.47567876, 0.47016572, 0.46839665, 0.47346861, 0.47229256],\n",
       "         [0.43301936, 0.4206864 , 0.43872366, 0.43037584, 0.41839697,\n",
       "          0.42797354, 0.39969186, 0.43439234, 0.43418219, 0.425629  ],\n",
       "         [0.41886701, 0.40468544, 0.41361384, 0.42099005, 0.42529287,\n",
       "          0.41186583, 0.40276222, 0.39740338, 0.41694614, 0.39940674],\n",
       "         [0.7405352 , 0.73166431, 0.73582223, 0.73717754, 0.74131348,\n",
       "          0.72354982, 0.72825713, 0.74074845, 0.73883134, 0.73763164]]),\n",
       "  'NK-4': array([[-0.13864885, -0.1392857 , -0.13403925, -0.1348497 , -0.13597051,\n",
       "          -0.13728272, -0.13361383, -0.13613112, -0.13208335, -0.13419111],\n",
       "         [-0.1386497 , -0.1401437 , -0.138703  , -0.13984781, -0.13871867,\n",
       "          -0.13582858, -0.12941248, -0.13386505, -0.14165044, -0.14505938],\n",
       "         [-0.1337916 , -0.13463143, -0.13646346, -0.14080994, -0.14051356,\n",
       "          -0.13776628, -0.14102512, -0.14398987, -0.13994336, -0.13686494],\n",
       "         [-0.13860689, -0.1429795 , -0.13709686, -0.13637858, -0.14196478,\n",
       "          -0.14027468, -0.13868338, -0.14483982, -0.13770084, -0.13388878],\n",
       "         [-0.13334615, -0.12453661, -0.12620852, -0.12847362, -0.14058433,\n",
       "          -0.13553245, -0.13819134, -0.13083011, -0.12897097, -0.12602948]])},\n",
       " 'Linear': {'NK-0': array([[0.06349946, 0.05767809, 0.06440522, 0.06115381, 0.05834556,\n",
       "          0.05853857, 0.06206986, 0.0644073 , 0.06281049, 0.06210905],\n",
       "         [0.04099033, 0.04637662, 0.04646268, 0.04320751, 0.04451677,\n",
       "          0.04598882, 0.04268451, 0.04745134, 0.03946153, 0.04540057],\n",
       "         [0.23095852, 0.22351787, 0.22371424, 0.22745515, 0.21258431,\n",
       "          0.2135665 , 0.21540611, 0.217062  , 0.22238897, 0.22650218],\n",
       "         [0.19637881, 0.20346207, 0.19567595, 0.20343954, 0.18863303,\n",
       "          0.19898984, 0.19067075, 0.19678318, 0.19634359, 0.2047819 ],\n",
       "         [0.04298139, 0.04371925, 0.0422936 , 0.04568814, 0.0453639 ,\n",
       "          0.0441032 , 0.04523847, 0.04362866, 0.04158621, 0.04615391]]),\n",
       "  'NK-1': array([[0.00684355, 0.00784042, 0.01048761, 0.00850228, 0.00823527,\n",
       "          0.00820808, 0.00604678, 0.00866585, 0.00886539, 0.00583649],\n",
       "         [0.00615391, 0.00701017, 0.00681241, 0.00671583, 0.00524476,\n",
       "          0.00381521, 0.00678823, 0.00570616, 0.00540049, 0.00762169],\n",
       "         [0.02076332, 0.01919392, 0.0206027 , 0.01889892, 0.02248247,\n",
       "          0.0178308 , 0.02342161, 0.01974887, 0.02240298, 0.02123244],\n",
       "         [0.01007211, 0.01144614, 0.00708008, 0.00710483, 0.00832655,\n",
       "          0.00775562, 0.00745359, 0.00820694, 0.00773613, 0.00797716],\n",
       "         [0.04734043, 0.04408128, 0.04416947, 0.04729867, 0.05177037,\n",
       "          0.04589218, 0.04532211, 0.04050978, 0.04811361, 0.0484109 ]]),\n",
       "  'NK-2': array([[0.00199454, 0.0021638 , 0.00082195, 0.00187532, 0.00199574,\n",
       "          0.00237859, 0.00219477, 0.00208036, 0.00264616, 0.00182876],\n",
       "         [0.00874246, 0.00918635, 0.010423  , 0.00850653, 0.01105871,\n",
       "          0.01067129, 0.00860246, 0.009291  , 0.00783867, 0.01082961],\n",
       "         [0.00858325, 0.00995332, 0.00989081, 0.00846113, 0.00772557,\n",
       "          0.0102455 , 0.00900041, 0.0098928 , 0.00829443, 0.00775956],\n",
       "         [0.00346919, 0.00248712, 0.00360729, 0.00218227, 0.00403983,\n",
       "          0.00336664, 0.00168952, 0.00261205, 0.00347105, 0.0024431 ],\n",
       "         [0.00632658, 0.00461587, 0.00504229, 0.00741019, 0.00396049,\n",
       "          0.00549163, 0.00457453, 0.0049268 , 0.00473297, 0.00607088]]),\n",
       "  'NK-3': array([[-0.00012418,  0.00043512,  0.00074885,  0.00071259,  0.00069847,\n",
       "           0.00083258,  0.00041223,  0.00049745,  0.0000398 ,  0.00008445],\n",
       "         [-0.000559  ,  0.00050077,  0.00063577,  0.00084964,  0.00083292,\n",
       "           0.00003587,  0.00024832,  0.00058849,  0.00059538,  0.00135342],\n",
       "         [-0.00041046, -0.00000974, -0.0001598 ,  0.0000638 , -0.00008033,\n",
       "           0.00017699,  0.00004961, -0.00005377,  0.00010979,  0.00005993],\n",
       "         [ 0.00050917,  0.00015255,  0.00032226,  0.00038793, -0.00024183,\n",
       "           0.00056767, -0.00023523,  0.00018401, -0.00024728,  0.00018932],\n",
       "         [-0.000034  , -0.00007479, -0.00030049, -0.0001062 ,  0.00031638,\n",
       "           0.00016833,  0.00031856, -0.0001165 , -0.00006651,  0.00009156]]),\n",
       "  'NK-4': array([[-0.00012962, -0.00038721, -0.0003417 , -0.00030168, -0.00109546,\n",
       "          -0.00006141, -0.00014914, -0.0001915 , -0.00029057, -0.00018396],\n",
       "         [-0.00013555, -0.00039002, -0.00040155, -0.00010934, -0.00034766,\n",
       "          -0.00018316, -0.00026111, -0.00020335, -0.00007553, -0.00003578],\n",
       "         [ 0.00014973,  0.00007874,  0.00001101, -0.00007331, -0.00014188,\n",
       "           0.00008345, -0.00021113,  0.00000385, -0.00022447,  0.00000415],\n",
       "         [-0.00005909, -0.00059615, -0.00034264, -0.00032799, -0.00027879,\n",
       "          -0.00003285, -0.00044603, -0.0001202 , -0.00038436, -0.00049061],\n",
       "         [ 0.00003173, -0.0002062 , -0.00106802, -0.0000693 ,  0.00009049,\n",
       "          -0.00006409, -0.00012246, -0.00028376, -0.0005366 , -0.00018687]])},\n",
       " 'GB': {'NK-0': array([[0.99877451, 0.99884578, 0.99891038, 0.99877812, 0.99868862,\n",
       "          0.99873499, 0.99878723, 0.99880113, 0.99888862, 0.99892156],\n",
       "         [0.99796897, 0.99817323, 0.99810762, 0.99799241, 0.99812518,\n",
       "          0.99814254, 0.99810856, 0.99829001, 0.99807704, 0.99796086],\n",
       "         [0.99891917, 0.99887263, 0.99892264, 0.99894827, 0.9988224 ,\n",
       "          0.99884831, 0.99910043, 0.9989327 , 0.99897177, 0.99889352],\n",
       "         [0.99826256, 0.99838698, 0.99853786, 0.99815749, 0.99814966,\n",
       "          0.9982223 , 0.9983638 , 0.99824245, 0.99818459, 0.99802354],\n",
       "         [0.9990517 , 0.99906   , 0.99888449, 0.99900127, 0.99901541,\n",
       "          0.99893399, 0.99896669, 0.99915211, 0.99904271, 0.99899189]]),\n",
       "  'NK-1': array([[0.8337903 , 0.84215691, 0.83107606, 0.8314969 , 0.8399596 ,\n",
       "          0.83603459, 0.83184297, 0.84099966, 0.82362918, 0.84678174],\n",
       "         [0.84404804, 0.84989941, 0.85467891, 0.8501289 , 0.85108516,\n",
       "          0.86611801, 0.85115523, 0.86307725, 0.86264167, 0.86278848],\n",
       "         [0.86375614, 0.86024121, 0.86770568, 0.85370408, 0.8442401 ,\n",
       "          0.84030371, 0.85902422, 0.85724663, 0.85106496, 0.8522563 ],\n",
       "         [0.88353555, 0.87980933, 0.88334652, 0.88916129, 0.87980119,\n",
       "          0.88273022, 0.88759622, 0.88294977, 0.8768418 , 0.8791924 ],\n",
       "         [0.91130504, 0.92001635, 0.92680665, 0.92856732, 0.9183264 ,\n",
       "          0.92638538, 0.92530245, 0.92281668, 0.92558868, 0.92546585]]),\n",
       "  'NK-2': array([[0.39520904, 0.40185762, 0.41056602, 0.40121461, 0.40635308,\n",
       "          0.39251697, 0.39730043, 0.39089246, 0.39125436, 0.39457878],\n",
       "         [0.30940945, 0.31520468, 0.30154733, 0.31210988, 0.32330748,\n",
       "          0.32007231, 0.31558303, 0.31677211, 0.31460192, 0.31526532],\n",
       "         [0.3230577 , 0.31317132, 0.30725848, 0.30502153, 0.31456595,\n",
       "          0.30875851, 0.30904909, 0.31568351, 0.3075737 , 0.31026484],\n",
       "         [0.35303521, 0.3537202 , 0.35518205, 0.35286187, 0.35023027,\n",
       "          0.34143806, 0.3458531 , 0.34577034, 0.35943098, 0.34892569],\n",
       "         [0.3781083 , 0.37368159, 0.38773425, 0.3785066 , 0.37182888,\n",
       "          0.36625871, 0.38309053, 0.37483788, 0.3790412 , 0.37259552]]),\n",
       "  'NK-3': array([[0.0692957 , 0.07035645, 0.0668257 , 0.06619957, 0.06767896,\n",
       "          0.06785393, 0.07103311, 0.06441739, 0.06631425, 0.06859721],\n",
       "         [0.07047651, 0.0669268 , 0.07243651, 0.0686723 , 0.07057237,\n",
       "          0.06808004, 0.0690971 , 0.07359974, 0.06732305, 0.06855711],\n",
       "         [0.06357338, 0.06579465, 0.0655166 , 0.06975996, 0.06870298,\n",
       "          0.06749464, 0.06708129, 0.07175131, 0.06562547, 0.07099755],\n",
       "         [0.05651104, 0.05507855, 0.05902315, 0.05879396, 0.05621225,\n",
       "          0.05543905, 0.05615927, 0.05494828, 0.05456397, 0.06146179],\n",
       "         [0.08523063, 0.08072325, 0.08115649, 0.08489253, 0.0848137 ,\n",
       "          0.08455588, 0.08276245, 0.07982345, 0.08130336, 0.0861429 ]]),\n",
       "  'NK-4': array([[-0.00779112, -0.00528268, -0.00466664, -0.00982463, -0.0082493 ,\n",
       "          -0.00718996, -0.00793644, -0.00589912, -0.00933726, -0.0053346 ],\n",
       "         [-0.01195862, -0.00685438, -0.00487246, -0.00703548, -0.00728277,\n",
       "          -0.00903019, -0.00722829, -0.00669212, -0.00746686, -0.00584809],\n",
       "         [-0.00777305, -0.00562979, -0.00640129, -0.00896289, -0.0076579 ,\n",
       "          -0.00686206, -0.00593417, -0.00759599, -0.00730783, -0.00941965],\n",
       "         [-0.0067897 , -0.00760311, -0.00776461, -0.0064398 , -0.00823722,\n",
       "          -0.0079096 , -0.00787918, -0.00588178, -0.00913287, -0.00836499],\n",
       "         [-0.00658497, -0.00666937, -0.00620566, -0.00687117, -0.00908209,\n",
       "          -0.00851922, -0.00912992, -0.00867079, -0.00656662, -0.00837665]])},\n",
       " 'RNN': {'NK-0': array([[ 0.47301986,  0.20520681, -0.00054108,  0.93999833,  0.96743765,\n",
       "          -0.00351752,  0.60989624, -0.02158751, -0.01835793, -0.00129347],\n",
       "         [ 0.98066166, -0.00717159, -0.02072308,  0.31772044, -0.00384002,\n",
       "          -0.00000596,  0.8785564 ,  0.91275634,  0.98717658,  0.99629064],\n",
       "         [-0.00045195, -0.00067672,  0.14808498, -0.02645118,  0.98390941,\n",
       "           0.98373579,  0.9834765 ,  0.32234866,  0.92177041,  0.3811404 ],\n",
       "         [-0.00160141, -0.01784591, -0.00708599,  0.98385204,  0.93934814,\n",
       "          -0.00031947, -0.01568575,  0.96822332,  0.95388077,  0.99024145],\n",
       "         [ 0.9387063 ,  0.19549426,  0.97968941, -0.0009075 , -0.01904227,\n",
       "          -0.02207415, -0.00005514, -0.0012868 ,  0.92741126,  0.98814825]]),\n",
       "  'NK-1': array([[-0.00296012,  0.0165903 , -0.00079333,  0.00549641,  0.01814318,\n",
       "           0.03674276, -0.00103186,  0.01745316, -0.01296275, -0.01645956],\n",
       "         [ 0.09618813,  0.36693436,  0.00921796,  0.00000018, -0.00962807,\n",
       "           0.01161889, -0.00197169, -0.00082751, -0.00314369,  0.01613934],\n",
       "         [-0.00000108,  0.1760217 ,  0.05887859, -0.0051934 ,  0.02812025,\n",
       "           0.01355604, -0.00310518,  0.06971926,  0.02606167,  0.61809455],\n",
       "         [-0.02291677,  0.6578617 , -0.00033243,  0.27140742,  0.02083998,\n",
       "           0.13959821, -0.00060123,  0.08090386,  0.41959309, -0.06239357],\n",
       "         [ 0.00875244, -0.00330208,  0.03162798,  0.01666969, -0.00022358,\n",
       "          -0.0223684 , -0.01162489, -0.00167203, -0.00524932,  0.03529335]]),\n",
       "  'NK-2': array([[ 0.00099122, -0.00224821,  0.00598971, -0.01220907,  0.00014176,\n",
       "           0.00680929, -0.00782992, -0.00109994, -0.00634704, -0.01057017],\n",
       "         [-0.00055301,  0.01004553,  0.00323062,  0.00817569,  0.00179779,\n",
       "          -0.00072295, -0.00443212, -0.02439131, -0.00001836, -0.00153288],\n",
       "         [-0.00113466,  0.0003685 , -0.00588095, -0.00264071, -0.00015092,\n",
       "          -0.00870449,  0.00282367, -0.00036335, -0.00849986, -0.00153115],\n",
       "         [ 0.00180263, -0.00440287, -0.00015302,  0.00057588, -0.00016979,\n",
       "          -0.00754419, -0.00531777,  0.00126525, -0.00081672,  0.00326384],\n",
       "         [-0.01124119, -0.00987994, -0.00110724, -0.009386  , -0.00967123,\n",
       "          -0.00683466, -0.0072144 , -0.00652629,  0.00172415, -0.00382716]]),\n",
       "  'NK-3': array([[-0.0200834 , -0.00093183, -0.00950253, -0.000023  , -0.00496703,\n",
       "           0.00000342, -0.00299604, -0.01338835, -0.00068845, -0.00331043],\n",
       "         [-0.01955514, -0.003696  , -0.00014171, -0.00418083, -0.02407864,\n",
       "          -0.01649934, -0.0001421 , -0.00482865, -0.01781258,  0.00035154],\n",
       "         [-0.00097584, -0.00437502, -0.0096517 , -0.00012231, -0.01289818,\n",
       "          -0.0011261 , -0.00016788, -0.01347677, -0.00023323, -0.00029615],\n",
       "         [-0.00199509, -0.0026329 , -0.00018299, -0.00007166, -0.00607065,\n",
       "          -0.01304868, -0.00808944, -0.00693938, -0.00561747, -0.00876593],\n",
       "         [-0.00554268, -0.00114315, -0.00038037, -0.00583327, -0.00024548,\n",
       "          -0.00901838, -0.00054843, -0.00182313, -0.00572597, -0.0015567 ]]),\n",
       "  'NK-4': array([[-0.0154727 , -0.0057605 , -0.00438448, -0.00129063, -0.00028051,\n",
       "          -0.01501872, -0.0001658 , -0.00319358, -0.01000709, -0.00197991],\n",
       "         [-0.0046992 , -0.00009958, -0.00046658, -0.00025151, -0.00331891,\n",
       "          -0.00007486, -0.00026378, -0.0000175 , -0.00099774, -0.00353632],\n",
       "         [-0.00344658, -0.00126169, -0.00082849, -0.020249  , -0.00256362,\n",
       "          -0.0010179 , -0.00030132, -0.00184719, -0.00964283, -0.01268648],\n",
       "         [-0.0001186 , -0.00189657, -0.00312674, -0.00306504, -0.00024911,\n",
       "          -0.00121015, -0.00008885, -0.00756541, -0.00006967, -0.00051809],\n",
       "         [-0.00121215, -0.00026753,  0.00001769, -0.00065436, -0.00309619,\n",
       "          -0.00092096, -0.00005821, -0.00051683, -0.00942132, -0.00109532]])},\n",
       " 'MLP': {'NK-0': array([[0.8148632 , 0.88355531, 0.86721574, 0.86742587, 0.76562529,\n",
       "          0.73686705, 0.86496067, 0.88653805, 0.8955704 , 0.80177391],\n",
       "         [0.80471016, 0.98726784, 0.74588529, 0.66505312, 0.65835468,\n",
       "          0.96238919, 0.97337729, 0.85389761, 0.77180087, 0.67874474],\n",
       "         [0.79129473, 0.79507693, 0.79656353, 0.95717844, 0.74545771,\n",
       "          0.8394989 , 0.75405355, 0.64893116, 0.87645875, 0.76942947],\n",
       "         [0.65815308, 0.96800966, 0.87673556, 0.79786919, 0.63040583,\n",
       "          0.99330667, 0.86228017, 0.95387432, 0.8710465 , 0.68986029],\n",
       "         [0.93618144, 0.88717775, 0.89620742, 0.77952315, 0.85348095,\n",
       "          0.86465934, 0.94702072, 0.8747389 , 0.82288894, 0.93763308]]),\n",
       "  'NK-1': array([[ 0.57296327,  0.04698531,  0.57784322,  0.55329918,  0.57232913,\n",
       "           0.49445528,  0.5360495 ,  0.52084258,  0.50244008,  0.43355452],\n",
       "         [ 0.50535173,  0.58639799,  0.50549314,  0.53687692,  0.64110579,\n",
       "           0.5152482 ,  0.65405033,  0.64194941,  0.5994276 ,  0.51863775],\n",
       "         [ 0.56015197,  0.60762245,  0.50158299,  0.49348919,  0.50989041,\n",
       "           0.57645142, -0.00767754,  0.53836404,  0.55041063,  0.60556388],\n",
       "         [ 0.60220923,  0.60653403,  0.69480159,  0.67333464,  0.58025327,\n",
       "           0.53782636,  0.64532629,  0.6712403 ,  0.59670954,  0.68869435],\n",
       "         [ 0.76033371,  0.67735052,  0.68874317,  0.67073027,  0.71159705,\n",
       "           0.7668163 ,  0.67392001,  0.64134409,  0.6451851 ,  0.69424628]]),\n",
       "  'NK-2': array([[ 0.00260534,  0.35991845,  0.39083065,  0.03064544,  0.03389996,\n",
       "           0.02188255,  0.03361881,  0.02403372,  0.01651004,  0.01396377],\n",
       "         [ 0.02406398,  0.02067898,  0.03128215, -0.06842648, -0.02294554,\n",
       "           0.02613855,  0.02495942,  0.03249205,  0.01964411, -0.01182148],\n",
       "         [-0.00834248,  0.02108774,  0.00928548, -0.03658208,  0.00232774,\n",
       "           0.01813442,  0.02006975, -0.00914145, -0.00092171,  0.01942644],\n",
       "         [-0.02495063,  0.00559475, -0.00728196,  0.01078544, -0.00575624,\n",
       "          -0.00402186, -0.00778664,  0.01192092,  0.02082981,  0.01508328],\n",
       "         [ 0.04884358,  0.03090303,  0.03578229,  0.01692934,  0.32207432,\n",
       "           0.04413546,  0.06285155,  0.08096296, -0.04128466,  0.27840236]]),\n",
       "  'NK-3': array([[-0.00881554, -0.00313536, -0.01498447,  0.00005016, -0.00267314,\n",
       "          -0.00817687, -0.00751464, -0.00833609, -0.00685501, -0.00207931],\n",
       "         [-0.00144453,  0.00069503,  0.00030599, -0.05545211, -0.0023055 ,\n",
       "          -0.00366291, -0.00383259, -0.00004123, -0.00035715, -0.00907144],\n",
       "         [-0.00186444, -0.00240431, -0.00356609, -0.014489  , -0.01339322,\n",
       "          -0.05216557, -0.00083533, -0.0090549 , -0.00848602, -0.01674101],\n",
       "         [-0.03084595, -0.00080583, -0.00127098, -0.01003454, -0.00198784,\n",
       "          -0.00463047, -0.00146319, -0.00107099, -0.00075557, -0.00601737],\n",
       "         [-0.00987986, -0.00274574, -0.00195097, -0.00150998, -0.01457741,\n",
       "          -0.00072316, -0.0021828 , -0.00228866, -0.0011803 , -0.00259285]]),\n",
       "  'NK-4': array([[-0.00443642, -0.00044241, -0.01656905, -0.00372716, -0.00211592,\n",
       "          -0.0012963 , -0.00348374, -0.00109559, -0.00234674, -0.0001785 ],\n",
       "         [-0.02158063, -0.00116966, -0.00473343, -0.00232096, -0.00336092,\n",
       "          -0.00093821, -0.00107259, -0.00163239, -0.00070114, -0.00006589],\n",
       "         [-0.00035355, -0.00003311, -0.00329721, -0.00008305, -0.00005425,\n",
       "          -0.00588384, -0.00217859, -0.00012386, -0.00609696, -0.00067946],\n",
       "         [-0.00051898, -0.00301749, -0.0013635 , -0.00344688, -0.00113454,\n",
       "          -0.01364337, -0.00111141, -0.0194112 , -0.00148216, -0.02715834],\n",
       "         [-0.00273987, -0.00078906, -0.00056427, -0.00224645, -0.00017605,\n",
       "          -0.00099036, -0.00124436, -0.00014221, -0.00597124, -0.0019307 ]])},\n",
       " 'SVR': {'NK-0': array([[0.25581802, 0.25586687, 0.24787721, 0.24743515, 0.25951678,\n",
       "          0.25863478, 0.26166549, 0.25398369, 0.26660488, 0.26424965],\n",
       "         [0.25190446, 0.25115644, 0.26265291, 0.25333392, 0.25121711,\n",
       "          0.25216548, 0.25086324, 0.24910172, 0.25776579, 0.24645039],\n",
       "         [0.3279112 , 0.33367176, 0.32665322, 0.3272763 , 0.32733108,\n",
       "          0.33946048, 0.33049955, 0.3316056 , 0.33979961, 0.33507111],\n",
       "         [0.30276722, 0.29746754, 0.30650107, 0.31393595, 0.30271521,\n",
       "          0.3093501 , 0.29704349, 0.30628721, 0.31629237, 0.30456168],\n",
       "         [0.38936337, 0.37531743, 0.39524057, 0.40222234, 0.41290514,\n",
       "          0.39495302, 0.40465628, 0.3984089 , 0.41217491, 0.38672643]]),\n",
       "  'NK-1': array([[0.08061951, 0.07935013, 0.07677857, 0.07974033, 0.07645696,\n",
       "          0.07242561, 0.08046086, 0.07847575, 0.0792261 , 0.07353501],\n",
       "         [0.10852969, 0.11373804, 0.11486876, 0.10459012, 0.10727377,\n",
       "          0.10782993, 0.09831085, 0.10686883, 0.10954134, 0.10367362],\n",
       "         [0.07053721, 0.08103711, 0.07864908, 0.07658978, 0.07382192,\n",
       "          0.08065114, 0.08346792, 0.07631591, 0.07692772, 0.0817704 ],\n",
       "         [0.0840117 , 0.08131784, 0.08275681, 0.08677905, 0.08625537,\n",
       "          0.08610642, 0.08292868, 0.07425821, 0.0918551 , 0.07992147],\n",
       "         [0.11363029, 0.10182995, 0.09791752, 0.10522374, 0.10726483,\n",
       "          0.1106888 , 0.10987533, 0.11718147, 0.10762485, 0.10585992]]),\n",
       "  'NK-2': array([[0.01644203, 0.02143242, 0.01941658, 0.01920345, 0.01832593,\n",
       "          0.01892185, 0.02006282, 0.02044801, 0.01795018, 0.02126533],\n",
       "         [0.02697406, 0.02551045, 0.02486199, 0.02577271, 0.02909019,\n",
       "          0.03006557, 0.02524317, 0.02977067, 0.02895738, 0.02653178],\n",
       "         [0.026189  , 0.0236854 , 0.02646606, 0.02596376, 0.02459286,\n",
       "          0.02188354, 0.02411221, 0.02480797, 0.02371377, 0.0290462 ],\n",
       "         [0.01984406, 0.01744392, 0.01984245, 0.02060129, 0.01826015,\n",
       "          0.02316727, 0.02122968, 0.01945247, 0.01689908, 0.01944805],\n",
       "         [0.02477653, 0.01941026, 0.01887236, 0.02153537, 0.01964103,\n",
       "          0.02259084, 0.02313408, 0.01929055, 0.0215111 , 0.02125715]]),\n",
       "  'NK-3': array([[0.00257631, 0.00221909, 0.00205419, 0.00324832, 0.00201111,\n",
       "          0.00336027, 0.0013219 , 0.00241452, 0.00287638, 0.00228102],\n",
       "         [0.00400587, 0.00393359, 0.00307952, 0.00206238, 0.00552455,\n",
       "          0.00449714, 0.0035661 , 0.00461264, 0.00401262, 0.00331181],\n",
       "         [0.00142652, 0.00304976, 0.00122205, 0.00257985, 0.00319788,\n",
       "          0.00332287, 0.00287618, 0.0024063 , 0.00158549, 0.00280822],\n",
       "         [0.00092678, 0.00263813, 0.00256649, 0.0010407 , 0.00189634,\n",
       "          0.00267948, 0.00249   , 0.0006216 , 0.00019616, 0.00246588],\n",
       "         [0.00289094, 0.00251721, 0.00386972, 0.00338735, 0.00156477,\n",
       "          0.00274287, 0.00368864, 0.00365614, 0.00217769, 0.00252588]]),\n",
       "  'NK-4': array([[-0.0008248 , -0.0022013 , -0.00201519, -0.00234762, -0.00184068,\n",
       "          -0.00077551, -0.0024964 , -0.00192773, -0.00131564, -0.0009659 ],\n",
       "         [-0.00079292, -0.00192712, -0.00138916, -0.00120213, -0.00121873,\n",
       "          -0.0023117 , -0.00282595, -0.00130119, -0.00323003, -0.00109467],\n",
       "         [-0.00116743, -0.00151126, -0.00126426, -0.00154741, -0.00235682,\n",
       "          -0.00156186, -0.00199707, -0.00094685, -0.00069719, -0.0011346 ],\n",
       "         [-0.00132518, -0.00101726, -0.00022509, -0.00180903, -0.00044822,\n",
       "          -0.00225831, -0.00139326, -0.00102106, -0.00192697, -0.00088443],\n",
       "         [-0.00065118, -0.00145209, -0.0011275 , -0.00126294, -0.00145083,\n",
       "          -0.00072591, -0.00149629, -0.00159549, -0.00194742, -0.00123651]])}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_length_dependence_results = load_dict(\"Results/Length Dependency/Full_Sequence_Length_NK_Results.pkl\")\n",
    "saved_length_dependence_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Protein_Evolution",
   "language": "python",
   "name": "protein_evolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
